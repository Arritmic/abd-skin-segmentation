{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Skin Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     32\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      4\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                14\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_bbox_loss': 1.0, 'rpn_class_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_mask_loss': 1.0, 'mrcnn_bbox_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           Pratheepan_skin\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class SkinConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"Pratheepan_skin\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 4\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = SkinConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
       "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
       "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
       "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
       "       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
       "       416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
       "       429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
       "       442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
       "       455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
       "       468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
       "       481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
       "       494, 495, 496, 497, 498, 499])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
