{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify the flowers shown in the flower dataset\n",
    "\n",
    "* The flowers are in color and you'll have to work with that. You can't turn them to greyscale etc.\n",
    "* Data augmentation is allowed here due to popular complaint, but the problem can be completed without it\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torchvision import datasets,models, transforms\n",
    "import time\n",
    "import sys\n",
    "# !pip install torchsummary \n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.backends.cudnn.benchmark=True\n",
    "from tqdm import tqdm\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "import random\n",
    "import scipy.misc as m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Training and Testing Data using Data Loader with Data Augmentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "IMG_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Dataset 1: Final abdomen images\n",
    "# TRAIN_PATH1 = ['../allabdomen/train/skin_train2019/']\n",
    "# MASK_PATH1 = ['../allabdomen/train/annotations/']\n",
    "# train_ids1 = next(os.walk(TRAIN_PATH1[0]))[2]\n",
    "# mask_ids1 = next(os.walk(MASK_PATH1[0]))[2]\n",
    "# train_ids1.sort()\n",
    "# mask_ids1.sort()\n",
    "# TRAIN_PATH1 = TRAIN_PATH1*len(train_ids1)\n",
    "# MASK_PATH1 = MASK_PATH1*len(train_ids1)\n",
    "\n",
    "# # # Dataset 2: Augmented Abdomen Images\n",
    "# TRAIN_PATH2 = ['../allabdomen/train/skin_augmented/']\n",
    "# MASK_PATH2 = ['../allabdomen/train/annotations_augmented/']\n",
    "# train_ids2 = next(os.walk(TRAIN_PATH2[0]))[2]\n",
    "# mask_ids2 = next(os.walk(MASK_PATH2[0]))[2]\n",
    "# train_ids2.sort()\n",
    "# mask_ids2.sort()\n",
    "# TRAIN_PATH2 = TRAIN_PATH2*len(train_ids2)\n",
    "# MASK_PATH2 = MASK_PATH2*len(train_ids2)\n",
    "\n",
    "# # # Combine everything\n",
    "# TRAIN_PATH = np.concatenate((TRAIN_PATH1,TRAIN_PATH2))\n",
    "# MASK_PATH = np.concatenate((MASK_PATH1,MASK_PATH2))\n",
    "# train_ids = np.concatenate((train_ids1,train_ids2))\n",
    "# mask_ids = np.concatenate((mask_ids1,mask_ids2))\n",
    "\n",
    "\n",
    "# # Get and resize train images and masks DONT RUN IN THIS CODE\n",
    "# X_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "# Y_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "# print('Getting and resizing train images and masks ... ')\n",
    "# sys.stdout.flush()\n",
    "# for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n",
    "#     path = TRAIN_PATH[n] + id_\n",
    "#     img = imread(path)[:,:,:IMG_CHANNELS]\n",
    "#     img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "#     X_train[n] = img\n",
    "\n",
    "# for n, id_ in tqdm(enumerate(mask_ids), total=len(mask_ids)):\n",
    "#     path = MASK_PATH[n] + id_\n",
    "#     img = imread(path)\n",
    "#     #if n in range(899,977):\n",
    "#         #img = img[:,:,1]\n",
    "#     img = np.expand_dims(resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', \n",
    "#                                       preserve_range=True), axis=-1)\n",
    "#     Y_train[n] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if training data looks all right\n",
    "# ix = random.randint(0, len(train_ids))\n",
    "# imshow(X_train[ix])\n",
    "# plt.show()\n",
    "# # imshow(np.squeeze(Y_train[436]))\n",
    "# imshow(Y_train[ix][:,:,0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Loading testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset 1: Final abdomen images\n",
    "# TRAIN_PATH_test = ['../../../../allabdomen/test/skin_test2019/']\n",
    "# MASK_PATH_test = ['../../../../allabdomen/test/annotations/']\n",
    "# train_ids_test = next(os.walk(TRAIN_PATH_test[0]))[2]\n",
    "# mask_ids_test = next(os.walk(MASK_PATH_test[0]))[2]\n",
    "# train_ids_test.sort()\n",
    "# mask_ids_test.sort()\n",
    "# TRAIN_PATH_test = TRAIN_PATH_test*len(train_ids_test)\n",
    "# MASK_PATH_test = MASK_PATH_test*len(train_ids_test)\n",
    "\n",
    "# X_test = np.zeros((len(train_ids_test), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "# Y_test = np.zeros((len(train_ids_test), IMG_HEIGHT, IMG_WIDTH, 1),dtype=np.bool)\n",
    "# print('Getting and resizing test images and masks ... ')\n",
    "# sys.stdout.flush()\n",
    "# for n, id_ in tqdm(enumerate(train_ids_test), total=len(train_ids_test)):\n",
    "#     path = TRAIN_PATH_test[n] + id_\n",
    "#     img = imread(path)[:,:,:IMG_CHANNELS]\n",
    "#     img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "#     X_test[n] = img\n",
    "\n",
    "# for n, id_ in tqdm(enumerate(mask_ids_test), total=len(mask_ids_test)):\n",
    "#     path = MASK_PATH_test[n] + id_\n",
    "#     img = imread(path)\n",
    "#     #if n in range(899,977):\n",
    "#         #img = img[:,:,1]\n",
    "#     img = np.expand_dims(resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', \n",
    "#                                       preserve_range=True), axis=-1)\n",
    "#     Y_test[n] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if training data looks all right\n",
    "# ix = random.randint(0, len(train_ids_test))\n",
    "# imshow(X_test[ix])\n",
    "# plt.show()\n",
    "# # imshow(np.squeeze(Y_train[436]))\n",
    "# imshow(Y_test[ix][:,:,0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the Loaded train np arrays\n",
    "# np.save('X_train.npy',X_train)\n",
    "# np.save('Y_train.npy',Y_train)\n",
    "\n",
    "# # Saving the Loaded test np arrays\n",
    "# np.save('X_test.npy',X_test)\n",
    "# np.save('Y_test.npy',Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: [0.53766632 0.55240142 0.52007403], std: [0.27631814 0.25418144 0.28067154]\n"
     ]
    }
   ],
   "source": [
    "# Loading the Training and Testing Data\n",
    "trainImages = np.load('X_train.npy')\n",
    "trainLabels = (np.load('Y_train.npy')*255).astype(np.uint8)\n",
    "\n",
    "X_test = np.load('X_test.npy')\n",
    "Y_test = (np.load('Y_test.npy')*255).astype(np.uint8)\n",
    "\n",
    "# Splitting the Data into Training and Test Data\n",
    "X_train, X_val,Y_train,Y_val = train_test_split(trainImages,trainLabels, test_size=0.15, shuffle = True)\n",
    "\n",
    "# Finding the mean and the Variance\n",
    "img_mean = np.mean(np.swapaxes(trainImages/255.0,0,1).reshape(3, -1), 1)\n",
    "img_std = np.std(np.swapaxes(trainImages/255.0,0,1).reshape(3, -1), 1)\n",
    "print(\"mean: {}, std: {}\".format(img_mean, img_std))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinLoader(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_arr, y_arr, transform=None, transform_label=None):\n",
    "        self.x_arr = x_arr\n",
    "        self.y_arr = y_arr\n",
    "        self.transform = transform\n",
    "        self.transform_label = transform_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x_arr.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.x_arr[index]\n",
    "        label = self.y_arr[index]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.transform_label is not None:\n",
    "            label = self.transform_label(label)\n",
    "        return img, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalizing the Data\n",
    "normalize = transforms.Normalize(mean=list(img_mean),std=list(img_std))\n",
    "batch_size =64\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    SkinLoader(X_train, Y_train, transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]),transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "# remove augmentation transforms in test loader\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    SkinLoader(X_val, Y_val, transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]),transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "    ])),shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    SkinLoader(X_test, Y_test, transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]),transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "    ])),shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Images below are not unormalized yet and therefore the variation in rgb colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for img,lbl in train_loader:\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(np.transpose(img[0], (1,2,0)).numpy())\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(np.transpose(lbl[0],(1,2,0))[:,:,0].numpy())\n",
    "    break\n",
    "# plt.show()\n",
    "# lbl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tying Segnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv2DBatchNormRelu(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        n_filters,\n",
    "        k_size,\n",
    "        stride,\n",
    "        padding,\n",
    "        bias=True,\n",
    "        dilation=1,\n",
    "        is_batchnorm=True,\n",
    "    ):\n",
    "        super(conv2DBatchNormRelu, self).__init__()\n",
    "\n",
    "        conv_mod = nn.Conv2d(\n",
    "            int(in_channels),\n",
    "            int(n_filters),\n",
    "            kernel_size=k_size,\n",
    "            padding=padding,\n",
    "            stride=stride,\n",
    "            bias=bias,\n",
    "            dilation=dilation,\n",
    "        )\n",
    "\n",
    "        if is_batchnorm:\n",
    "            self.cbr_unit = nn.Sequential(\n",
    "                conv_mod, nn.BatchNorm2d(int(n_filters)), nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.cbr_unit = nn.Sequential(conv_mod, nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.cbr_unit(inputs)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class segnetDown2(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(segnetDown2, self).__init__()\n",
    "        self.conv1 = conv2DBatchNormRelu(in_size, out_size, 3, 1, 1)\n",
    "        self.conv2 = conv2DBatchNormRelu(out_size, out_size, 3, 1, 1)\n",
    "        self.maxpool_with_argmax = nn.MaxPool2d(2, 2, return_indices=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.conv1(inputs)\n",
    "        outputs = self.conv2(outputs)\n",
    "        unpooled_shape = outputs.size()\n",
    "        outputs, indices = self.maxpool_with_argmax(outputs)\n",
    "        return outputs, indices, unpooled_shape\n",
    "\n",
    "\n",
    "class segnetDown3(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(segnetDown3, self).__init__()\n",
    "        self.conv1 = conv2DBatchNormRelu(in_size, out_size, 3, 1, 1)\n",
    "        self.conv2 = conv2DBatchNormRelu(out_size, out_size, 3, 1, 1)\n",
    "        self.conv3 = conv2DBatchNormRelu(out_size, out_size, 3, 1, 1)\n",
    "        self.maxpool_with_argmax = nn.MaxPool2d(2, 2, return_indices=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.conv1(inputs)\n",
    "        outputs = self.conv2(outputs)\n",
    "        outputs = self.conv3(outputs)\n",
    "        unpooled_shape = outputs.size()\n",
    "        outputs, indices = self.maxpool_with_argmax(outputs)\n",
    "        return outputs, indices, unpooled_shape\n",
    "\n",
    "\n",
    "class segnetUp2(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(segnetUp2, self).__init__()\n",
    "        self.unpool = nn.MaxUnpool2d(2, 2)\n",
    "        self.conv1 = conv2DBatchNormRelu(in_size, in_size, 3, 1, 1)\n",
    "        self.conv2 = conv2DBatchNormRelu(in_size, out_size, 3, 1, 1)\n",
    "\n",
    "    def forward(self, inputs, indices, output_shape):\n",
    "        outputs = self.unpool(input=inputs, indices=indices, output_size=output_shape)\n",
    "        outputs = self.conv1(outputs)\n",
    "        outputs = self.conv2(outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class segnetUp3(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(segnetUp3, self).__init__()\n",
    "        self.unpool = nn.MaxUnpool2d(2, 2)\n",
    "        self.conv1 = conv2DBatchNormRelu(in_size, in_size, 3, 1, 1)\n",
    "        self.conv2 = conv2DBatchNormRelu(in_size, in_size, 3, 1, 1)\n",
    "        self.conv3 = conv2DBatchNormRelu(in_size, out_size, 3, 1, 1)\n",
    "\n",
    "    def forward(self, inputs, indices, output_shape):\n",
    "        outputs = self.unpool(input=inputs, indices=indices, output_size=output_shape)\n",
    "        outputs = self.conv1(outputs)\n",
    "        outputs = self.conv2(outputs)\n",
    "        outputs = self.conv3(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): segnet(\n",
       "    (down1): segnetDown2(\n",
       "      (conv1): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv2): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (maxpool_with_argmax): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (down2): segnetDown2(\n",
       "      (conv1): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv2): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (maxpool_with_argmax): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (down3): segnetDown3(\n",
       "      (conv1): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv2): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv3): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (maxpool_with_argmax): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (down4): segnetDown3(\n",
       "      (conv1): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv2): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv3): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (maxpool_with_argmax): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (down5): segnetDown3(\n",
       "      (conv1): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv2): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv3): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (maxpool_with_argmax): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (up5): segnetUp3(\n",
       "      (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "      (conv1): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv2): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv3): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up4): segnetUp3(\n",
       "      (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "      (conv1): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv2): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv3): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up3): segnetUp3(\n",
       "      (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "      (conv1): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv2): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv3): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up2): segnetUp2(\n",
       "      (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "      (conv1): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv2): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up1): segnetUp2(\n",
       "      (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "      (conv1): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv2): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class segnet(nn.Module):\n",
    "    def __init__(self, n_classes=1, in_channels=3, is_unpooling=True):\n",
    "        super(segnet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.is_unpooling = is_unpooling\n",
    "\n",
    "        self.down1 = segnetDown2(self.in_channels, 64)\n",
    "        self.down2 = segnetDown2(64, 128)\n",
    "        self.down3 = segnetDown3(128, 256)\n",
    "        self.down4 = segnetDown3(256, 512)\n",
    "        self.down5 = segnetDown3(512, 512)\n",
    "\n",
    "        self.up5 = segnetUp3(512, 512)\n",
    "        self.up4 = segnetUp3(512, 256)\n",
    "        self.up3 = segnetUp3(256, 128)\n",
    "        self.up2 = segnetUp2(128, 64)\n",
    "        self.up1 = segnetUp2(64, n_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        down1, indices_1, unpool_shape1 = self.down1(inputs)\n",
    "        down2, indices_2, unpool_shape2 = self.down2(down1)\n",
    "        down3, indices_3, unpool_shape3 = self.down3(down2)\n",
    "        down4, indices_4, unpool_shape4 = self.down4(down3)\n",
    "        down5, indices_5, unpool_shape5 = self.down5(down4)\n",
    "\n",
    "        up5 = self.up5(down5, indices_5, unpool_shape5)\n",
    "        up4 = self.up4(up5, indices_4, unpool_shape4)\n",
    "        up3 = self.up3(up4, indices_3, unpool_shape3)\n",
    "        up2 = self.up2(up3, indices_2, unpool_shape2)\n",
    "        up1 = self.up1(up2, indices_1, unpool_shape1)\n",
    "\n",
    "        return up1\n",
    "\n",
    "    def init_vgg16_params(self, vgg16):\n",
    "        blocks = [self.down1, self.down2, self.down3, self.down4, self.down5]\n",
    "\n",
    "        features = list(vgg16.features.children())\n",
    "\n",
    "        vgg_layers = []\n",
    "        for _layer in features:\n",
    "            if isinstance(_layer, nn.Conv2d):\n",
    "                vgg_layers.append(_layer)\n",
    "\n",
    "        merged_layers = []\n",
    "        for idx, conv_block in enumerate(blocks):\n",
    "            if idx < 2:\n",
    "                units = [conv_block.conv1.cbr_unit, conv_block.conv2.cbr_unit]\n",
    "            else:\n",
    "                units = [\n",
    "                    conv_block.conv1.cbr_unit,\n",
    "                    conv_block.conv2.cbr_unit,\n",
    "                    conv_block.conv3.cbr_unit,\n",
    "                ]\n",
    "            for _unit in units:\n",
    "                for _layer in _unit:\n",
    "                    if isinstance(_layer, nn.Conv2d):\n",
    "                        merged_layers.append(_layer)\n",
    "\n",
    "        assert len(vgg_layers) == len(merged_layers)\n",
    "\n",
    "        for l1, l2 in zip(vgg_layers, merged_layers):\n",
    "            if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n",
    "                assert l1.weight.size() == l2.weight.size()\n",
    "                assert l1.bias.size() == l2.bias.size()\n",
    "                l2.weight.data = l1.weight.data\n",
    "                l2.bias.data = l1.bias.data\n",
    "net = segnet()\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "net.init_vgg16_params(vgg16)\n",
    "net.to(device)\n",
    "net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying PSPNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# class _PyramidPoolingModule(nn.Module):\n",
    "#     def __init__(self, in_dim, reduction_dim, setting):\n",
    "#         super(_PyramidPoolingModule, self).__init__()\n",
    "#         self.features = []\n",
    "#         for s in setting:\n",
    "#             self.features.append(nn.Sequential(\n",
    "#                 nn.AdaptiveAvgPool2d(s),\n",
    "#                 nn.Conv2d(in_dim, reduction_dim, kernel_size=1, bias=False),\n",
    "#                 nn.BatchNorm2d(reduction_dim, momentum=.95),\n",
    "#                 nn.ReLU(inplace=True)\n",
    "#             ))\n",
    "#         self.features = nn.ModuleList(self.features)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x_size = x.size()\n",
    "#         out = [x]\n",
    "#         for f in self.features:\n",
    "#             out.append(F.upsample(f(x), x_size[2:], mode='bilinear'))\n",
    "#         out = torch.cat(out, 1)\n",
    "#         return out\n",
    "\n",
    "# class PSPNet(nn.Module):\n",
    "#     def __init__(self, num_classes, pretrained=True, use_aux=True):\n",
    "#         super(PSPNet, self).__init__()\n",
    "#         self.use_aux = use_aux\n",
    "#         resnet = models.resnet101()\n",
    "#         if pretrained:\n",
    "#             resnet.load_state_dict(torch.load('../../resnet101-5d3b4d8f.pth'))\n",
    "#         self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n",
    "#         self.layer1, self.layer2, self.layer3, self.layer4 = resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4\n",
    "\n",
    "#         for n, m in self.layer3.named_modules():\n",
    "#             if 'conv2' in n:\n",
    "#                 m.dilation, m.padding, m.stride = (2, 2), (2, 2), (1, 1)\n",
    "#             elif 'downsample.0' in n:\n",
    "#                 m.stride = (1, 1)\n",
    "#         for n, m in self.layer4.named_modules():\n",
    "#             if 'conv2' in n:\n",
    "#                 m.dilation, m.padding, m.stride = (4, 4), (4, 4), (1, 1)\n",
    "#             elif 'downsample.0' in n:\n",
    "#                 m.stride = (1, 1)\n",
    "\n",
    "#         self.ppm = _PyramidPoolingModule(2048, 512, (1, 2, 3, 6))\n",
    "#         self.final = nn.Sequential(\n",
    "#             nn.Conv2d(4096, 512, kernel_size=3, padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(512, momentum=.95),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "#         )\n",
    "\n",
    "#         if use_aux:\n",
    "#             self.aux_logits = nn.Conv2d(1024, num_classes, kernel_size=1)\n",
    "#             initialize_weights(self.aux_logits)\n",
    "\n",
    "#         initialize_weights(self.ppm, self.final)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x_size = x.size()\n",
    "#         x = self.layer0(x)\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "#         if self.training and self.use_aux:\n",
    "#             aux = self.aux_logits(x)\n",
    "#         x = self.layer4(x)\n",
    "#         x = self.ppm(x)\n",
    "#         x = self.final(x)\n",
    "#         if self.training and self.use_aux:\n",
    "#             return F.upsample(x, x_size[2:], mode='bilinear'), F.upsample(aux, x_size[2:], mode='bilinear')\n",
    "#         return F.upsample(x, x_size[2:], mode='bilinear')\n",
    "    \n",
    "# def initialize_weights(*models):\n",
    "#     for model in models:\n",
    "#         for module in model.modules():\n",
    "#             if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "#                 nn.init.kaiming_normal(module.weight)\n",
    "#                 if module.bias is not None:\n",
    "#                     module.bias.data.zero_()\n",
    "#             elif isinstance(module, nn.BatchNorm2d):\n",
    "#                 module.weight.data.fill_(1)    \n",
    "#                 module.bias.data.zero_()\n",
    "# net = PSPNet(num_classes=1)\n",
    "# net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If want to get a summary of the network uncomment the below line as well as the one in importing libraries.\n",
    "# summary(net,(3,32,32),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss2d(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True, ignore_index=255):\n",
    "        super(CrossEntropyLoss2d, self).__init__()\n",
    "        self.nll_loss = nn.NLLLoss2d(weight, size_average, ignore_index)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        return self.nll_loss(F.log_softmax(inputs), targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# criterion = nn.CrossEntropyLoss() \n",
    "criterion =  nn.BCELoss()\n",
    "# criterion = CrossEntropyLoss2d(size_average=True)# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9,weight_decay = 5e-4)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay= 5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # For Debugging\n",
    "# for batch_idx, (data, target) in enumerate(test_loader):\n",
    "# #     data, target = data.to(device), target.to(device)\n",
    "#     optimizer.zero_grad()\n",
    "#     output = net(data)\n",
    "#     break\n",
    "# data.shape\n",
    "# target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = criterion(output, target)\n",
    "# loss.backward()\n",
    "# optimizer.step()\n",
    "# train_hist['train_loss'].append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hist = {}\n",
    "train_hist['train_loss'] = []\n",
    "train_hist['test_loss'] = []\n",
    "train_hist['train_loss_epoch'] = []\n",
    "train_hist['test_loss_epoch'] = []\n",
    "train_hist['per_epoch_time'] = []\n",
    "train_hist['total_time'] = []\n",
    "train_hist['train_accu'] = []\n",
    "train_hist['test_accu'] = []\n",
    "epochs = 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    print('training start!!')\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_start_time = time.time()\n",
    "#         training_loss = 0\n",
    "        train_correct = 0\n",
    "        lo = []\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            #==== Forward Pass=====\n",
    "            output = model(data)\n",
    "            output[output>1] =1\n",
    "            output[output<1] =0\n",
    "            loss = criterion(output, target)\n",
    "            #=====Backward Pass=======\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #=== Loss Append to get loss of entire Batch====\n",
    "            train_hist['train_loss'].append(loss.item())\n",
    "            #==== Calculating Training Accuracy========= \n",
    "            train_correct += output.eq(target).sum().item()\n",
    "            #======= Logging results after every 20th batch============ \n",
    "            if batch_idx % 20 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "        #======== Getting Accuracy of the entire epoch by averaging of each batch===========    \n",
    "        train_hist['train_accu'].append(100 * (train_correct / (Y_train.shape[0]*128*128)))\n",
    "        #======== Getting Training Loss of the epoch by averaging across each batch\n",
    "        train_hist['train_loss_epoch'].append(np.mean(train_hist['train_loss']))\n",
    "        test(model)\n",
    "\n",
    "def test(model):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad(): # as we dont need to backpropogate when calculating testing error and accuracy\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            #==== Getting the Prediction======\n",
    "            output = model(data)\n",
    "            output[output>1] =1\n",
    "            output[output<1] =0\n",
    "            #===== Calculating the Loss=========\n",
    "            test_loss = criterion(output, target)\n",
    "            train_hist['test_loss'].append(test_loss.item())\n",
    "            # Calculating Testing Accuracy for the all inputs=========\n",
    "            correct += output.eq(target).sum().item()\n",
    "    #======= Getting Testing Accuracy for the Epoch========\n",
    "    train_hist['test_accu'].append(100. * correct / (Y_val.shape[0]*128*128))\n",
    "    #====== Getting Testing Error of Epoch========\n",
    "    train_hist['test_loss_epoch'].append(np.mean(train_hist['test_loss']))\n",
    "   #======= Logging results after every epoch ============ \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        np.mean(train_hist['test_loss']), correct, val_loader.dataset.y_arr.size,\n",
    "        100 * correct / (Y_val.shape[0]*128*128)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start!!\n",
      "Train Epoch: 0 [0/3276 (0%)]\tLoss: 17.344385\n",
      "Train Epoch: 0 [1280/3276 (38%)]\tLoss: 19.759876\n",
      "Train Epoch: 0 [2560/3276 (77%)]\tLoss: 16.813889\n",
      "\n",
      "Test set: Average loss: 17.8590, Accuracy: 3354950/9486336 (35%)\n",
      "\n",
      "Train Epoch: 1 [0/3276 (0%)]\tLoss: 18.003214\n",
      "Train Epoch: 1 [1280/3276 (38%)]\tLoss: 18.476240\n",
      "Train Epoch: 1 [2560/3276 (77%)]\tLoss: 17.897308\n",
      "\n",
      "Test set: Average loss: 17.7884, Accuracy: 3403442/9486336 (36%)\n",
      "\n",
      "Train Epoch: 2 [0/3276 (0%)]\tLoss: 18.582699\n",
      "Train Epoch: 2 [1280/3276 (38%)]\tLoss: 17.777491\n",
      "Train Epoch: 2 [2560/3276 (77%)]\tLoss: 19.860830\n",
      "\n",
      "Test set: Average loss: 17.7649, Accuracy: 3403359/9486336 (36%)\n",
      "\n",
      "Train Epoch: 3 [0/3276 (0%)]\tLoss: 19.437975\n",
      "Train Epoch: 3 [1280/3276 (38%)]\tLoss: 18.141926\n",
      "Train Epoch: 3 [2560/3276 (77%)]\tLoss: 18.203165\n",
      "\n",
      "Test set: Average loss: 17.7532, Accuracy: 3403358/9486336 (36%)\n",
      "\n",
      "Train Epoch: 4 [0/3276 (0%)]\tLoss: 18.646704\n",
      "Train Epoch: 4 [1280/3276 (38%)]\tLoss: 18.114336\n",
      "Train Epoch: 4 [2560/3276 (77%)]\tLoss: 18.095707\n",
      "\n",
      "Test set: Average loss: 17.7461, Accuracy: 3403363/9486336 (36%)\n",
      "\n",
      "Train Epoch: 5 [0/3276 (0%)]\tLoss: 17.924267\n",
      "Train Epoch: 5 [1280/3276 (38%)]\tLoss: 18.820385\n",
      "Train Epoch: 5 [2560/3276 (77%)]\tLoss: 17.575748\n",
      "\n",
      "Test set: Average loss: 17.7415, Accuracy: 3403363/9486336 (36%)\n",
      "\n",
      "Train Epoch: 6 [0/3276 (0%)]\tLoss: 18.990507\n",
      "Train Epoch: 6 [1280/3276 (38%)]\tLoss: 17.898388\n",
      "Train Epoch: 6 [2560/3276 (77%)]\tLoss: 17.416615\n",
      "\n",
      "Test set: Average loss: 17.7381, Accuracy: 3403363/9486336 (36%)\n",
      "\n",
      "Train Epoch: 7 [0/3276 (0%)]\tLoss: 16.533461\n",
      "Train Epoch: 7 [1280/3276 (38%)]\tLoss: 18.387754\n",
      "Train Epoch: 7 [2560/3276 (77%)]\tLoss: 18.886894\n",
      "\n",
      "Test set: Average loss: 17.7356, Accuracy: 3403362/9486336 (36%)\n",
      "\n",
      "Train Epoch: 8 [0/3276 (0%)]\tLoss: 17.629713\n",
      "Train Epoch: 8 [1280/3276 (38%)]\tLoss: 18.122980\n",
      "Train Epoch: 8 [2560/3276 (77%)]\tLoss: 17.249470\n",
      "\n",
      "Test set: Average loss: 17.7336, Accuracy: 3403359/9486336 (36%)\n",
      "\n",
      "Train Epoch: 9 [0/3276 (0%)]\tLoss: 17.166096\n",
      "Train Epoch: 9 [1280/3276 (38%)]\tLoss: 19.277075\n",
      "Train Epoch: 9 [2560/3276 (77%)]\tLoss: 18.588522\n",
      "\n",
      "Test set: Average loss: 17.7321, Accuracy: 3403359/9486336 (36%)\n",
      "\n",
      "Train Epoch: 10 [0/3276 (0%)]\tLoss: 18.609524\n",
      "Train Epoch: 10 [1280/3276 (38%)]\tLoss: 18.726837\n",
      "Train Epoch: 10 [2560/3276 (77%)]\tLoss: 18.029617\n",
      "\n",
      "Test set: Average loss: 17.7308, Accuracy: 3403361/9486336 (36%)\n",
      "\n",
      "Train Epoch: 11 [0/3276 (0%)]\tLoss: 19.374308\n",
      "Train Epoch: 11 [1280/3276 (38%)]\tLoss: 18.460772\n",
      "Train Epoch: 11 [2560/3276 (77%)]\tLoss: 19.173094\n",
      "\n",
      "Test set: Average loss: 17.7297, Accuracy: 3403357/9486336 (36%)\n",
      "\n",
      "Train Epoch: 12 [0/3276 (0%)]\tLoss: 18.964102\n",
      "Train Epoch: 12 [1280/3276 (38%)]\tLoss: 18.519588\n",
      "Train Epoch: 12 [2560/3276 (77%)]\tLoss: 18.095337\n",
      "\n",
      "Test set: Average loss: 17.7288, Accuracy: 3403356/9486336 (36%)\n",
      "\n",
      "Train Epoch: 13 [0/3276 (0%)]\tLoss: 17.992596\n",
      "Train Epoch: 13 [1280/3276 (38%)]\tLoss: 18.689657\n",
      "Train Epoch: 13 [2560/3276 (77%)]\tLoss: 18.944946\n",
      "\n",
      "Test set: Average loss: 17.7280, Accuracy: 3403352/9486336 (36%)\n",
      "\n",
      "Train Epoch: 14 [0/3276 (0%)]\tLoss: 18.442646\n",
      "Train Epoch: 14 [1280/3276 (38%)]\tLoss: 18.944710\n",
      "Train Epoch: 14 [2560/3276 (77%)]\tLoss: 18.872137\n",
      "\n",
      "Test set: Average loss: 17.7274, Accuracy: 3403354/9486336 (36%)\n",
      "\n",
      "Train Epoch: 15 [0/3276 (0%)]\tLoss: 19.086163\n",
      "Train Epoch: 15 [1280/3276 (38%)]\tLoss: 18.620354\n",
      "Train Epoch: 15 [2560/3276 (77%)]\tLoss: 18.432156\n",
      "\n",
      "Test set: Average loss: 17.7268, Accuracy: 3403356/9486336 (36%)\n",
      "\n",
      "Train Epoch: 16 [0/3276 (0%)]\tLoss: 18.042740\n",
      "Train Epoch: 16 [1280/3276 (38%)]\tLoss: 18.750898\n",
      "Train Epoch: 16 [2560/3276 (77%)]\tLoss: 17.577408\n",
      "\n",
      "Test set: Average loss: 17.7263, Accuracy: 3403358/9486336 (36%)\n",
      "\n",
      "Train Epoch: 17 [0/3276 (0%)]\tLoss: 18.084110\n",
      "Train Epoch: 17 [1280/3276 (38%)]\tLoss: 18.167248\n",
      "Train Epoch: 17 [2560/3276 (77%)]\tLoss: 18.692318\n",
      "\n",
      "Test set: Average loss: 17.7258, Accuracy: 3403355/9486336 (36%)\n",
      "\n",
      "Train Epoch: 18 [0/3276 (0%)]\tLoss: 18.483856\n",
      "Train Epoch: 18 [1280/3276 (38%)]\tLoss: 17.448076\n",
      "Train Epoch: 18 [2560/3276 (77%)]\tLoss: 19.746176\n",
      "\n",
      "Test set: Average loss: 17.7254, Accuracy: 3403358/9486336 (36%)\n",
      "\n",
      "Train Epoch: 19 [0/3276 (0%)]\tLoss: 19.803831\n",
      "Train Epoch: 19 [1280/3276 (38%)]\tLoss: 18.669788\n",
      "Train Epoch: 19 [2560/3276 (77%)]\tLoss: 16.232927\n",
      "\n",
      "Test set: Average loss: 17.7250, Accuracy: 3403350/9486336 (36%)\n",
      "\n",
      "Train Epoch: 20 [0/3276 (0%)]\tLoss: 18.666759\n",
      "Train Epoch: 20 [1280/3276 (38%)]\tLoss: 19.741011\n",
      "Train Epoch: 20 [2560/3276 (77%)]\tLoss: 19.086426\n",
      "\n",
      "Test set: Average loss: 17.7247, Accuracy: 3403348/9486336 (36%)\n",
      "\n",
      "Train Epoch: 21 [0/3276 (0%)]\tLoss: 17.224277\n",
      "Train Epoch: 21 [1280/3276 (38%)]\tLoss: 17.601624\n",
      "Train Epoch: 21 [2560/3276 (77%)]\tLoss: 18.248699\n",
      "\n",
      "Test set: Average loss: 17.7244, Accuracy: 3403352/9486336 (36%)\n",
      "\n",
      "Train Epoch: 22 [0/3276 (0%)]\tLoss: 18.833059\n",
      "Train Epoch: 22 [1280/3276 (38%)]\tLoss: 16.538231\n",
      "Train Epoch: 22 [2560/3276 (77%)]\tLoss: 18.188751\n",
      "\n",
      "Test set: Average loss: 17.7241, Accuracy: 3403352/9486336 (36%)\n",
      "\n",
      "Train Epoch: 23 [0/3276 (0%)]\tLoss: 18.438164\n",
      "Train Epoch: 23 [1280/3276 (38%)]\tLoss: 18.295105\n",
      "Train Epoch: 23 [2560/3276 (77%)]\tLoss: 17.572533\n",
      "\n",
      "Test set: Average loss: 17.7239, Accuracy: 3403356/9486336 (36%)\n",
      "\n",
      "Train Epoch: 24 [0/3276 (0%)]\tLoss: 18.409124\n",
      "Train Epoch: 24 [1280/3276 (38%)]\tLoss: 18.621910\n",
      "Train Epoch: 24 [2560/3276 (77%)]\tLoss: 17.213923\n",
      "\n",
      "Test set: Average loss: 17.7236, Accuracy: 3403352/9486336 (36%)\n",
      "\n",
      "Train Epoch: 25 [0/3276 (0%)]\tLoss: 18.784969\n",
      "Train Epoch: 25 [1280/3276 (38%)]\tLoss: 17.667000\n",
      "Train Epoch: 25 [2560/3276 (77%)]\tLoss: 17.113998\n",
      "\n",
      "Test set: Average loss: 17.7234, Accuracy: 3403354/9486336 (36%)\n",
      "\n",
      "Train Epoch: 26 [0/3276 (0%)]\tLoss: 18.556980\n",
      "Train Epoch: 26 [1280/3276 (38%)]\tLoss: 17.946375\n",
      "Train Epoch: 26 [2560/3276 (77%)]\tLoss: 18.235077\n",
      "\n",
      "Test set: Average loss: 17.7232, Accuracy: 3403359/9486336 (36%)\n",
      "\n",
      "Train Epoch: 27 [0/3276 (0%)]\tLoss: 18.427967\n",
      "Train Epoch: 27 [1280/3276 (38%)]\tLoss: 18.684704\n",
      "Train Epoch: 27 [2560/3276 (77%)]\tLoss: 17.833170\n",
      "\n",
      "Test set: Average loss: 17.7230, Accuracy: 3403353/9486336 (36%)\n",
      "\n",
      "Train Epoch: 28 [0/3276 (0%)]\tLoss: 17.862843\n",
      "Train Epoch: 28 [1280/3276 (38%)]\tLoss: 16.954153\n",
      "Train Epoch: 28 [2560/3276 (77%)]\tLoss: 17.751667\n",
      "\n",
      "Test set: Average loss: 17.7229, Accuracy: 3403352/9486336 (36%)\n",
      "\n",
      "Train Epoch: 29 [0/3276 (0%)]\tLoss: 17.682100\n",
      "Train Epoch: 29 [1280/3276 (38%)]\tLoss: 18.999992\n",
      "Train Epoch: 29 [2560/3276 (77%)]\tLoss: 17.083248\n",
      "\n",
      "Test set: Average loss: 17.7227, Accuracy: 3403350/9486336 (36%)\n",
      "\n",
      "Train Epoch: 30 [0/3276 (0%)]\tLoss: 19.518108\n",
      "Train Epoch: 30 [1280/3276 (38%)]\tLoss: 18.735455\n",
      "Train Epoch: 30 [2560/3276 (77%)]\tLoss: 18.656588\n",
      "\n",
      "Test set: Average loss: 17.7225, Accuracy: 3403340/9486336 (36%)\n",
      "\n",
      "Train Epoch: 31 [0/3276 (0%)]\tLoss: 17.861261\n",
      "Train Epoch: 31 [1280/3276 (38%)]\tLoss: 17.450739\n",
      "Train Epoch: 31 [2560/3276 (77%)]\tLoss: 18.024927\n",
      "\n",
      "Test set: Average loss: 17.7224, Accuracy: 3403345/9486336 (36%)\n",
      "\n",
      "Train Epoch: 32 [0/3276 (0%)]\tLoss: 19.023840\n",
      "Train Epoch: 32 [1280/3276 (38%)]\tLoss: 18.758381\n",
      "Train Epoch: 32 [2560/3276 (77%)]\tLoss: 17.644104\n",
      "\n",
      "Test set: Average loss: 17.7223, Accuracy: 3403346/9486336 (36%)\n",
      "\n",
      "Train Epoch: 33 [0/3276 (0%)]\tLoss: 18.571474\n",
      "Train Epoch: 33 [1280/3276 (38%)]\tLoss: 18.112043\n",
      "Train Epoch: 33 [2560/3276 (77%)]\tLoss: 17.970486\n",
      "\n",
      "Test set: Average loss: 17.7221, Accuracy: 3403349/9486336 (36%)\n",
      "\n",
      "Train Epoch: 34 [0/3276 (0%)]\tLoss: 18.468731\n",
      "Train Epoch: 34 [1280/3276 (38%)]\tLoss: 18.260399\n",
      "Train Epoch: 34 [2560/3276 (77%)]\tLoss: 17.314953\n",
      "\n",
      "Test set: Average loss: 17.7220, Accuracy: 3403350/9486336 (36%)\n",
      "\n",
      "Train Epoch: 35 [0/3276 (0%)]\tLoss: 18.406252\n",
      "Train Epoch: 35 [1280/3276 (38%)]\tLoss: 17.623917\n",
      "Train Epoch: 35 [2560/3276 (77%)]\tLoss: 17.761600\n",
      "\n",
      "Test set: Average loss: 17.7219, Accuracy: 3403352/9486336 (36%)\n",
      "\n",
      "Train Epoch: 36 [0/3276 (0%)]\tLoss: 18.959148\n",
      "Train Epoch: 36 [1280/3276 (38%)]\tLoss: 18.139212\n",
      "Train Epoch: 36 [2560/3276 (77%)]\tLoss: 19.214966\n",
      "\n",
      "Test set: Average loss: 17.7218, Accuracy: 3403354/9486336 (36%)\n",
      "\n",
      "Train Epoch: 37 [0/3276 (0%)]\tLoss: 17.740652\n",
      "Train Epoch: 37 [1280/3276 (38%)]\tLoss: 19.745937\n",
      "Train Epoch: 37 [2560/3276 (77%)]\tLoss: 17.306231\n",
      "\n",
      "Test set: Average loss: 17.7217, Accuracy: 3403358/9486336 (36%)\n",
      "\n",
      "Train Epoch: 38 [0/3276 (0%)]\tLoss: 17.612297\n",
      "Train Epoch: 38 [1280/3276 (38%)]\tLoss: 18.630262\n",
      "Train Epoch: 38 [2560/3276 (77%)]\tLoss: 17.529213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 17.7216, Accuracy: 3403360/9486336 (36%)\n",
      "\n",
      "Train Epoch: 39 [0/3276 (0%)]\tLoss: 18.828503\n",
      "Train Epoch: 39 [1280/3276 (38%)]\tLoss: 18.110041\n",
      "Train Epoch: 39 [2560/3276 (77%)]\tLoss: 18.242428\n",
      "\n",
      "Test set: Average loss: 17.7215, Accuracy: 3403369/9486336 (36%)\n",
      "\n",
      "Train Epoch: 40 [0/3276 (0%)]\tLoss: 18.372892\n",
      "Train Epoch: 40 [1280/3276 (38%)]\tLoss: 17.989168\n",
      "Train Epoch: 40 [2560/3276 (77%)]\tLoss: 18.734718\n",
      "\n",
      "Test set: Average loss: 17.7214, Accuracy: 3403373/9486336 (36%)\n",
      "\n",
      "Train Epoch: 41 [0/3276 (0%)]\tLoss: 17.868956\n",
      "Train Epoch: 41 [1280/3276 (38%)]\tLoss: 18.170410\n",
      "Train Epoch: 41 [2560/3276 (77%)]\tLoss: 17.719439\n",
      "\n",
      "Test set: Average loss: 17.7214, Accuracy: 3403373/9486336 (36%)\n",
      "\n",
      "Train Epoch: 42 [0/3276 (0%)]\tLoss: 17.905266\n",
      "Train Epoch: 42 [1280/3276 (38%)]\tLoss: 18.695850\n",
      "Train Epoch: 42 [2560/3276 (77%)]\tLoss: 17.812984\n",
      "\n",
      "Test set: Average loss: 17.7213, Accuracy: 3403378/9486336 (36%)\n",
      "\n",
      "Train Epoch: 43 [0/3276 (0%)]\tLoss: 18.441404\n",
      "Train Epoch: 43 [1280/3276 (38%)]\tLoss: 18.959518\n",
      "Train Epoch: 43 [2560/3276 (77%)]\tLoss: 19.115965\n",
      "\n",
      "Test set: Average loss: 17.7212, Accuracy: 3403380/9486336 (36%)\n",
      "\n",
      "Train Epoch: 44 [0/3276 (0%)]\tLoss: 18.010803\n",
      "Train Epoch: 44 [1280/3276 (38%)]\tLoss: 18.614162\n",
      "Train Epoch: 44 [2560/3276 (77%)]\tLoss: 18.293602\n",
      "\n",
      "Test set: Average loss: 17.7211, Accuracy: 3403385/9486336 (36%)\n",
      "\n",
      "Train Epoch: 45 [0/3276 (0%)]\tLoss: 17.617619\n",
      "Train Epoch: 45 [1280/3276 (38%)]\tLoss: 17.615591\n",
      "Train Epoch: 45 [2560/3276 (77%)]\tLoss: 18.117550\n",
      "\n",
      "Test set: Average loss: 17.7211, Accuracy: 3403388/9486336 (36%)\n",
      "\n",
      "Train Epoch: 46 [0/3276 (0%)]\tLoss: 18.305143\n",
      "Train Epoch: 46 [1280/3276 (38%)]\tLoss: 18.463039\n",
      "Train Epoch: 46 [2560/3276 (77%)]\tLoss: 18.387993\n",
      "\n",
      "Test set: Average loss: 17.7210, Accuracy: 3403392/9486336 (36%)\n",
      "\n",
      "Train Epoch: 47 [0/3276 (0%)]\tLoss: 18.245827\n",
      "Train Epoch: 47 [1280/3276 (38%)]\tLoss: 16.837315\n",
      "Train Epoch: 47 [2560/3276 (77%)]\tLoss: 18.031174\n",
      "\n",
      "Test set: Average loss: 17.7209, Accuracy: 3403386/9486336 (36%)\n",
      "\n",
      "Train Epoch: 48 [0/3276 (0%)]\tLoss: 18.257685\n",
      "Train Epoch: 48 [1280/3276 (38%)]\tLoss: 16.855576\n",
      "Train Epoch: 48 [2560/3276 (77%)]\tLoss: 18.088724\n",
      "\n",
      "Test set: Average loss: 17.7209, Accuracy: 3403385/9486336 (36%)\n",
      "\n",
      "Train Epoch: 49 [0/3276 (0%)]\tLoss: 18.126299\n",
      "Train Epoch: 49 [1280/3276 (38%)]\tLoss: 18.164904\n",
      "Train Epoch: 49 [2560/3276 (77%)]\tLoss: 18.420113\n",
      "\n",
      "Test set: Average loss: 17.7208, Accuracy: 3403385/9486336 (36%)\n",
      "\n",
      "Train Epoch: 50 [0/3276 (0%)]\tLoss: 18.828766\n",
      "Train Epoch: 50 [1280/3276 (38%)]\tLoss: 16.856260\n",
      "Train Epoch: 50 [2560/3276 (77%)]\tLoss: 18.577429\n",
      "\n",
      "Test set: Average loss: 17.7207, Accuracy: 3403381/9486336 (36%)\n",
      "\n",
      "Train Epoch: 51 [0/3276 (0%)]\tLoss: 17.789585\n",
      "Train Epoch: 51 [1280/3276 (38%)]\tLoss: 17.441858\n",
      "Train Epoch: 51 [2560/3276 (77%)]\tLoss: 17.830853\n",
      "\n",
      "Test set: Average loss: 17.7207, Accuracy: 3403382/9486336 (36%)\n",
      "\n",
      "Train Epoch: 52 [0/3276 (0%)]\tLoss: 18.028933\n",
      "Train Epoch: 52 [1280/3276 (38%)]\tLoss: 18.115522\n",
      "Train Epoch: 52 [2560/3276 (77%)]\tLoss: 19.196861\n",
      "\n",
      "Test set: Average loss: 17.7206, Accuracy: 3403382/9486336 (36%)\n",
      "\n",
      "Train Epoch: 53 [0/3276 (0%)]\tLoss: 17.383860\n",
      "Train Epoch: 53 [1280/3276 (38%)]\tLoss: 18.759119\n",
      "Train Epoch: 53 [2560/3276 (77%)]\tLoss: 19.558187\n",
      "\n",
      "Test set: Average loss: 17.7206, Accuracy: 3403382/9486336 (36%)\n",
      "\n",
      "Train Epoch: 54 [0/3276 (0%)]\tLoss: 20.029341\n",
      "Train Epoch: 54 [1280/3276 (38%)]\tLoss: 17.976072\n",
      "Train Epoch: 54 [2560/3276 (77%)]\tLoss: 19.048742\n",
      "\n",
      "Test set: Average loss: 17.7263, Accuracy: 3294502/9486336 (35%)\n",
      "\n",
      "Train Epoch: 55 [0/3276 (0%)]\tLoss: 17.773643\n",
      "Train Epoch: 55 [1280/3276 (38%)]\tLoss: 16.944561\n",
      "Train Epoch: 55 [2560/3276 (77%)]\tLoss: 19.317867\n",
      "\n",
      "Test set: Average loss: 17.7318, Accuracy: 3294059/9486336 (35%)\n",
      "\n",
      "Train Epoch: 56 [0/3276 (0%)]\tLoss: 18.462803\n",
      "Train Epoch: 56 [1280/3276 (38%)]\tLoss: 18.121529\n",
      "Train Epoch: 56 [2560/3276 (77%)]\tLoss: 18.562725\n",
      "\n",
      "Test set: Average loss: 17.7372, Accuracy: 3294053/9486336 (35%)\n",
      "\n",
      "Train Epoch: 57 [0/3276 (0%)]\tLoss: 17.079243\n",
      "Train Epoch: 57 [1280/3276 (38%)]\tLoss: 18.133915\n",
      "Train Epoch: 57 [2560/3276 (77%)]\tLoss: 19.332647\n",
      "\n",
      "Test set: Average loss: 17.7423, Accuracy: 3294043/9486336 (35%)\n",
      "\n",
      "Train Epoch: 58 [0/3276 (0%)]\tLoss: 18.005112\n",
      "Train Epoch: 58 [1280/3276 (38%)]\tLoss: 18.216183\n",
      "Train Epoch: 58 [2560/3276 (77%)]\tLoss: 17.922159\n",
      "\n",
      "Test set: Average loss: 17.7473, Accuracy: 3294048/9486336 (35%)\n",
      "\n",
      "Train Epoch: 59 [0/3276 (0%)]\tLoss: 17.374214\n",
      "Train Epoch: 59 [1280/3276 (38%)]\tLoss: 19.636713\n",
      "Train Epoch: 59 [2560/3276 (77%)]\tLoss: 18.669447\n",
      "\n",
      "Test set: Average loss: 17.7521, Accuracy: 3294050/9486336 (35%)\n",
      "\n",
      "Train Epoch: 60 [0/3276 (0%)]\tLoss: 18.309465\n",
      "Train Epoch: 60 [1280/3276 (38%)]\tLoss: 17.702471\n",
      "Train Epoch: 60 [2560/3276 (77%)]\tLoss: 19.086845\n",
      "\n",
      "Test set: Average loss: 17.7568, Accuracy: 3294047/9486336 (35%)\n",
      "\n",
      "Train Epoch: 61 [0/3276 (0%)]\tLoss: 18.260189\n",
      "Train Epoch: 61 [1280/3276 (38%)]\tLoss: 18.091385\n",
      "Train Epoch: 61 [2560/3276 (77%)]\tLoss: 19.085054\n",
      "\n",
      "Test set: Average loss: 17.7613, Accuracy: 3294052/9486336 (35%)\n",
      "\n",
      "Train Epoch: 62 [0/3276 (0%)]\tLoss: 18.977516\n",
      "Train Epoch: 62 [1280/3276 (38%)]\tLoss: 17.437958\n",
      "Train Epoch: 62 [2560/3276 (77%)]\tLoss: 17.950563\n",
      "\n",
      "Test set: Average loss: 17.7657, Accuracy: 3294051/9486336 (35%)\n",
      "\n",
      "Train Epoch: 63 [0/3276 (0%)]\tLoss: 17.862289\n",
      "Train Epoch: 63 [1280/3276 (38%)]\tLoss: 18.282665\n",
      "Train Epoch: 63 [2560/3276 (77%)]\tLoss: 19.330832\n",
      "\n",
      "Test set: Average loss: 17.7699, Accuracy: 3294047/9486336 (35%)\n",
      "\n",
      "Train Epoch: 64 [0/3276 (0%)]\tLoss: 17.597013\n",
      "Train Epoch: 64 [1280/3276 (38%)]\tLoss: 19.259867\n",
      "Train Epoch: 64 [2560/3276 (77%)]\tLoss: 18.581856\n",
      "\n",
      "Test set: Average loss: 17.7740, Accuracy: 3294046/9486336 (35%)\n",
      "\n",
      "Train Epoch: 65 [0/3276 (0%)]\tLoss: 17.440067\n",
      "Train Epoch: 65 [1280/3276 (38%)]\tLoss: 19.257259\n",
      "Train Epoch: 65 [2560/3276 (77%)]\tLoss: 17.852407\n",
      "\n",
      "Test set: Average loss: 17.7780, Accuracy: 3294046/9486336 (35%)\n",
      "\n",
      "Train Epoch: 66 [0/3276 (0%)]\tLoss: 17.096870\n",
      "Train Epoch: 66 [1280/3276 (38%)]\tLoss: 18.720831\n",
      "Train Epoch: 66 [2560/3276 (77%)]\tLoss: 18.129066\n",
      "\n",
      "Test set: Average loss: 17.7818, Accuracy: 3294049/9486336 (35%)\n",
      "\n",
      "Train Epoch: 67 [0/3276 (0%)]\tLoss: 17.747135\n",
      "Train Epoch: 67 [1280/3276 (38%)]\tLoss: 18.149778\n",
      "Train Epoch: 67 [2560/3276 (77%)]\tLoss: 18.767260\n",
      "\n",
      "Test set: Average loss: 17.7856, Accuracy: 3294049/9486336 (35%)\n",
      "\n",
      "Train Epoch: 68 [0/3276 (0%)]\tLoss: 16.465158\n",
      "Train Epoch: 68 [1280/3276 (38%)]\tLoss: 18.609207\n",
      "Train Epoch: 68 [2560/3276 (77%)]\tLoss: 18.379770\n",
      "\n",
      "Test set: Average loss: 17.7892, Accuracy: 3294051/9486336 (35%)\n",
      "\n",
      "Train Epoch: 69 [0/3276 (0%)]\tLoss: 18.426016\n",
      "Train Epoch: 69 [1280/3276 (38%)]\tLoss: 18.519062\n",
      "Train Epoch: 69 [2560/3276 (77%)]\tLoss: 17.458328\n",
      "\n",
      "Test set: Average loss: 17.7928, Accuracy: 3294049/9486336 (35%)\n",
      "\n",
      "Train Epoch: 70 [0/3276 (0%)]\tLoss: 17.889088\n",
      "Train Epoch: 70 [1280/3276 (38%)]\tLoss: 18.695139\n",
      "Train Epoch: 70 [2560/3276 (77%)]\tLoss: 18.925129\n",
      "\n",
      "Test set: Average loss: 17.7962, Accuracy: 3294050/9486336 (35%)\n",
      "\n",
      "Train Epoch: 71 [0/3276 (0%)]\tLoss: 18.786314\n",
      "Train Epoch: 71 [1280/3276 (38%)]\tLoss: 18.796062\n",
      "Train Epoch: 71 [2560/3276 (77%)]\tLoss: 18.506201\n",
      "\n",
      "Test set: Average loss: 17.7995, Accuracy: 3294039/9486336 (35%)\n",
      "\n",
      "Train Epoch: 72 [0/3276 (0%)]\tLoss: 18.200241\n",
      "Train Epoch: 72 [1280/3276 (38%)]\tLoss: 17.214951\n",
      "Train Epoch: 72 [2560/3276 (77%)]\tLoss: 17.355637\n",
      "\n",
      "Test set: Average loss: 17.8028, Accuracy: 3294043/9486336 (35%)\n",
      "\n",
      "Train Epoch: 73 [0/3276 (0%)]\tLoss: 17.037556\n",
      "Train Epoch: 73 [1280/3276 (38%)]\tLoss: 18.296711\n",
      "Train Epoch: 73 [2560/3276 (77%)]\tLoss: 17.451504\n",
      "\n",
      "Test set: Average loss: 17.8059, Accuracy: 3294042/9486336 (35%)\n",
      "\n",
      "Train Epoch: 74 [0/3276 (0%)]\tLoss: 18.761465\n",
      "Train Epoch: 74 [1280/3276 (38%)]\tLoss: 17.763893\n",
      "Train Epoch: 74 [2560/3276 (77%)]\tLoss: 17.257929\n",
      "\n",
      "Test set: Average loss: 17.8090, Accuracy: 3294042/9486336 (35%)\n",
      "\n",
      "Train Epoch: 75 [0/3276 (0%)]\tLoss: 17.585497\n",
      "Train Epoch: 75 [1280/3276 (38%)]\tLoss: 18.089487\n",
      "Train Epoch: 75 [2560/3276 (77%)]\tLoss: 19.250961\n",
      "\n",
      "Test set: Average loss: 17.8127, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 76 [0/3276 (0%)]\tLoss: 16.727219\n",
      "Train Epoch: 76 [1280/3276 (38%)]\tLoss: 16.931808\n",
      "Train Epoch: 76 [2560/3276 (77%)]\tLoss: 18.512501\n",
      "\n",
      "Test set: Average loss: 17.8163, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 77 [0/3276 (0%)]\tLoss: 17.738913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 77 [1280/3276 (38%)]\tLoss: 17.949009\n",
      "Train Epoch: 77 [2560/3276 (77%)]\tLoss: 19.561823\n",
      "\n",
      "Test set: Average loss: 17.8197, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 78 [0/3276 (0%)]\tLoss: 18.293497\n",
      "Train Epoch: 78 [1280/3276 (38%)]\tLoss: 18.746655\n",
      "Train Epoch: 78 [2560/3276 (77%)]\tLoss: 19.239763\n",
      "\n",
      "Test set: Average loss: 17.8231, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 79 [0/3276 (0%)]\tLoss: 17.805872\n",
      "Train Epoch: 79 [1280/3276 (38%)]\tLoss: 17.577225\n",
      "Train Epoch: 79 [2560/3276 (77%)]\tLoss: 18.885708\n",
      "\n",
      "Test set: Average loss: 17.8265, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 80 [0/3276 (0%)]\tLoss: 19.089508\n",
      "Train Epoch: 80 [1280/3276 (38%)]\tLoss: 18.602463\n",
      "Train Epoch: 80 [2560/3276 (77%)]\tLoss: 17.294268\n",
      "\n",
      "Test set: Average loss: 17.8297, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 81 [0/3276 (0%)]\tLoss: 18.250912\n",
      "Train Epoch: 81 [1280/3276 (38%)]\tLoss: 18.195312\n",
      "Train Epoch: 81 [2560/3276 (77%)]\tLoss: 18.040922\n",
      "\n",
      "Test set: Average loss: 17.8328, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 82 [0/3276 (0%)]\tLoss: 18.899307\n",
      "Train Epoch: 82 [1280/3276 (38%)]\tLoss: 18.032490\n",
      "Train Epoch: 82 [2560/3276 (77%)]\tLoss: 18.051859\n",
      "\n",
      "Test set: Average loss: 17.8359, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 83 [0/3276 (0%)]\tLoss: 17.473980\n",
      "Train Epoch: 83 [1280/3276 (38%)]\tLoss: 17.403543\n",
      "Train Epoch: 83 [2560/3276 (77%)]\tLoss: 18.303221\n",
      "\n",
      "Test set: Average loss: 17.8389, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 84 [0/3276 (0%)]\tLoss: 19.169140\n",
      "Train Epoch: 84 [1280/3276 (38%)]\tLoss: 17.016342\n",
      "Train Epoch: 84 [2560/3276 (77%)]\tLoss: 17.700653\n",
      "\n",
      "Test set: Average loss: 17.8419, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 85 [0/3276 (0%)]\tLoss: 17.906479\n",
      "Train Epoch: 85 [1280/3276 (38%)]\tLoss: 18.121635\n",
      "Train Epoch: 85 [2560/3276 (77%)]\tLoss: 18.416136\n",
      "\n",
      "Test set: Average loss: 17.8447, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 86 [0/3276 (0%)]\tLoss: 18.691582\n",
      "Train Epoch: 86 [1280/3276 (38%)]\tLoss: 18.367701\n",
      "Train Epoch: 86 [2560/3276 (77%)]\tLoss: 18.035862\n",
      "\n",
      "Test set: Average loss: 17.8475, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 87 [0/3276 (0%)]\tLoss: 18.278080\n",
      "Train Epoch: 87 [1280/3276 (38%)]\tLoss: 18.133730\n",
      "Train Epoch: 87 [2560/3276 (77%)]\tLoss: 18.102373\n",
      "\n",
      "Test set: Average loss: 17.8503, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 88 [0/3276 (0%)]\tLoss: 18.002897\n",
      "Train Epoch: 88 [1280/3276 (38%)]\tLoss: 18.788975\n",
      "Train Epoch: 88 [2560/3276 (77%)]\tLoss: 17.507130\n",
      "\n",
      "Test set: Average loss: 17.8529, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 89 [0/3276 (0%)]\tLoss: 17.532454\n",
      "Train Epoch: 89 [1280/3276 (38%)]\tLoss: 18.031410\n",
      "Train Epoch: 89 [2560/3276 (77%)]\tLoss: 17.199562\n",
      "\n",
      "Test set: Average loss: 17.8555, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 90 [0/3276 (0%)]\tLoss: 17.927849\n",
      "Train Epoch: 90 [1280/3276 (38%)]\tLoss: 19.010429\n",
      "Train Epoch: 90 [2560/3276 (77%)]\tLoss: 18.516663\n",
      "\n",
      "Test set: Average loss: 17.8581, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 91 [0/3276 (0%)]\tLoss: 19.827415\n",
      "Train Epoch: 91 [1280/3276 (38%)]\tLoss: 17.817623\n",
      "Train Epoch: 91 [2560/3276 (77%)]\tLoss: 18.418005\n",
      "\n",
      "Test set: Average loss: 17.8606, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 92 [0/3276 (0%)]\tLoss: 18.861519\n",
      "Train Epoch: 92 [1280/3276 (38%)]\tLoss: 18.316160\n",
      "Train Epoch: 92 [2560/3276 (77%)]\tLoss: 18.435555\n",
      "\n",
      "Test set: Average loss: 17.8630, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 93 [0/3276 (0%)]\tLoss: 19.148455\n",
      "Train Epoch: 93 [1280/3276 (38%)]\tLoss: 17.216846\n",
      "Train Epoch: 93 [2560/3276 (77%)]\tLoss: 19.302135\n",
      "\n",
      "Test set: Average loss: 17.8654, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 94 [0/3276 (0%)]\tLoss: 18.363354\n",
      "Train Epoch: 94 [1280/3276 (38%)]\tLoss: 17.793327\n",
      "Train Epoch: 94 [2560/3276 (77%)]\tLoss: 18.609339\n",
      "\n",
      "Test set: Average loss: 17.8678, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 95 [0/3276 (0%)]\tLoss: 17.980896\n",
      "Train Epoch: 95 [1280/3276 (38%)]\tLoss: 17.738096\n",
      "Train Epoch: 95 [2560/3276 (77%)]\tLoss: 15.390961\n",
      "\n",
      "Test set: Average loss: 17.8701, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 96 [0/3276 (0%)]\tLoss: 17.194817\n",
      "Train Epoch: 96 [1280/3276 (38%)]\tLoss: 16.613173\n",
      "Train Epoch: 96 [2560/3276 (77%)]\tLoss: 18.647102\n",
      "\n",
      "Test set: Average loss: 17.8723, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 97 [0/3276 (0%)]\tLoss: 16.115690\n",
      "Train Epoch: 97 [1280/3276 (38%)]\tLoss: 17.145700\n",
      "Train Epoch: 97 [2560/3276 (77%)]\tLoss: 18.657034\n",
      "\n",
      "Test set: Average loss: 17.8745, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 98 [0/3276 (0%)]\tLoss: 17.219955\n",
      "Train Epoch: 98 [1280/3276 (38%)]\tLoss: 18.476320\n",
      "Train Epoch: 98 [2560/3276 (77%)]\tLoss: 18.279900\n",
      "\n",
      "Test set: Average loss: 17.8767, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 99 [0/3276 (0%)]\tLoss: 18.349888\n",
      "Train Epoch: 99 [1280/3276 (38%)]\tLoss: 16.038036\n",
      "Train Epoch: 99 [2560/3276 (77%)]\tLoss: 17.862209\n",
      "\n",
      "Test set: Average loss: 17.8788, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 100 [0/3276 (0%)]\tLoss: 17.279985\n",
      "Train Epoch: 100 [1280/3276 (38%)]\tLoss: 18.502882\n",
      "Train Epoch: 100 [2560/3276 (77%)]\tLoss: 17.823212\n",
      "\n",
      "Test set: Average loss: 17.8809, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 101 [0/3276 (0%)]\tLoss: 17.098454\n",
      "Train Epoch: 101 [1280/3276 (38%)]\tLoss: 17.139481\n",
      "Train Epoch: 101 [2560/3276 (77%)]\tLoss: 18.413261\n",
      "\n",
      "Test set: Average loss: 17.8829, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 102 [0/3276 (0%)]\tLoss: 18.488600\n",
      "Train Epoch: 102 [1280/3276 (38%)]\tLoss: 18.714031\n",
      "Train Epoch: 102 [2560/3276 (77%)]\tLoss: 17.362490\n",
      "\n",
      "Test set: Average loss: 17.8849, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 103 [0/3276 (0%)]\tLoss: 17.536457\n",
      "Train Epoch: 103 [1280/3276 (38%)]\tLoss: 19.237839\n",
      "Train Epoch: 103 [2560/3276 (77%)]\tLoss: 18.969006\n",
      "\n",
      "Test set: Average loss: 17.8869, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 104 [0/3276 (0%)]\tLoss: 17.980974\n",
      "Train Epoch: 104 [1280/3276 (38%)]\tLoss: 17.993622\n",
      "Train Epoch: 104 [2560/3276 (77%)]\tLoss: 16.969543\n",
      "\n",
      "Test set: Average loss: 17.8888, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 105 [0/3276 (0%)]\tLoss: 18.264774\n",
      "Train Epoch: 105 [1280/3276 (38%)]\tLoss: 17.040455\n",
      "Train Epoch: 105 [2560/3276 (77%)]\tLoss: 19.110695\n",
      "\n",
      "Test set: Average loss: 17.8907, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 106 [0/3276 (0%)]\tLoss: 17.394585\n",
      "Train Epoch: 106 [1280/3276 (38%)]\tLoss: 17.915676\n",
      "Train Epoch: 106 [2560/3276 (77%)]\tLoss: 17.628160\n",
      "\n",
      "Test set: Average loss: 17.8925, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 107 [0/3276 (0%)]\tLoss: 18.762280\n",
      "Train Epoch: 107 [1280/3276 (38%)]\tLoss: 16.579048\n",
      "Train Epoch: 107 [2560/3276 (77%)]\tLoss: 16.561760\n",
      "\n",
      "Test set: Average loss: 17.8943, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 108 [0/3276 (0%)]\tLoss: 18.143322\n",
      "Train Epoch: 108 [1280/3276 (38%)]\tLoss: 18.212996\n",
      "Train Epoch: 108 [2560/3276 (77%)]\tLoss: 17.029150\n",
      "\n",
      "Test set: Average loss: 17.8961, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 109 [0/3276 (0%)]\tLoss: 19.759666\n",
      "Train Epoch: 109 [1280/3276 (38%)]\tLoss: 18.573713\n",
      "Train Epoch: 109 [2560/3276 (77%)]\tLoss: 18.291309\n",
      "\n",
      "Test set: Average loss: 17.8978, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 110 [0/3276 (0%)]\tLoss: 18.297554\n",
      "Train Epoch: 110 [1280/3276 (38%)]\tLoss: 18.679327\n",
      "Train Epoch: 110 [2560/3276 (77%)]\tLoss: 19.111511\n",
      "\n",
      "Test set: Average loss: 17.8996, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 111 [0/3276 (0%)]\tLoss: 18.544542\n",
      "Train Epoch: 111 [1280/3276 (38%)]\tLoss: 17.115053\n",
      "Train Epoch: 111 [2560/3276 (77%)]\tLoss: 17.712799\n",
      "\n",
      "Test set: Average loss: 17.9012, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 112 [0/3276 (0%)]\tLoss: 19.748680\n",
      "Train Epoch: 112 [1280/3276 (38%)]\tLoss: 17.977127\n",
      "Train Epoch: 112 [2560/3276 (77%)]\tLoss: 18.302799\n",
      "\n",
      "Test set: Average loss: 17.9029, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 113 [0/3276 (0%)]\tLoss: 18.919781\n",
      "Train Epoch: 113 [1280/3276 (38%)]\tLoss: 17.235741\n",
      "Train Epoch: 113 [2560/3276 (77%)]\tLoss: 17.481255\n",
      "\n",
      "Test set: Average loss: 17.9045, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 114 [0/3276 (0%)]\tLoss: 17.254475\n",
      "Train Epoch: 114 [1280/3276 (38%)]\tLoss: 19.330355\n",
      "Train Epoch: 114 [2560/3276 (77%)]\tLoss: 19.210592\n",
      "\n",
      "Test set: Average loss: 17.9061, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 115 [0/3276 (0%)]\tLoss: 17.489159\n",
      "Train Epoch: 115 [1280/3276 (38%)]\tLoss: 20.526875\n",
      "Train Epoch: 115 [2560/3276 (77%)]\tLoss: 17.634195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 17.9077, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 116 [0/3276 (0%)]\tLoss: 17.506893\n",
      "Train Epoch: 116 [1280/3276 (38%)]\tLoss: 19.150879\n",
      "Train Epoch: 116 [2560/3276 (77%)]\tLoss: 18.207672\n",
      "\n",
      "Test set: Average loss: 17.9092, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 117 [0/3276 (0%)]\tLoss: 18.505621\n",
      "Train Epoch: 117 [1280/3276 (38%)]\tLoss: 18.221981\n",
      "Train Epoch: 117 [2560/3276 (77%)]\tLoss: 17.683235\n",
      "\n",
      "Test set: Average loss: 17.9108, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 118 [0/3276 (0%)]\tLoss: 17.495537\n",
      "Train Epoch: 118 [1280/3276 (38%)]\tLoss: 17.624050\n",
      "Train Epoch: 118 [2560/3276 (77%)]\tLoss: 17.993254\n",
      "\n",
      "Test set: Average loss: 17.9122, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 119 [0/3276 (0%)]\tLoss: 18.790424\n",
      "Train Epoch: 119 [1280/3276 (38%)]\tLoss: 17.365547\n",
      "Train Epoch: 119 [2560/3276 (77%)]\tLoss: 16.574093\n",
      "\n",
      "Test set: Average loss: 17.9137, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 120 [0/3276 (0%)]\tLoss: 18.231491\n",
      "Train Epoch: 120 [1280/3276 (38%)]\tLoss: 17.929169\n",
      "Train Epoch: 120 [2560/3276 (77%)]\tLoss: 18.086666\n",
      "\n",
      "Test set: Average loss: 17.9152, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 121 [0/3276 (0%)]\tLoss: 17.529501\n",
      "Train Epoch: 121 [1280/3276 (38%)]\tLoss: 17.250235\n",
      "Train Epoch: 121 [2560/3276 (77%)]\tLoss: 17.401278\n",
      "\n",
      "Test set: Average loss: 17.9166, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 122 [0/3276 (0%)]\tLoss: 18.890793\n",
      "Train Epoch: 122 [1280/3276 (38%)]\tLoss: 19.213673\n",
      "Train Epoch: 122 [2560/3276 (77%)]\tLoss: 17.519884\n",
      "\n",
      "Test set: Average loss: 17.9180, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 123 [0/3276 (0%)]\tLoss: 17.004248\n",
      "Train Epoch: 123 [1280/3276 (38%)]\tLoss: 17.392239\n",
      "Train Epoch: 123 [2560/3276 (77%)]\tLoss: 17.820759\n",
      "\n",
      "Test set: Average loss: 17.9193, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 124 [0/3276 (0%)]\tLoss: 18.322958\n",
      "Train Epoch: 124 [1280/3276 (38%)]\tLoss: 18.763071\n",
      "Train Epoch: 124 [2560/3276 (77%)]\tLoss: 18.427994\n",
      "\n",
      "Test set: Average loss: 17.9207, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 125 [0/3276 (0%)]\tLoss: 18.008669\n",
      "Train Epoch: 125 [1280/3276 (38%)]\tLoss: 17.568159\n",
      "Train Epoch: 125 [2560/3276 (77%)]\tLoss: 19.116440\n",
      "\n",
      "Test set: Average loss: 17.9220, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 126 [0/3276 (0%)]\tLoss: 19.697372\n",
      "Train Epoch: 126 [1280/3276 (38%)]\tLoss: 18.452682\n",
      "Train Epoch: 126 [2560/3276 (77%)]\tLoss: 18.144428\n",
      "\n",
      "Test set: Average loss: 17.9233, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 127 [0/3276 (0%)]\tLoss: 17.917601\n",
      "Train Epoch: 127 [1280/3276 (38%)]\tLoss: 17.926876\n",
      "Train Epoch: 127 [2560/3276 (77%)]\tLoss: 18.098869\n",
      "\n",
      "Test set: Average loss: 17.9246, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 128 [0/3276 (0%)]\tLoss: 16.757734\n",
      "Train Epoch: 128 [1280/3276 (38%)]\tLoss: 17.733932\n",
      "Train Epoch: 128 [2560/3276 (77%)]\tLoss: 17.719967\n",
      "\n",
      "Test set: Average loss: 17.9259, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 129 [0/3276 (0%)]\tLoss: 18.746418\n",
      "Train Epoch: 129 [1280/3276 (38%)]\tLoss: 17.957680\n",
      "Train Epoch: 129 [2560/3276 (77%)]\tLoss: 19.017941\n",
      "\n",
      "Test set: Average loss: 17.9271, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 130 [0/3276 (0%)]\tLoss: 18.663492\n",
      "Train Epoch: 130 [1280/3276 (38%)]\tLoss: 19.048428\n",
      "Train Epoch: 130 [2560/3276 (77%)]\tLoss: 17.896202\n",
      "\n",
      "Test set: Average loss: 17.9284, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 131 [0/3276 (0%)]\tLoss: 18.380245\n",
      "Train Epoch: 131 [1280/3276 (38%)]\tLoss: 20.461077\n",
      "Train Epoch: 131 [2560/3276 (77%)]\tLoss: 18.832901\n",
      "\n",
      "Test set: Average loss: 17.9296, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 132 [0/3276 (0%)]\tLoss: 17.467077\n",
      "Train Epoch: 132 [1280/3276 (38%)]\tLoss: 18.698250\n",
      "Train Epoch: 132 [2560/3276 (77%)]\tLoss: 17.666765\n",
      "\n",
      "Test set: Average loss: 17.9308, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 133 [0/3276 (0%)]\tLoss: 17.574509\n",
      "Train Epoch: 133 [1280/3276 (38%)]\tLoss: 17.869114\n",
      "Train Epoch: 133 [2560/3276 (77%)]\tLoss: 18.768841\n",
      "\n",
      "Test set: Average loss: 17.9319, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 134 [0/3276 (0%)]\tLoss: 19.161051\n",
      "Train Epoch: 134 [1280/3276 (38%)]\tLoss: 17.220589\n",
      "Train Epoch: 134 [2560/3276 (77%)]\tLoss: 16.861530\n",
      "\n",
      "Test set: Average loss: 17.9331, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 135 [0/3276 (0%)]\tLoss: 18.354790\n",
      "Train Epoch: 135 [1280/3276 (38%)]\tLoss: 18.012199\n",
      "Train Epoch: 135 [2560/3276 (77%)]\tLoss: 19.703064\n",
      "\n",
      "Test set: Average loss: 17.9342, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 136 [0/3276 (0%)]\tLoss: 18.548864\n",
      "Train Epoch: 136 [1280/3276 (38%)]\tLoss: 17.763947\n",
      "Train Epoch: 136 [2560/3276 (77%)]\tLoss: 17.432926\n",
      "\n",
      "Test set: Average loss: 17.9354, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 137 [0/3276 (0%)]\tLoss: 17.031336\n",
      "Train Epoch: 137 [1280/3276 (38%)]\tLoss: 18.950794\n",
      "Train Epoch: 137 [2560/3276 (77%)]\tLoss: 18.357660\n",
      "\n",
      "Test set: Average loss: 17.9365, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 138 [0/3276 (0%)]\tLoss: 18.298634\n",
      "Train Epoch: 138 [1280/3276 (38%)]\tLoss: 18.572371\n",
      "Train Epoch: 138 [2560/3276 (77%)]\tLoss: 18.445490\n",
      "\n",
      "Test set: Average loss: 17.9376, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 139 [0/3276 (0%)]\tLoss: 18.395763\n",
      "Train Epoch: 139 [1280/3276 (38%)]\tLoss: 19.339130\n",
      "Train Epoch: 139 [2560/3276 (77%)]\tLoss: 17.564312\n",
      "\n",
      "Test set: Average loss: 17.9386, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 140 [0/3276 (0%)]\tLoss: 17.789139\n",
      "Train Epoch: 140 [1280/3276 (38%)]\tLoss: 19.100286\n",
      "Train Epoch: 140 [2560/3276 (77%)]\tLoss: 18.107933\n",
      "\n",
      "Test set: Average loss: 17.9397, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 141 [0/3276 (0%)]\tLoss: 18.183086\n",
      "Train Epoch: 141 [1280/3276 (38%)]\tLoss: 18.524332\n",
      "Train Epoch: 141 [2560/3276 (77%)]\tLoss: 17.965506\n",
      "\n",
      "Test set: Average loss: 17.9408, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 142 [0/3276 (0%)]\tLoss: 18.580854\n",
      "Train Epoch: 142 [1280/3276 (38%)]\tLoss: 18.425278\n",
      "Train Epoch: 142 [2560/3276 (77%)]\tLoss: 17.977573\n",
      "\n",
      "Test set: Average loss: 17.9418, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 143 [0/3276 (0%)]\tLoss: 17.895861\n",
      "Train Epoch: 143 [1280/3276 (38%)]\tLoss: 18.153177\n",
      "Train Epoch: 143 [2560/3276 (77%)]\tLoss: 17.116398\n",
      "\n",
      "Test set: Average loss: 17.9428, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 144 [0/3276 (0%)]\tLoss: 18.163033\n",
      "Train Epoch: 144 [1280/3276 (38%)]\tLoss: 18.172625\n",
      "Train Epoch: 144 [2560/3276 (77%)]\tLoss: 18.779697\n",
      "\n",
      "Test set: Average loss: 17.9438, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 145 [0/3276 (0%)]\tLoss: 19.149326\n",
      "Train Epoch: 145 [1280/3276 (38%)]\tLoss: 18.966898\n",
      "Train Epoch: 145 [2560/3276 (77%)]\tLoss: 18.570580\n",
      "\n",
      "Test set: Average loss: 17.9448, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 146 [0/3276 (0%)]\tLoss: 18.281296\n",
      "Train Epoch: 146 [1280/3276 (38%)]\tLoss: 18.783257\n",
      "Train Epoch: 146 [2560/3276 (77%)]\tLoss: 18.591209\n",
      "\n",
      "Test set: Average loss: 17.9458, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 147 [0/3276 (0%)]\tLoss: 18.112280\n",
      "Train Epoch: 147 [1280/3276 (38%)]\tLoss: 18.345856\n",
      "Train Epoch: 147 [2560/3276 (77%)]\tLoss: 18.216024\n",
      "\n",
      "Test set: Average loss: 17.9467, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 148 [0/3276 (0%)]\tLoss: 18.285854\n",
      "Train Epoch: 148 [1280/3276 (38%)]\tLoss: 19.125610\n",
      "Train Epoch: 148 [2560/3276 (77%)]\tLoss: 18.201532\n",
      "\n",
      "Test set: Average loss: 17.9477, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 149 [0/3276 (0%)]\tLoss: 18.404566\n",
      "Train Epoch: 149 [1280/3276 (38%)]\tLoss: 16.693913\n",
      "Train Epoch: 149 [2560/3276 (77%)]\tLoss: 18.924919\n",
      "\n",
      "Test set: Average loss: 17.9486, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 150 [0/3276 (0%)]\tLoss: 17.593298\n",
      "Train Epoch: 150 [1280/3276 (38%)]\tLoss: 17.940762\n",
      "Train Epoch: 150 [2560/3276 (77%)]\tLoss: 18.385540\n",
      "\n",
      "Test set: Average loss: 17.9495, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 151 [0/3276 (0%)]\tLoss: 19.820669\n",
      "Train Epoch: 151 [1280/3276 (38%)]\tLoss: 17.751904\n",
      "Train Epoch: 151 [2560/3276 (77%)]\tLoss: 19.195621\n",
      "\n",
      "Test set: Average loss: 17.9505, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 152 [0/3276 (0%)]\tLoss: 17.669373\n",
      "Train Epoch: 152 [1280/3276 (38%)]\tLoss: 17.970301\n",
      "Train Epoch: 152 [2560/3276 (77%)]\tLoss: 19.123133\n",
      "\n",
      "Test set: Average loss: 17.9514, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 153 [0/3276 (0%)]\tLoss: 18.305275\n",
      "Train Epoch: 153 [1280/3276 (38%)]\tLoss: 19.509357\n",
      "Train Epoch: 153 [2560/3276 (77%)]\tLoss: 18.131096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 17.9522, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 154 [0/3276 (0%)]\tLoss: 17.872065\n",
      "Train Epoch: 154 [1280/3276 (38%)]\tLoss: 18.740252\n",
      "Train Epoch: 154 [2560/3276 (77%)]\tLoss: 17.552113\n",
      "\n",
      "Test set: Average loss: 17.9531, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 155 [0/3276 (0%)]\tLoss: 18.715851\n",
      "Train Epoch: 155 [1280/3276 (38%)]\tLoss: 18.217842\n",
      "Train Epoch: 155 [2560/3276 (77%)]\tLoss: 17.385653\n",
      "\n",
      "Test set: Average loss: 17.9540, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 156 [0/3276 (0%)]\tLoss: 18.275843\n",
      "Train Epoch: 156 [1280/3276 (38%)]\tLoss: 18.940414\n",
      "Train Epoch: 156 [2560/3276 (77%)]\tLoss: 18.485411\n",
      "\n",
      "Test set: Average loss: 17.9548, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 157 [0/3276 (0%)]\tLoss: 19.122526\n",
      "Train Epoch: 157 [1280/3276 (38%)]\tLoss: 20.054796\n",
      "Train Epoch: 157 [2560/3276 (77%)]\tLoss: 19.524139\n",
      "\n",
      "Test set: Average loss: 17.9557, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 158 [0/3276 (0%)]\tLoss: 18.926607\n",
      "Train Epoch: 158 [1280/3276 (38%)]\tLoss: 17.788927\n",
      "Train Epoch: 158 [2560/3276 (77%)]\tLoss: 18.832348\n",
      "\n",
      "Test set: Average loss: 17.9565, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 159 [0/3276 (0%)]\tLoss: 16.687508\n",
      "Train Epoch: 159 [1280/3276 (38%)]\tLoss: 18.410231\n",
      "Train Epoch: 159 [2560/3276 (77%)]\tLoss: 16.932915\n",
      "\n",
      "Test set: Average loss: 17.9573, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 160 [0/3276 (0%)]\tLoss: 19.061787\n",
      "Train Epoch: 160 [1280/3276 (38%)]\tLoss: 18.261086\n",
      "Train Epoch: 160 [2560/3276 (77%)]\tLoss: 17.860207\n",
      "\n",
      "Test set: Average loss: 17.9582, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 161 [0/3276 (0%)]\tLoss: 18.555111\n",
      "Train Epoch: 161 [1280/3276 (38%)]\tLoss: 17.985718\n",
      "Train Epoch: 161 [2560/3276 (77%)]\tLoss: 18.232071\n",
      "\n",
      "Test set: Average loss: 17.9590, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 162 [0/3276 (0%)]\tLoss: 17.747318\n",
      "Train Epoch: 162 [1280/3276 (38%)]\tLoss: 16.773174\n",
      "Train Epoch: 162 [2560/3276 (77%)]\tLoss: 18.212152\n",
      "\n",
      "Test set: Average loss: 17.9598, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 163 [0/3276 (0%)]\tLoss: 18.010828\n",
      "Train Epoch: 163 [1280/3276 (38%)]\tLoss: 17.354610\n",
      "Train Epoch: 163 [2560/3276 (77%)]\tLoss: 16.360018\n",
      "\n",
      "Test set: Average loss: 17.9605, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 164 [0/3276 (0%)]\tLoss: 18.370073\n",
      "Train Epoch: 164 [1280/3276 (38%)]\tLoss: 19.255810\n",
      "Train Epoch: 164 [2560/3276 (77%)]\tLoss: 18.267408\n",
      "\n",
      "Test set: Average loss: 17.9613, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 165 [0/3276 (0%)]\tLoss: 19.512466\n",
      "Train Epoch: 165 [1280/3276 (38%)]\tLoss: 18.851715\n",
      "Train Epoch: 165 [2560/3276 (77%)]\tLoss: 17.392056\n",
      "\n",
      "Test set: Average loss: 17.9621, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 166 [0/3276 (0%)]\tLoss: 18.908951\n",
      "Train Epoch: 166 [1280/3276 (38%)]\tLoss: 17.962791\n",
      "Train Epoch: 166 [2560/3276 (77%)]\tLoss: 18.842255\n",
      "\n",
      "Test set: Average loss: 17.9628, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 167 [0/3276 (0%)]\tLoss: 19.081339\n",
      "Train Epoch: 167 [1280/3276 (38%)]\tLoss: 18.064058\n",
      "Train Epoch: 167 [2560/3276 (77%)]\tLoss: 18.936882\n",
      "\n",
      "Test set: Average loss: 17.9636, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 168 [0/3276 (0%)]\tLoss: 18.312679\n",
      "Train Epoch: 168 [1280/3276 (38%)]\tLoss: 17.293503\n",
      "Train Epoch: 168 [2560/3276 (77%)]\tLoss: 17.490292\n",
      "\n",
      "Test set: Average loss: 17.9643, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 169 [0/3276 (0%)]\tLoss: 17.606447\n",
      "Train Epoch: 169 [1280/3276 (38%)]\tLoss: 19.138760\n",
      "Train Epoch: 169 [2560/3276 (77%)]\tLoss: 18.270863\n",
      "\n",
      "Test set: Average loss: 17.9650, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 170 [0/3276 (0%)]\tLoss: 19.252884\n",
      "Train Epoch: 170 [1280/3276 (38%)]\tLoss: 18.193151\n",
      "Train Epoch: 170 [2560/3276 (77%)]\tLoss: 17.358431\n",
      "\n",
      "Test set: Average loss: 17.9658, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 171 [0/3276 (0%)]\tLoss: 18.737459\n",
      "Train Epoch: 171 [1280/3276 (38%)]\tLoss: 18.591370\n",
      "Train Epoch: 171 [2560/3276 (77%)]\tLoss: 18.461880\n",
      "\n",
      "Test set: Average loss: 17.9665, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 172 [0/3276 (0%)]\tLoss: 17.415192\n",
      "Train Epoch: 172 [1280/3276 (38%)]\tLoss: 18.535847\n",
      "Train Epoch: 172 [2560/3276 (77%)]\tLoss: 19.468647\n",
      "\n",
      "Test set: Average loss: 17.9672, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 173 [0/3276 (0%)]\tLoss: 18.156235\n",
      "Train Epoch: 173 [1280/3276 (38%)]\tLoss: 17.387310\n",
      "Train Epoch: 173 [2560/3276 (77%)]\tLoss: 19.871420\n",
      "\n",
      "Test set: Average loss: 17.9679, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 174 [0/3276 (0%)]\tLoss: 18.358452\n",
      "Train Epoch: 174 [1280/3276 (38%)]\tLoss: 17.557066\n",
      "Train Epoch: 174 [2560/3276 (77%)]\tLoss: 19.083052\n",
      "\n",
      "Test set: Average loss: 17.9686, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 175 [0/3276 (0%)]\tLoss: 18.030909\n",
      "Train Epoch: 175 [1280/3276 (38%)]\tLoss: 17.587368\n",
      "Train Epoch: 175 [2560/3276 (77%)]\tLoss: 17.560753\n",
      "\n",
      "Test set: Average loss: 17.9692, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 176 [0/3276 (0%)]\tLoss: 18.023741\n",
      "Train Epoch: 176 [1280/3276 (38%)]\tLoss: 17.307732\n",
      "Train Epoch: 176 [2560/3276 (77%)]\tLoss: 17.523809\n",
      "\n",
      "Test set: Average loss: 17.9699, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 177 [0/3276 (0%)]\tLoss: 18.285196\n",
      "Train Epoch: 177 [1280/3276 (38%)]\tLoss: 17.097555\n",
      "Train Epoch: 177 [2560/3276 (77%)]\tLoss: 17.956099\n",
      "\n",
      "Test set: Average loss: 17.9706, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 178 [0/3276 (0%)]\tLoss: 18.930164\n",
      "Train Epoch: 178 [1280/3276 (38%)]\tLoss: 18.059210\n",
      "Train Epoch: 178 [2560/3276 (77%)]\tLoss: 18.401932\n",
      "\n",
      "Test set: Average loss: 17.9712, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 179 [0/3276 (0%)]\tLoss: 17.842445\n",
      "Train Epoch: 179 [1280/3276 (38%)]\tLoss: 17.703655\n",
      "Train Epoch: 179 [2560/3276 (77%)]\tLoss: 18.485411\n",
      "\n",
      "Test set: Average loss: 17.9719, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 180 [0/3276 (0%)]\tLoss: 19.848944\n",
      "Train Epoch: 180 [1280/3276 (38%)]\tLoss: 17.790613\n",
      "Train Epoch: 180 [2560/3276 (77%)]\tLoss: 17.048807\n",
      "\n",
      "Test set: Average loss: 17.9725, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 181 [0/3276 (0%)]\tLoss: 17.203541\n",
      "Train Epoch: 181 [1280/3276 (38%)]\tLoss: 16.469585\n",
      "Train Epoch: 181 [2560/3276 (77%)]\tLoss: 17.691061\n",
      "\n",
      "Test set: Average loss: 17.9732, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 182 [0/3276 (0%)]\tLoss: 18.102583\n",
      "Train Epoch: 182 [1280/3276 (38%)]\tLoss: 17.888771\n",
      "Train Epoch: 182 [2560/3276 (77%)]\tLoss: 18.530918\n",
      "\n",
      "Test set: Average loss: 17.9738, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 183 [0/3276 (0%)]\tLoss: 18.691265\n",
      "Train Epoch: 183 [1280/3276 (38%)]\tLoss: 18.930532\n",
      "Train Epoch: 183 [2560/3276 (77%)]\tLoss: 18.829845\n",
      "\n",
      "Test set: Average loss: 17.9744, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 184 [0/3276 (0%)]\tLoss: 18.780542\n",
      "Train Epoch: 184 [1280/3276 (38%)]\tLoss: 18.907845\n",
      "Train Epoch: 184 [2560/3276 (77%)]\tLoss: 18.473499\n",
      "\n",
      "Test set: Average loss: 17.9750, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 185 [0/3276 (0%)]\tLoss: 17.983820\n",
      "Train Epoch: 185 [1280/3276 (38%)]\tLoss: 16.913836\n",
      "Train Epoch: 185 [2560/3276 (77%)]\tLoss: 17.481886\n",
      "\n",
      "Test set: Average loss: 17.9756, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 186 [0/3276 (0%)]\tLoss: 18.582594\n",
      "Train Epoch: 186 [1280/3276 (38%)]\tLoss: 18.396791\n",
      "Train Epoch: 186 [2560/3276 (77%)]\tLoss: 18.024822\n",
      "\n",
      "Test set: Average loss: 17.9762, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 187 [0/3276 (0%)]\tLoss: 19.175465\n",
      "Train Epoch: 187 [1280/3276 (38%)]\tLoss: 19.462875\n",
      "Train Epoch: 187 [2560/3276 (77%)]\tLoss: 18.487755\n",
      "\n",
      "Test set: Average loss: 17.9768, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 188 [0/3276 (0%)]\tLoss: 18.869740\n",
      "Train Epoch: 188 [1280/3276 (38%)]\tLoss: 17.383438\n",
      "Train Epoch: 188 [2560/3276 (77%)]\tLoss: 17.827847\n",
      "\n",
      "Test set: Average loss: 17.9774, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 189 [0/3276 (0%)]\tLoss: 18.890532\n",
      "Train Epoch: 189 [1280/3276 (38%)]\tLoss: 17.938442\n",
      "Train Epoch: 189 [2560/3276 (77%)]\tLoss: 17.394743\n",
      "\n",
      "Test set: Average loss: 17.9780, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 190 [0/3276 (0%)]\tLoss: 18.553396\n",
      "Train Epoch: 190 [1280/3276 (38%)]\tLoss: 17.187334\n",
      "Train Epoch: 190 [2560/3276 (77%)]\tLoss: 19.267088\n",
      "\n",
      "Test set: Average loss: 17.9786, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 191 [0/3276 (0%)]\tLoss: 17.305862\n",
      "Train Epoch: 191 [1280/3276 (38%)]\tLoss: 18.293945\n",
      "Train Epoch: 191 [2560/3276 (77%)]\tLoss: 18.109489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 17.9792, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 192 [0/3276 (0%)]\tLoss: 18.122110\n",
      "Train Epoch: 192 [1280/3276 (38%)]\tLoss: 18.458269\n",
      "Train Epoch: 192 [2560/3276 (77%)]\tLoss: 19.608412\n",
      "\n",
      "Test set: Average loss: 17.9797, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 193 [0/3276 (0%)]\tLoss: 18.932299\n",
      "Train Epoch: 193 [1280/3276 (38%)]\tLoss: 18.125851\n",
      "Train Epoch: 193 [2560/3276 (77%)]\tLoss: 18.210859\n",
      "\n",
      "Test set: Average loss: 17.9803, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 194 [0/3276 (0%)]\tLoss: 16.985853\n",
      "Train Epoch: 194 [1280/3276 (38%)]\tLoss: 19.113119\n",
      "Train Epoch: 194 [2560/3276 (77%)]\tLoss: 18.050566\n",
      "\n",
      "Test set: Average loss: 17.9808, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 195 [0/3276 (0%)]\tLoss: 18.274101\n",
      "Train Epoch: 195 [1280/3276 (38%)]\tLoss: 17.696226\n",
      "Train Epoch: 195 [2560/3276 (77%)]\tLoss: 18.186300\n",
      "\n",
      "Test set: Average loss: 17.9814, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 196 [0/3276 (0%)]\tLoss: 18.008274\n",
      "Train Epoch: 196 [1280/3276 (38%)]\tLoss: 18.020763\n",
      "Train Epoch: 196 [2560/3276 (77%)]\tLoss: 17.664682\n",
      "\n",
      "Test set: Average loss: 17.9819, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 197 [0/3276 (0%)]\tLoss: 18.323458\n",
      "Train Epoch: 197 [1280/3276 (38%)]\tLoss: 17.343147\n",
      "Train Epoch: 197 [2560/3276 (77%)]\tLoss: 18.139397\n",
      "\n",
      "Test set: Average loss: 17.9825, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 198 [0/3276 (0%)]\tLoss: 18.027746\n",
      "Train Epoch: 198 [1280/3276 (38%)]\tLoss: 17.293976\n",
      "Train Epoch: 198 [2560/3276 (77%)]\tLoss: 17.959656\n",
      "\n",
      "Test set: Average loss: 17.9830, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 199 [0/3276 (0%)]\tLoss: 18.970375\n",
      "Train Epoch: 199 [1280/3276 (38%)]\tLoss: 18.115681\n",
      "Train Epoch: 199 [2560/3276 (77%)]\tLoss: 18.703308\n",
      "\n",
      "Test set: Average loss: 17.9835, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 200 [0/3276 (0%)]\tLoss: 17.756832\n",
      "Train Epoch: 200 [1280/3276 (38%)]\tLoss: 16.671040\n",
      "Train Epoch: 200 [2560/3276 (77%)]\tLoss: 18.617378\n",
      "\n",
      "Test set: Average loss: 17.9840, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 201 [0/3276 (0%)]\tLoss: 16.894127\n",
      "Train Epoch: 201 [1280/3276 (38%)]\tLoss: 16.928963\n",
      "Train Epoch: 201 [2560/3276 (77%)]\tLoss: 18.825392\n",
      "\n",
      "Test set: Average loss: 17.9846, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 202 [0/3276 (0%)]\tLoss: 17.437117\n",
      "Train Epoch: 202 [1280/3276 (38%)]\tLoss: 19.069717\n",
      "Train Epoch: 202 [2560/3276 (77%)]\tLoss: 18.501328\n",
      "\n",
      "Test set: Average loss: 17.9851, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 203 [0/3276 (0%)]\tLoss: 18.869110\n",
      "Train Epoch: 203 [1280/3276 (38%)]\tLoss: 18.689737\n",
      "Train Epoch: 203 [2560/3276 (77%)]\tLoss: 17.624496\n",
      "\n",
      "Test set: Average loss: 17.9856, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 204 [0/3276 (0%)]\tLoss: 17.014208\n",
      "Train Epoch: 204 [1280/3276 (38%)]\tLoss: 18.511499\n",
      "Train Epoch: 204 [2560/3276 (77%)]\tLoss: 18.110489\n",
      "\n",
      "Test set: Average loss: 17.9861, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 205 [0/3276 (0%)]\tLoss: 17.747372\n",
      "Train Epoch: 205 [1280/3276 (38%)]\tLoss: 19.045921\n",
      "Train Epoch: 205 [2560/3276 (77%)]\tLoss: 18.308754\n",
      "\n",
      "Test set: Average loss: 17.9866, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 206 [0/3276 (0%)]\tLoss: 17.908876\n",
      "Train Epoch: 206 [1280/3276 (38%)]\tLoss: 19.732132\n",
      "Train Epoch: 206 [2560/3276 (77%)]\tLoss: 18.487282\n",
      "\n",
      "Test set: Average loss: 17.9871, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 207 [0/3276 (0%)]\tLoss: 19.491282\n",
      "Train Epoch: 207 [1280/3276 (38%)]\tLoss: 19.590336\n",
      "Train Epoch: 207 [2560/3276 (77%)]\tLoss: 18.563042\n",
      "\n",
      "Test set: Average loss: 17.9876, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 208 [0/3276 (0%)]\tLoss: 18.517506\n",
      "Train Epoch: 208 [1280/3276 (38%)]\tLoss: 18.463091\n",
      "Train Epoch: 208 [2560/3276 (77%)]\tLoss: 17.753853\n",
      "\n",
      "Test set: Average loss: 17.9880, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 209 [0/3276 (0%)]\tLoss: 17.372028\n",
      "Train Epoch: 209 [1280/3276 (38%)]\tLoss: 18.984077\n",
      "Train Epoch: 209 [2560/3276 (77%)]\tLoss: 18.577007\n",
      "\n",
      "Test set: Average loss: 17.9885, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 210 [0/3276 (0%)]\tLoss: 17.942394\n",
      "Train Epoch: 210 [1280/3276 (38%)]\tLoss: 18.033569\n",
      "Train Epoch: 210 [2560/3276 (77%)]\tLoss: 17.425362\n",
      "\n",
      "Test set: Average loss: 17.9890, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 211 [0/3276 (0%)]\tLoss: 17.476194\n",
      "Train Epoch: 211 [1280/3276 (38%)]\tLoss: 17.456799\n",
      "Train Epoch: 211 [2560/3276 (77%)]\tLoss: 18.859409\n",
      "\n",
      "Test set: Average loss: 17.9895, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 212 [0/3276 (0%)]\tLoss: 17.014946\n",
      "Train Epoch: 212 [1280/3276 (38%)]\tLoss: 18.634794\n",
      "Train Epoch: 212 [2560/3276 (77%)]\tLoss: 18.700779\n",
      "\n",
      "Test set: Average loss: 17.9899, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 213 [0/3276 (0%)]\tLoss: 20.432487\n",
      "Train Epoch: 213 [1280/3276 (38%)]\tLoss: 16.746351\n",
      "Train Epoch: 213 [2560/3276 (77%)]\tLoss: 17.510740\n",
      "\n",
      "Test set: Average loss: 17.9904, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 214 [0/3276 (0%)]\tLoss: 17.724974\n",
      "Train Epoch: 214 [1280/3276 (38%)]\tLoss: 16.972284\n",
      "Train Epoch: 214 [2560/3276 (77%)]\tLoss: 19.167534\n",
      "\n",
      "Test set: Average loss: 17.9908, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 215 [0/3276 (0%)]\tLoss: 17.446760\n",
      "Train Epoch: 215 [1280/3276 (38%)]\tLoss: 17.858837\n",
      "Train Epoch: 215 [2560/3276 (77%)]\tLoss: 17.294397\n",
      "\n",
      "Test set: Average loss: 17.9913, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 216 [0/3276 (0%)]\tLoss: 17.377457\n",
      "Train Epoch: 216 [1280/3276 (38%)]\tLoss: 17.440910\n",
      "Train Epoch: 216 [2560/3276 (77%)]\tLoss: 17.575590\n",
      "\n",
      "Test set: Average loss: 17.9917, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 217 [0/3276 (0%)]\tLoss: 17.476721\n",
      "Train Epoch: 217 [1280/3276 (38%)]\tLoss: 18.301113\n",
      "Train Epoch: 217 [2560/3276 (77%)]\tLoss: 19.574999\n",
      "\n",
      "Test set: Average loss: 17.9922, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 218 [0/3276 (0%)]\tLoss: 17.277323\n",
      "Train Epoch: 218 [1280/3276 (38%)]\tLoss: 19.078756\n",
      "Train Epoch: 218 [2560/3276 (77%)]\tLoss: 17.984268\n",
      "\n",
      "Test set: Average loss: 17.9926, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 219 [0/3276 (0%)]\tLoss: 19.210220\n",
      "Train Epoch: 219 [1280/3276 (38%)]\tLoss: 17.690638\n",
      "Train Epoch: 219 [2560/3276 (77%)]\tLoss: 17.770905\n",
      "\n",
      "Test set: Average loss: 17.9930, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 220 [0/3276 (0%)]\tLoss: 17.269550\n",
      "Train Epoch: 220 [1280/3276 (38%)]\tLoss: 18.185854\n",
      "Train Epoch: 220 [2560/3276 (77%)]\tLoss: 18.843889\n",
      "\n",
      "Test set: Average loss: 17.9935, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 221 [0/3276 (0%)]\tLoss: 17.633245\n",
      "Train Epoch: 221 [1280/3276 (38%)]\tLoss: 19.127584\n",
      "Train Epoch: 221 [2560/3276 (77%)]\tLoss: 19.112169\n",
      "\n",
      "Test set: Average loss: 17.9939, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 222 [0/3276 (0%)]\tLoss: 17.253185\n",
      "Train Epoch: 222 [1280/3276 (38%)]\tLoss: 18.529999\n",
      "Train Epoch: 222 [2560/3276 (77%)]\tLoss: 17.940262\n",
      "\n",
      "Test set: Average loss: 17.9943, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 223 [0/3276 (0%)]\tLoss: 18.616402\n",
      "Train Epoch: 223 [1280/3276 (38%)]\tLoss: 17.954359\n",
      "Train Epoch: 223 [2560/3276 (77%)]\tLoss: 18.054731\n",
      "\n",
      "Test set: Average loss: 17.9947, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 224 [0/3276 (0%)]\tLoss: 19.526356\n",
      "Train Epoch: 224 [1280/3276 (38%)]\tLoss: 18.249386\n",
      "Train Epoch: 224 [2560/3276 (77%)]\tLoss: 17.845161\n",
      "\n",
      "Test set: Average loss: 17.9952, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 225 [0/3276 (0%)]\tLoss: 19.167953\n",
      "Train Epoch: 225 [1280/3276 (38%)]\tLoss: 18.843365\n",
      "Train Epoch: 225 [2560/3276 (77%)]\tLoss: 17.498724\n",
      "\n",
      "Test set: Average loss: 17.9956, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 226 [0/3276 (0%)]\tLoss: 18.282377\n",
      "Train Epoch: 226 [1280/3276 (38%)]\tLoss: 17.558014\n",
      "Train Epoch: 226 [2560/3276 (77%)]\tLoss: 18.458139\n",
      "\n",
      "Test set: Average loss: 17.9960, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 227 [0/3276 (0%)]\tLoss: 18.036495\n",
      "Train Epoch: 227 [1280/3276 (38%)]\tLoss: 17.465757\n",
      "Train Epoch: 227 [2560/3276 (77%)]\tLoss: 18.756008\n",
      "\n",
      "Test set: Average loss: 17.9964, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 228 [0/3276 (0%)]\tLoss: 18.583174\n",
      "Train Epoch: 228 [1280/3276 (38%)]\tLoss: 20.125418\n",
      "Train Epoch: 228 [2560/3276 (77%)]\tLoss: 17.277481\n",
      "\n",
      "Test set: Average loss: 17.9968, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 229 [0/3276 (0%)]\tLoss: 19.205584\n",
      "Train Epoch: 229 [1280/3276 (38%)]\tLoss: 17.391476\n",
      "Train Epoch: 229 [2560/3276 (77%)]\tLoss: 17.999657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 17.9972, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 230 [0/3276 (0%)]\tLoss: 16.378252\n",
      "Train Epoch: 230 [1280/3276 (38%)]\tLoss: 18.388939\n",
      "Train Epoch: 230 [2560/3276 (77%)]\tLoss: 19.419371\n",
      "\n",
      "Test set: Average loss: 17.9976, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 231 [0/3276 (0%)]\tLoss: 17.672905\n",
      "Train Epoch: 231 [1280/3276 (38%)]\tLoss: 18.904945\n",
      "Train Epoch: 231 [2560/3276 (77%)]\tLoss: 17.352659\n",
      "\n",
      "Test set: Average loss: 17.9980, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 232 [0/3276 (0%)]\tLoss: 18.395975\n",
      "Train Epoch: 232 [1280/3276 (38%)]\tLoss: 18.915144\n",
      "Train Epoch: 232 [2560/3276 (77%)]\tLoss: 17.901657\n",
      "\n",
      "Test set: Average loss: 17.9984, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 233 [0/3276 (0%)]\tLoss: 18.194918\n",
      "Train Epoch: 233 [1280/3276 (38%)]\tLoss: 17.930696\n",
      "Train Epoch: 233 [2560/3276 (77%)]\tLoss: 17.559883\n",
      "\n",
      "Test set: Average loss: 17.9987, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 234 [0/3276 (0%)]\tLoss: 19.233490\n",
      "Train Epoch: 234 [1280/3276 (38%)]\tLoss: 18.941099\n",
      "Train Epoch: 234 [2560/3276 (77%)]\tLoss: 18.310545\n",
      "\n",
      "Test set: Average loss: 17.9991, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 235 [0/3276 (0%)]\tLoss: 17.415825\n",
      "Train Epoch: 235 [1280/3276 (38%)]\tLoss: 17.509871\n",
      "Train Epoch: 235 [2560/3276 (77%)]\tLoss: 19.191803\n",
      "\n",
      "Test set: Average loss: 17.9995, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 236 [0/3276 (0%)]\tLoss: 19.120577\n",
      "Train Epoch: 236 [1280/3276 (38%)]\tLoss: 17.698042\n",
      "Train Epoch: 236 [2560/3276 (77%)]\tLoss: 17.052258\n",
      "\n",
      "Test set: Average loss: 17.9999, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 237 [0/3276 (0%)]\tLoss: 19.312279\n",
      "Train Epoch: 237 [1280/3276 (38%)]\tLoss: 18.374052\n",
      "Train Epoch: 237 [2560/3276 (77%)]\tLoss: 17.892199\n",
      "\n",
      "Test set: Average loss: 18.0002, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 238 [0/3276 (0%)]\tLoss: 17.666395\n",
      "Train Epoch: 238 [1280/3276 (38%)]\tLoss: 18.491024\n",
      "Train Epoch: 238 [2560/3276 (77%)]\tLoss: 18.282612\n",
      "\n",
      "Test set: Average loss: 18.0006, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 239 [0/3276 (0%)]\tLoss: 17.298166\n",
      "Train Epoch: 239 [1280/3276 (38%)]\tLoss: 17.395191\n",
      "Train Epoch: 239 [2560/3276 (77%)]\tLoss: 18.848738\n",
      "\n",
      "Test set: Average loss: 18.0010, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 240 [0/3276 (0%)]\tLoss: 17.266466\n",
      "Train Epoch: 240 [1280/3276 (38%)]\tLoss: 18.549892\n",
      "Train Epoch: 240 [2560/3276 (77%)]\tLoss: 18.901413\n",
      "\n",
      "Test set: Average loss: 18.0013, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 241 [0/3276 (0%)]\tLoss: 17.631796\n",
      "Train Epoch: 241 [1280/3276 (38%)]\tLoss: 17.620018\n",
      "Train Epoch: 241 [2560/3276 (77%)]\tLoss: 18.315447\n",
      "\n",
      "Test set: Average loss: 18.0017, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 242 [0/3276 (0%)]\tLoss: 16.962875\n",
      "Train Epoch: 242 [1280/3276 (38%)]\tLoss: 19.807915\n",
      "Train Epoch: 242 [2560/3276 (77%)]\tLoss: 18.619379\n",
      "\n",
      "Test set: Average loss: 18.0021, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 243 [0/3276 (0%)]\tLoss: 19.111643\n",
      "Train Epoch: 243 [1280/3276 (38%)]\tLoss: 18.256552\n",
      "Train Epoch: 243 [2560/3276 (77%)]\tLoss: 18.005533\n",
      "\n",
      "Test set: Average loss: 18.0024, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 244 [0/3276 (0%)]\tLoss: 18.863522\n",
      "Train Epoch: 244 [1280/3276 (38%)]\tLoss: 17.924398\n",
      "Train Epoch: 244 [2560/3276 (77%)]\tLoss: 18.631344\n",
      "\n",
      "Test set: Average loss: 18.0028, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 245 [0/3276 (0%)]\tLoss: 18.954197\n",
      "Train Epoch: 245 [1280/3276 (38%)]\tLoss: 19.009005\n",
      "Train Epoch: 245 [2560/3276 (77%)]\tLoss: 18.763466\n",
      "\n",
      "Test set: Average loss: 18.0031, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 246 [0/3276 (0%)]\tLoss: 17.896467\n",
      "Train Epoch: 246 [1280/3276 (38%)]\tLoss: 17.721231\n",
      "Train Epoch: 246 [2560/3276 (77%)]\tLoss: 18.309517\n",
      "\n",
      "Test set: Average loss: 18.0035, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 247 [0/3276 (0%)]\tLoss: 18.566572\n",
      "Train Epoch: 247 [1280/3276 (38%)]\tLoss: 18.097235\n",
      "Train Epoch: 247 [2560/3276 (77%)]\tLoss: 18.682463\n",
      "\n",
      "Test set: Average loss: 18.0038, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 248 [0/3276 (0%)]\tLoss: 18.083321\n",
      "Train Epoch: 248 [1280/3276 (38%)]\tLoss: 18.285355\n",
      "Train Epoch: 248 [2560/3276 (77%)]\tLoss: 17.492662\n",
      "\n",
      "Test set: Average loss: 18.0041, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 249 [0/3276 (0%)]\tLoss: 18.254576\n",
      "Train Epoch: 249 [1280/3276 (38%)]\tLoss: 18.902126\n",
      "Train Epoch: 249 [2560/3276 (77%)]\tLoss: 19.888998\n",
      "\n",
      "Test set: Average loss: 18.0045, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 250 [0/3276 (0%)]\tLoss: 17.182381\n",
      "Train Epoch: 250 [1280/3276 (38%)]\tLoss: 19.488621\n",
      "Train Epoch: 250 [2560/3276 (77%)]\tLoss: 17.310867\n",
      "\n",
      "Test set: Average loss: 18.0048, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 251 [0/3276 (0%)]\tLoss: 18.625177\n",
      "Train Epoch: 251 [1280/3276 (38%)]\tLoss: 16.399597\n",
      "Train Epoch: 251 [2560/3276 (77%)]\tLoss: 17.126858\n",
      "\n",
      "Test set: Average loss: 18.0051, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 252 [0/3276 (0%)]\tLoss: 18.355263\n",
      "Train Epoch: 252 [1280/3276 (38%)]\tLoss: 19.086557\n",
      "Train Epoch: 252 [2560/3276 (77%)]\tLoss: 18.009354\n",
      "\n",
      "Test set: Average loss: 18.0055, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 253 [0/3276 (0%)]\tLoss: 18.381615\n",
      "Train Epoch: 253 [1280/3276 (38%)]\tLoss: 18.772980\n",
      "Train Epoch: 253 [2560/3276 (77%)]\tLoss: 18.333893\n",
      "\n",
      "Test set: Average loss: 18.0058, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 254 [0/3276 (0%)]\tLoss: 18.863337\n",
      "Train Epoch: 254 [1280/3276 (38%)]\tLoss: 18.176472\n",
      "Train Epoch: 254 [2560/3276 (77%)]\tLoss: 17.756174\n",
      "\n",
      "Test set: Average loss: 18.0061, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 255 [0/3276 (0%)]\tLoss: 18.077999\n",
      "Train Epoch: 255 [1280/3276 (38%)]\tLoss: 18.563885\n",
      "Train Epoch: 255 [2560/3276 (77%)]\tLoss: 18.506098\n",
      "\n",
      "Test set: Average loss: 18.0064, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 256 [0/3276 (0%)]\tLoss: 17.481676\n",
      "Train Epoch: 256 [1280/3276 (38%)]\tLoss: 16.919212\n",
      "Train Epoch: 256 [2560/3276 (77%)]\tLoss: 17.517275\n",
      "\n",
      "Test set: Average loss: 18.0068, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 257 [0/3276 (0%)]\tLoss: 17.135607\n",
      "Train Epoch: 257 [1280/3276 (38%)]\tLoss: 17.901262\n",
      "Train Epoch: 257 [2560/3276 (77%)]\tLoss: 17.030809\n",
      "\n",
      "Test set: Average loss: 18.0071, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 258 [0/3276 (0%)]\tLoss: 17.267889\n",
      "Train Epoch: 258 [1280/3276 (38%)]\tLoss: 19.284531\n",
      "Train Epoch: 258 [2560/3276 (77%)]\tLoss: 18.977253\n",
      "\n",
      "Test set: Average loss: 18.0074, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 259 [0/3276 (0%)]\tLoss: 19.180103\n",
      "Train Epoch: 259 [1280/3276 (38%)]\tLoss: 18.434711\n",
      "Train Epoch: 259 [2560/3276 (77%)]\tLoss: 17.327522\n",
      "\n",
      "Test set: Average loss: 18.0077, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 260 [0/3276 (0%)]\tLoss: 18.819067\n",
      "Train Epoch: 260 [1280/3276 (38%)]\tLoss: 19.168245\n",
      "Train Epoch: 260 [2560/3276 (77%)]\tLoss: 18.235735\n",
      "\n",
      "Test set: Average loss: 18.0080, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 261 [0/3276 (0%)]\tLoss: 18.358768\n",
      "Train Epoch: 261 [1280/3276 (38%)]\tLoss: 17.527710\n",
      "Train Epoch: 261 [2560/3276 (77%)]\tLoss: 17.212763\n",
      "\n",
      "Test set: Average loss: 18.0083, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 262 [0/3276 (0%)]\tLoss: 18.886974\n",
      "Train Epoch: 262 [1280/3276 (38%)]\tLoss: 18.253733\n",
      "Train Epoch: 262 [2560/3276 (77%)]\tLoss: 18.948397\n",
      "\n",
      "Test set: Average loss: 18.0086, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 263 [0/3276 (0%)]\tLoss: 18.581907\n",
      "Train Epoch: 263 [1280/3276 (38%)]\tLoss: 17.546894\n",
      "Train Epoch: 263 [2560/3276 (77%)]\tLoss: 19.010666\n",
      "\n",
      "Test set: Average loss: 18.0089, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 264 [0/3276 (0%)]\tLoss: 17.305124\n",
      "Train Epoch: 264 [1280/3276 (38%)]\tLoss: 18.045929\n",
      "Train Epoch: 264 [2560/3276 (77%)]\tLoss: 17.447971\n",
      "\n",
      "Test set: Average loss: 18.0092, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 265 [0/3276 (0%)]\tLoss: 18.547178\n",
      "Train Epoch: 265 [1280/3276 (38%)]\tLoss: 16.825432\n",
      "Train Epoch: 265 [2560/3276 (77%)]\tLoss: 18.046877\n",
      "\n",
      "Test set: Average loss: 18.0095, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 266 [0/3276 (0%)]\tLoss: 18.308516\n",
      "Train Epoch: 266 [1280/3276 (38%)]\tLoss: 16.629457\n",
      "Train Epoch: 266 [2560/3276 (77%)]\tLoss: 17.071154\n",
      "\n",
      "Test set: Average loss: 18.0098, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 267 [0/3276 (0%)]\tLoss: 17.931540\n",
      "Train Epoch: 267 [1280/3276 (38%)]\tLoss: 18.462723\n",
      "Train Epoch: 267 [2560/3276 (77%)]\tLoss: 17.217585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 18.0101, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 268 [0/3276 (0%)]\tLoss: 17.741417\n",
      "Train Epoch: 268 [1280/3276 (38%)]\tLoss: 18.877222\n",
      "Train Epoch: 268 [2560/3276 (77%)]\tLoss: 17.758440\n",
      "\n",
      "Test set: Average loss: 18.0104, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 269 [0/3276 (0%)]\tLoss: 19.556131\n",
      "Train Epoch: 269 [1280/3276 (38%)]\tLoss: 18.843996\n",
      "Train Epoch: 269 [2560/3276 (77%)]\tLoss: 18.055494\n",
      "\n",
      "Test set: Average loss: 18.0107, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 270 [0/3276 (0%)]\tLoss: 18.723703\n",
      "Train Epoch: 270 [1280/3276 (38%)]\tLoss: 18.987240\n",
      "Train Epoch: 270 [2560/3276 (77%)]\tLoss: 18.609287\n",
      "\n",
      "Test set: Average loss: 18.0110, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 271 [0/3276 (0%)]\tLoss: 18.122980\n",
      "Train Epoch: 271 [1280/3276 (38%)]\tLoss: 18.888739\n",
      "Train Epoch: 271 [2560/3276 (77%)]\tLoss: 17.063932\n",
      "\n",
      "Test set: Average loss: 18.0112, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 272 [0/3276 (0%)]\tLoss: 19.330540\n",
      "Train Epoch: 272 [1280/3276 (38%)]\tLoss: 17.340408\n",
      "Train Epoch: 272 [2560/3276 (77%)]\tLoss: 17.923290\n",
      "\n",
      "Test set: Average loss: 18.0115, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 273 [0/3276 (0%)]\tLoss: 18.728474\n",
      "Train Epoch: 273 [1280/3276 (38%)]\tLoss: 17.042036\n",
      "Train Epoch: 273 [2560/3276 (77%)]\tLoss: 18.654083\n",
      "\n",
      "Test set: Average loss: 18.0118, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 274 [0/3276 (0%)]\tLoss: 18.471760\n",
      "Train Epoch: 274 [1280/3276 (38%)]\tLoss: 18.271467\n",
      "Train Epoch: 274 [2560/3276 (77%)]\tLoss: 17.722311\n",
      "\n",
      "Test set: Average loss: 18.0121, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 275 [0/3276 (0%)]\tLoss: 19.528883\n",
      "Train Epoch: 275 [1280/3276 (38%)]\tLoss: 17.925531\n",
      "Train Epoch: 275 [2560/3276 (77%)]\tLoss: 18.189041\n",
      "\n",
      "Test set: Average loss: 18.0124, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 276 [0/3276 (0%)]\tLoss: 17.240248\n",
      "Train Epoch: 276 [1280/3276 (38%)]\tLoss: 18.203218\n",
      "Train Epoch: 276 [2560/3276 (77%)]\tLoss: 17.401514\n",
      "\n",
      "Test set: Average loss: 18.0126, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 277 [0/3276 (0%)]\tLoss: 18.946447\n",
      "Train Epoch: 277 [1280/3276 (38%)]\tLoss: 18.189568\n",
      "Train Epoch: 277 [2560/3276 (77%)]\tLoss: 18.326752\n",
      "\n",
      "Test set: Average loss: 18.0129, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 278 [0/3276 (0%)]\tLoss: 19.293913\n",
      "Train Epoch: 278 [1280/3276 (38%)]\tLoss: 17.994703\n",
      "Train Epoch: 278 [2560/3276 (77%)]\tLoss: 18.023636\n",
      "\n",
      "Test set: Average loss: 18.0132, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 279 [0/3276 (0%)]\tLoss: 17.649925\n",
      "Train Epoch: 279 [1280/3276 (38%)]\tLoss: 17.762682\n",
      "Train Epoch: 279 [2560/3276 (77%)]\tLoss: 18.954670\n",
      "\n",
      "Test set: Average loss: 18.0134, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 280 [0/3276 (0%)]\tLoss: 17.423992\n",
      "Train Epoch: 280 [1280/3276 (38%)]\tLoss: 17.027990\n",
      "Train Epoch: 280 [2560/3276 (77%)]\tLoss: 19.440475\n",
      "\n",
      "Test set: Average loss: 18.0137, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 281 [0/3276 (0%)]\tLoss: 19.088612\n",
      "Train Epoch: 281 [1280/3276 (38%)]\tLoss: 19.099205\n",
      "Train Epoch: 281 [2560/3276 (77%)]\tLoss: 18.069199\n",
      "\n",
      "Test set: Average loss: 18.0140, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 282 [0/3276 (0%)]\tLoss: 17.547817\n",
      "Train Epoch: 282 [1280/3276 (38%)]\tLoss: 17.981632\n",
      "Train Epoch: 282 [2560/3276 (77%)]\tLoss: 18.256420\n",
      "\n",
      "Test set: Average loss: 18.0142, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 283 [0/3276 (0%)]\tLoss: 18.185932\n",
      "Train Epoch: 283 [1280/3276 (38%)]\tLoss: 15.938614\n",
      "Train Epoch: 283 [2560/3276 (77%)]\tLoss: 17.045433\n",
      "\n",
      "Test set: Average loss: 18.0145, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 284 [0/3276 (0%)]\tLoss: 17.674194\n",
      "Train Epoch: 284 [1280/3276 (38%)]\tLoss: 18.194996\n",
      "Train Epoch: 284 [2560/3276 (77%)]\tLoss: 18.273548\n",
      "\n",
      "Test set: Average loss: 18.0148, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 285 [0/3276 (0%)]\tLoss: 18.538929\n",
      "Train Epoch: 285 [1280/3276 (38%)]\tLoss: 17.702047\n",
      "Train Epoch: 285 [2560/3276 (77%)]\tLoss: 18.224958\n",
      "\n",
      "Test set: Average loss: 18.0150, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 286 [0/3276 (0%)]\tLoss: 17.948139\n",
      "Train Epoch: 286 [1280/3276 (38%)]\tLoss: 17.834515\n",
      "Train Epoch: 286 [2560/3276 (77%)]\tLoss: 17.648451\n",
      "\n",
      "Test set: Average loss: 18.0153, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 287 [0/3276 (0%)]\tLoss: 17.798546\n",
      "Train Epoch: 287 [1280/3276 (38%)]\tLoss: 18.187618\n",
      "Train Epoch: 287 [2560/3276 (77%)]\tLoss: 18.009407\n",
      "\n",
      "Test set: Average loss: 18.0155, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 288 [0/3276 (0%)]\tLoss: 19.213516\n",
      "Train Epoch: 288 [1280/3276 (38%)]\tLoss: 17.254213\n",
      "Train Epoch: 288 [2560/3276 (77%)]\tLoss: 17.130758\n",
      "\n",
      "Test set: Average loss: 18.0158, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 289 [0/3276 (0%)]\tLoss: 18.270994\n",
      "Train Epoch: 289 [1280/3276 (38%)]\tLoss: 18.302273\n",
      "Train Epoch: 289 [2560/3276 (77%)]\tLoss: 18.564384\n",
      "\n",
      "Test set: Average loss: 18.0160, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 290 [0/3276 (0%)]\tLoss: 17.651455\n",
      "Train Epoch: 290 [1280/3276 (38%)]\tLoss: 18.790030\n",
      "Train Epoch: 290 [2560/3276 (77%)]\tLoss: 19.521137\n",
      "\n",
      "Test set: Average loss: 18.0163, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 291 [0/3276 (0%)]\tLoss: 19.081497\n",
      "Train Epoch: 291 [1280/3276 (38%)]\tLoss: 17.774935\n",
      "Train Epoch: 291 [2560/3276 (77%)]\tLoss: 18.166670\n",
      "\n",
      "Test set: Average loss: 18.0165, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 292 [0/3276 (0%)]\tLoss: 18.303087\n",
      "Train Epoch: 292 [1280/3276 (38%)]\tLoss: 17.904556\n",
      "Train Epoch: 292 [2560/3276 (77%)]\tLoss: 17.526550\n",
      "\n",
      "Test set: Average loss: 18.0168, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 293 [0/3276 (0%)]\tLoss: 19.034328\n",
      "Train Epoch: 293 [1280/3276 (38%)]\tLoss: 19.234623\n",
      "Train Epoch: 293 [2560/3276 (77%)]\tLoss: 18.815958\n",
      "\n",
      "Test set: Average loss: 18.0170, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 294 [0/3276 (0%)]\tLoss: 17.974966\n",
      "Train Epoch: 294 [1280/3276 (38%)]\tLoss: 17.006329\n",
      "Train Epoch: 294 [2560/3276 (77%)]\tLoss: 18.121952\n",
      "\n",
      "Test set: Average loss: 18.0172, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 295 [0/3276 (0%)]\tLoss: 18.802967\n",
      "Train Epoch: 295 [1280/3276 (38%)]\tLoss: 18.450441\n",
      "Train Epoch: 295 [2560/3276 (77%)]\tLoss: 18.185116\n",
      "\n",
      "Test set: Average loss: 18.0175, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 296 [0/3276 (0%)]\tLoss: 17.455456\n",
      "Train Epoch: 296 [1280/3276 (38%)]\tLoss: 16.638601\n",
      "Train Epoch: 296 [2560/3276 (77%)]\tLoss: 16.778946\n",
      "\n",
      "Test set: Average loss: 18.0177, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 297 [0/3276 (0%)]\tLoss: 18.536190\n",
      "Train Epoch: 297 [1280/3276 (38%)]\tLoss: 17.955545\n",
      "Train Epoch: 297 [2560/3276 (77%)]\tLoss: 18.993643\n",
      "\n",
      "Test set: Average loss: 18.0180, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 298 [0/3276 (0%)]\tLoss: 18.770897\n",
      "Train Epoch: 298 [1280/3276 (38%)]\tLoss: 18.483963\n",
      "Train Epoch: 298 [2560/3276 (77%)]\tLoss: 17.618357\n",
      "\n",
      "Test set: Average loss: 18.0182, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 299 [0/3276 (0%)]\tLoss: 18.750845\n",
      "Train Epoch: 299 [1280/3276 (38%)]\tLoss: 17.885162\n",
      "Train Epoch: 299 [2560/3276 (77%)]\tLoss: 17.731535\n",
      "\n",
      "Test set: Average loss: 18.0184, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 300 [0/3276 (0%)]\tLoss: 17.565657\n",
      "Train Epoch: 300 [1280/3276 (38%)]\tLoss: 18.508705\n",
      "Train Epoch: 300 [2560/3276 (77%)]\tLoss: 17.063932\n",
      "\n",
      "Test set: Average loss: 18.0187, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 301 [0/3276 (0%)]\tLoss: 18.511972\n",
      "Train Epoch: 301 [1280/3276 (38%)]\tLoss: 17.653404\n",
      "Train Epoch: 301 [2560/3276 (77%)]\tLoss: 20.341208\n",
      "\n",
      "Test set: Average loss: 18.0189, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 302 [0/3276 (0%)]\tLoss: 17.557117\n",
      "Train Epoch: 302 [1280/3276 (38%)]\tLoss: 17.493532\n",
      "Train Epoch: 302 [2560/3276 (77%)]\tLoss: 18.742941\n",
      "\n",
      "Test set: Average loss: 18.0191, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 303 [0/3276 (0%)]\tLoss: 17.467577\n",
      "Train Epoch: 303 [1280/3276 (38%)]\tLoss: 17.598804\n",
      "Train Epoch: 303 [2560/3276 (77%)]\tLoss: 19.151485\n",
      "\n",
      "Test set: Average loss: 18.0194, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 304 [0/3276 (0%)]\tLoss: 18.549156\n",
      "Train Epoch: 304 [1280/3276 (38%)]\tLoss: 17.965216\n",
      "Train Epoch: 304 [2560/3276 (77%)]\tLoss: 18.909239\n",
      "\n",
      "Test set: Average loss: 18.0196, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 305 [0/3276 (0%)]\tLoss: 17.120798\n",
      "Train Epoch: 305 [1280/3276 (38%)]\tLoss: 19.773710\n",
      "Train Epoch: 305 [2560/3276 (77%)]\tLoss: 17.178349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 18.0198, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 306 [0/3276 (0%)]\tLoss: 18.717300\n",
      "Train Epoch: 306 [1280/3276 (38%)]\tLoss: 18.027166\n",
      "Train Epoch: 306 [2560/3276 (77%)]\tLoss: 18.676062\n",
      "\n",
      "Test set: Average loss: 18.0200, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 307 [0/3276 (0%)]\tLoss: 17.579807\n",
      "Train Epoch: 307 [1280/3276 (38%)]\tLoss: 16.868277\n",
      "Train Epoch: 307 [2560/3276 (77%)]\tLoss: 17.573824\n",
      "\n",
      "Test set: Average loss: 18.0202, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 308 [0/3276 (0%)]\tLoss: 17.695354\n",
      "Train Epoch: 308 [1280/3276 (38%)]\tLoss: 18.407410\n",
      "Train Epoch: 308 [2560/3276 (77%)]\tLoss: 18.738485\n",
      "\n",
      "Test set: Average loss: 18.0205, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 309 [0/3276 (0%)]\tLoss: 18.191149\n",
      "Train Epoch: 309 [1280/3276 (38%)]\tLoss: 17.662733\n",
      "Train Epoch: 309 [2560/3276 (77%)]\tLoss: 18.511051\n",
      "\n",
      "Test set: Average loss: 18.0207, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 310 [0/3276 (0%)]\tLoss: 18.534845\n",
      "Train Epoch: 310 [1280/3276 (38%)]\tLoss: 17.587502\n",
      "Train Epoch: 310 [2560/3276 (77%)]\tLoss: 18.311098\n",
      "\n",
      "Test set: Average loss: 18.0209, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 311 [0/3276 (0%)]\tLoss: 18.928293\n",
      "Train Epoch: 311 [1280/3276 (38%)]\tLoss: 18.211098\n",
      "Train Epoch: 311 [2560/3276 (77%)]\tLoss: 18.624149\n",
      "\n",
      "Test set: Average loss: 18.0211, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 312 [0/3276 (0%)]\tLoss: 18.165642\n",
      "Train Epoch: 312 [1280/3276 (38%)]\tLoss: 18.047167\n",
      "Train Epoch: 312 [2560/3276 (77%)]\tLoss: 17.521412\n",
      "\n",
      "Test set: Average loss: 18.0213, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 313 [0/3276 (0%)]\tLoss: 19.275572\n",
      "Train Epoch: 313 [1280/3276 (38%)]\tLoss: 18.364803\n",
      "Train Epoch: 313 [2560/3276 (77%)]\tLoss: 18.092937\n",
      "\n",
      "Test set: Average loss: 18.0215, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 314 [0/3276 (0%)]\tLoss: 18.638378\n",
      "Train Epoch: 314 [1280/3276 (38%)]\tLoss: 16.662395\n",
      "Train Epoch: 314 [2560/3276 (77%)]\tLoss: 16.260069\n",
      "\n",
      "Test set: Average loss: 18.0218, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 315 [0/3276 (0%)]\tLoss: 18.196365\n",
      "Train Epoch: 315 [1280/3276 (38%)]\tLoss: 19.465405\n",
      "Train Epoch: 315 [2560/3276 (77%)]\tLoss: 17.662601\n",
      "\n",
      "Test set: Average loss: 18.0220, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 316 [0/3276 (0%)]\tLoss: 18.306435\n",
      "Train Epoch: 316 [1280/3276 (38%)]\tLoss: 17.422859\n",
      "Train Epoch: 316 [2560/3276 (77%)]\tLoss: 17.408682\n",
      "\n",
      "Test set: Average loss: 18.0222, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 317 [0/3276 (0%)]\tLoss: 16.615044\n",
      "Train Epoch: 317 [1280/3276 (38%)]\tLoss: 16.812176\n",
      "Train Epoch: 317 [2560/3276 (77%)]\tLoss: 18.637377\n",
      "\n",
      "Test set: Average loss: 18.0224, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 318 [0/3276 (0%)]\tLoss: 19.524220\n",
      "Train Epoch: 318 [1280/3276 (38%)]\tLoss: 18.373947\n",
      "Train Epoch: 318 [2560/3276 (77%)]\tLoss: 17.677620\n",
      "\n",
      "Test set: Average loss: 18.0226, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 319 [0/3276 (0%)]\tLoss: 17.343939\n",
      "Train Epoch: 319 [1280/3276 (38%)]\tLoss: 17.751036\n",
      "Train Epoch: 319 [2560/3276 (77%)]\tLoss: 19.702274\n",
      "\n",
      "Test set: Average loss: 18.0228, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 320 [0/3276 (0%)]\tLoss: 18.739803\n",
      "Train Epoch: 320 [1280/3276 (38%)]\tLoss: 16.191635\n",
      "Train Epoch: 320 [2560/3276 (77%)]\tLoss: 18.051411\n",
      "\n",
      "Test set: Average loss: 18.0230, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 321 [0/3276 (0%)]\tLoss: 19.075726\n",
      "Train Epoch: 321 [1280/3276 (38%)]\tLoss: 17.481754\n",
      "Train Epoch: 321 [2560/3276 (77%)]\tLoss: 17.931328\n",
      "\n",
      "Test set: Average loss: 18.0232, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 322 [0/3276 (0%)]\tLoss: 18.287727\n",
      "Train Epoch: 322 [1280/3276 (38%)]\tLoss: 17.825212\n",
      "Train Epoch: 322 [2560/3276 (77%)]\tLoss: 19.380791\n",
      "\n",
      "Test set: Average loss: 18.0234, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 323 [0/3276 (0%)]\tLoss: 18.176340\n",
      "Train Epoch: 323 [1280/3276 (38%)]\tLoss: 16.357647\n",
      "Train Epoch: 323 [2560/3276 (77%)]\tLoss: 19.701246\n",
      "\n",
      "Test set: Average loss: 18.0236, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 324 [0/3276 (0%)]\tLoss: 17.649925\n",
      "Train Epoch: 324 [1280/3276 (38%)]\tLoss: 18.018919\n",
      "Train Epoch: 324 [2560/3276 (77%)]\tLoss: 18.533371\n",
      "\n",
      "Test set: Average loss: 18.0238, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 325 [0/3276 (0%)]\tLoss: 19.492968\n",
      "Train Epoch: 325 [1280/3276 (38%)]\tLoss: 17.007673\n",
      "Train Epoch: 325 [2560/3276 (77%)]\tLoss: 17.441200\n",
      "\n",
      "Test set: Average loss: 18.0240, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 326 [0/3276 (0%)]\tLoss: 18.165194\n",
      "Train Epoch: 326 [1280/3276 (38%)]\tLoss: 17.890696\n",
      "Train Epoch: 326 [2560/3276 (77%)]\tLoss: 19.254965\n",
      "\n",
      "Test set: Average loss: 18.0242, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 327 [0/3276 (0%)]\tLoss: 19.077230\n",
      "Train Epoch: 327 [1280/3276 (38%)]\tLoss: 16.939371\n",
      "Train Epoch: 327 [2560/3276 (77%)]\tLoss: 18.959572\n",
      "\n",
      "Test set: Average loss: 18.0244, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 328 [0/3276 (0%)]\tLoss: 17.838257\n",
      "Train Epoch: 328 [1280/3276 (38%)]\tLoss: 17.690508\n",
      "Train Epoch: 328 [2560/3276 (77%)]\tLoss: 18.184958\n",
      "\n",
      "Test set: Average loss: 18.0246, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 329 [0/3276 (0%)]\tLoss: 17.968351\n",
      "Train Epoch: 329 [1280/3276 (38%)]\tLoss: 18.550999\n",
      "Train Epoch: 329 [2560/3276 (77%)]\tLoss: 18.896381\n",
      "\n",
      "Test set: Average loss: 18.0248, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 330 [0/3276 (0%)]\tLoss: 16.904772\n",
      "Train Epoch: 330 [1280/3276 (38%)]\tLoss: 17.693695\n",
      "Train Epoch: 330 [2560/3276 (77%)]\tLoss: 18.054573\n",
      "\n",
      "Test set: Average loss: 18.0250, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 331 [0/3276 (0%)]\tLoss: 17.376745\n",
      "Train Epoch: 331 [1280/3276 (38%)]\tLoss: 18.664730\n",
      "Train Epoch: 331 [2560/3276 (77%)]\tLoss: 19.342028\n",
      "\n",
      "Test set: Average loss: 18.0252, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 332 [0/3276 (0%)]\tLoss: 18.861546\n",
      "Train Epoch: 332 [1280/3276 (38%)]\tLoss: 17.165436\n",
      "Train Epoch: 332 [2560/3276 (77%)]\tLoss: 18.620224\n",
      "\n",
      "Test set: Average loss: 18.0254, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 333 [0/3276 (0%)]\tLoss: 17.756966\n",
      "Train Epoch: 333 [1280/3276 (38%)]\tLoss: 18.081291\n",
      "Train Epoch: 333 [2560/3276 (77%)]\tLoss: 18.417557\n",
      "\n",
      "Test set: Average loss: 18.0255, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 334 [0/3276 (0%)]\tLoss: 19.927284\n",
      "Train Epoch: 334 [1280/3276 (38%)]\tLoss: 17.742392\n",
      "Train Epoch: 334 [2560/3276 (77%)]\tLoss: 18.582146\n",
      "\n",
      "Test set: Average loss: 18.0257, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 335 [0/3276 (0%)]\tLoss: 17.478382\n",
      "Train Epoch: 335 [1280/3276 (38%)]\tLoss: 18.646521\n",
      "Train Epoch: 335 [2560/3276 (77%)]\tLoss: 18.851400\n",
      "\n",
      "Test set: Average loss: 18.0259, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 336 [0/3276 (0%)]\tLoss: 18.311810\n",
      "Train Epoch: 336 [1280/3276 (38%)]\tLoss: 17.912672\n",
      "Train Epoch: 336 [2560/3276 (77%)]\tLoss: 18.525963\n",
      "\n",
      "Test set: Average loss: 18.0261, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 337 [0/3276 (0%)]\tLoss: 16.687325\n",
      "Train Epoch: 337 [1280/3276 (38%)]\tLoss: 18.914616\n",
      "Train Epoch: 337 [2560/3276 (77%)]\tLoss: 17.890352\n",
      "\n",
      "Test set: Average loss: 18.0263, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 338 [0/3276 (0%)]\tLoss: 19.749020\n",
      "Train Epoch: 338 [1280/3276 (38%)]\tLoss: 18.474791\n",
      "Train Epoch: 338 [2560/3276 (77%)]\tLoss: 17.311211\n",
      "\n",
      "Test set: Average loss: 18.0265, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 339 [0/3276 (0%)]\tLoss: 18.069698\n",
      "Train Epoch: 339 [1280/3276 (38%)]\tLoss: 18.672241\n",
      "Train Epoch: 339 [2560/3276 (77%)]\tLoss: 19.095015\n",
      "\n",
      "Test set: Average loss: 18.0266, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 340 [0/3276 (0%)]\tLoss: 17.915413\n",
      "Train Epoch: 340 [1280/3276 (38%)]\tLoss: 17.319933\n",
      "Train Epoch: 340 [2560/3276 (77%)]\tLoss: 18.289782\n",
      "\n",
      "Test set: Average loss: 18.0268, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 341 [0/3276 (0%)]\tLoss: 18.613449\n",
      "Train Epoch: 341 [1280/3276 (38%)]\tLoss: 18.556873\n",
      "Train Epoch: 341 [2560/3276 (77%)]\tLoss: 18.493027\n",
      "\n",
      "Test set: Average loss: 18.0270, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 342 [0/3276 (0%)]\tLoss: 18.118895\n",
      "Train Epoch: 342 [1280/3276 (38%)]\tLoss: 17.578541\n",
      "Train Epoch: 342 [2560/3276 (77%)]\tLoss: 17.122538\n",
      "\n",
      "Test set: Average loss: 18.0272, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 343 [0/3276 (0%)]\tLoss: 19.091038\n",
      "Train Epoch: 343 [1280/3276 (38%)]\tLoss: 18.452103\n",
      "Train Epoch: 343 [2560/3276 (77%)]\tLoss: 18.784470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 18.0274, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 344 [0/3276 (0%)]\tLoss: 18.406332\n",
      "Train Epoch: 344 [1280/3276 (38%)]\tLoss: 18.974751\n",
      "Train Epoch: 344 [2560/3276 (77%)]\tLoss: 17.285940\n",
      "\n",
      "Test set: Average loss: 18.0275, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 345 [0/3276 (0%)]\tLoss: 17.355268\n",
      "Train Epoch: 345 [1280/3276 (38%)]\tLoss: 18.021713\n",
      "Train Epoch: 345 [2560/3276 (77%)]\tLoss: 17.454691\n",
      "\n",
      "Test set: Average loss: 18.0277, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 346 [0/3276 (0%)]\tLoss: 16.433407\n",
      "Train Epoch: 346 [1280/3276 (38%)]\tLoss: 18.781624\n",
      "Train Epoch: 346 [2560/3276 (77%)]\tLoss: 17.041639\n",
      "\n",
      "Test set: Average loss: 18.0279, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 347 [0/3276 (0%)]\tLoss: 20.963882\n",
      "Train Epoch: 347 [1280/3276 (38%)]\tLoss: 18.414818\n",
      "Train Epoch: 347 [2560/3276 (77%)]\tLoss: 16.966145\n",
      "\n",
      "Test set: Average loss: 18.0281, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 348 [0/3276 (0%)]\tLoss: 18.040133\n",
      "Train Epoch: 348 [1280/3276 (38%)]\tLoss: 18.791187\n",
      "Train Epoch: 348 [2560/3276 (77%)]\tLoss: 19.047029\n",
      "\n",
      "Test set: Average loss: 18.0282, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 349 [0/3276 (0%)]\tLoss: 17.707502\n",
      "Train Epoch: 349 [1280/3276 (38%)]\tLoss: 19.536392\n",
      "Train Epoch: 349 [2560/3276 (77%)]\tLoss: 17.897705\n",
      "\n",
      "Test set: Average loss: 18.0284, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 350 [0/3276 (0%)]\tLoss: 18.648365\n",
      "Train Epoch: 350 [1280/3276 (38%)]\tLoss: 19.138916\n",
      "Train Epoch: 350 [2560/3276 (77%)]\tLoss: 17.427866\n",
      "\n",
      "Test set: Average loss: 18.0286, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 351 [0/3276 (0%)]\tLoss: 18.120239\n",
      "Train Epoch: 351 [1280/3276 (38%)]\tLoss: 18.859411\n",
      "Train Epoch: 351 [2560/3276 (77%)]\tLoss: 18.651978\n",
      "\n",
      "Test set: Average loss: 18.0287, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 352 [0/3276 (0%)]\tLoss: 18.513290\n",
      "Train Epoch: 352 [1280/3276 (38%)]\tLoss: 18.279766\n",
      "Train Epoch: 352 [2560/3276 (77%)]\tLoss: 17.206966\n",
      "\n",
      "Test set: Average loss: 18.0289, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 353 [0/3276 (0%)]\tLoss: 18.500641\n",
      "Train Epoch: 353 [1280/3276 (38%)]\tLoss: 19.250038\n",
      "Train Epoch: 353 [2560/3276 (77%)]\tLoss: 19.325613\n",
      "\n",
      "Test set: Average loss: 18.0291, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 354 [0/3276 (0%)]\tLoss: 18.191835\n",
      "Train Epoch: 354 [1280/3276 (38%)]\tLoss: 18.939543\n",
      "Train Epoch: 354 [2560/3276 (77%)]\tLoss: 18.280506\n",
      "\n",
      "Test set: Average loss: 18.0292, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 355 [0/3276 (0%)]\tLoss: 16.290924\n",
      "Train Epoch: 355 [1280/3276 (38%)]\tLoss: 17.199984\n",
      "Train Epoch: 355 [2560/3276 (77%)]\tLoss: 17.749506\n",
      "\n",
      "Test set: Average loss: 18.0294, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 356 [0/3276 (0%)]\tLoss: 17.645208\n",
      "Train Epoch: 356 [1280/3276 (38%)]\tLoss: 17.647503\n",
      "Train Epoch: 356 [2560/3276 (77%)]\tLoss: 18.396898\n",
      "\n",
      "Test set: Average loss: 18.0296, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 357 [0/3276 (0%)]\tLoss: 18.287645\n",
      "Train Epoch: 357 [1280/3276 (38%)]\tLoss: 18.040449\n",
      "Train Epoch: 357 [2560/3276 (77%)]\tLoss: 18.666285\n",
      "\n",
      "Test set: Average loss: 18.0297, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 358 [0/3276 (0%)]\tLoss: 18.146984\n",
      "Train Epoch: 358 [1280/3276 (38%)]\tLoss: 17.618332\n",
      "Train Epoch: 358 [2560/3276 (77%)]\tLoss: 18.654743\n",
      "\n",
      "Test set: Average loss: 18.0299, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 359 [0/3276 (0%)]\tLoss: 18.471075\n",
      "Train Epoch: 359 [1280/3276 (38%)]\tLoss: 18.231939\n",
      "Train Epoch: 359 [2560/3276 (77%)]\tLoss: 18.948240\n",
      "\n",
      "Test set: Average loss: 18.0301, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 360 [0/3276 (0%)]\tLoss: 18.973881\n",
      "Train Epoch: 360 [1280/3276 (38%)]\tLoss: 17.620914\n",
      "Train Epoch: 360 [2560/3276 (77%)]\tLoss: 16.830566\n",
      "\n",
      "Test set: Average loss: 18.0302, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 361 [0/3276 (0%)]\tLoss: 18.306831\n",
      "Train Epoch: 361 [1280/3276 (38%)]\tLoss: 18.169725\n",
      "Train Epoch: 361 [2560/3276 (77%)]\tLoss: 17.168493\n",
      "\n",
      "Test set: Average loss: 18.0304, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 362 [0/3276 (0%)]\tLoss: 18.782019\n",
      "Train Epoch: 362 [1280/3276 (38%)]\tLoss: 19.209932\n",
      "Train Epoch: 362 [2560/3276 (77%)]\tLoss: 18.627443\n",
      "\n",
      "Test set: Average loss: 18.0305, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 363 [0/3276 (0%)]\tLoss: 18.010830\n",
      "Train Epoch: 363 [1280/3276 (38%)]\tLoss: 17.545338\n",
      "Train Epoch: 363 [2560/3276 (77%)]\tLoss: 18.718906\n",
      "\n",
      "Test set: Average loss: 18.0307, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 364 [0/3276 (0%)]\tLoss: 16.332930\n",
      "Train Epoch: 364 [1280/3276 (38%)]\tLoss: 19.141394\n",
      "Train Epoch: 364 [2560/3276 (77%)]\tLoss: 18.473475\n",
      "\n",
      "Test set: Average loss: 18.0309, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 365 [0/3276 (0%)]\tLoss: 17.511425\n",
      "Train Epoch: 365 [1280/3276 (38%)]\tLoss: 18.261534\n",
      "Train Epoch: 365 [2560/3276 (77%)]\tLoss: 16.969439\n",
      "\n",
      "Test set: Average loss: 18.0310, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 366 [0/3276 (0%)]\tLoss: 18.911480\n",
      "Train Epoch: 366 [1280/3276 (38%)]\tLoss: 18.224957\n",
      "Train Epoch: 366 [2560/3276 (77%)]\tLoss: 17.638859\n",
      "\n",
      "Test set: Average loss: 18.0312, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 367 [0/3276 (0%)]\tLoss: 17.054077\n",
      "Train Epoch: 367 [1280/3276 (38%)]\tLoss: 18.034468\n",
      "Train Epoch: 367 [2560/3276 (77%)]\tLoss: 17.497959\n",
      "\n",
      "Test set: Average loss: 18.0313, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 368 [0/3276 (0%)]\tLoss: 17.550768\n",
      "Train Epoch: 368 [1280/3276 (38%)]\tLoss: 18.434422\n",
      "Train Epoch: 368 [2560/3276 (77%)]\tLoss: 16.877579\n",
      "\n",
      "Test set: Average loss: 18.0315, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 369 [0/3276 (0%)]\tLoss: 18.307014\n",
      "Train Epoch: 369 [1280/3276 (38%)]\tLoss: 18.119738\n",
      "Train Epoch: 369 [2560/3276 (77%)]\tLoss: 20.133453\n",
      "\n",
      "Test set: Average loss: 18.0316, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 370 [0/3276 (0%)]\tLoss: 16.701370\n",
      "Train Epoch: 370 [1280/3276 (38%)]\tLoss: 18.285671\n",
      "Train Epoch: 370 [2560/3276 (77%)]\tLoss: 18.476978\n",
      "\n",
      "Test set: Average loss: 18.0318, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 371 [0/3276 (0%)]\tLoss: 18.490023\n",
      "Train Epoch: 371 [1280/3276 (38%)]\tLoss: 17.679150\n",
      "Train Epoch: 371 [2560/3276 (77%)]\tLoss: 19.803883\n",
      "\n",
      "Test set: Average loss: 18.0319, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 372 [0/3276 (0%)]\tLoss: 18.100475\n",
      "Train Epoch: 372 [1280/3276 (38%)]\tLoss: 16.592882\n",
      "Train Epoch: 372 [2560/3276 (77%)]\tLoss: 19.825808\n",
      "\n",
      "Test set: Average loss: 18.0321, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 373 [0/3276 (0%)]\tLoss: 17.672930\n",
      "Train Epoch: 373 [1280/3276 (38%)]\tLoss: 17.674616\n",
      "Train Epoch: 373 [2560/3276 (77%)]\tLoss: 17.116001\n",
      "\n",
      "Test set: Average loss: 18.0322, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 374 [0/3276 (0%)]\tLoss: 18.012278\n",
      "Train Epoch: 374 [1280/3276 (38%)]\tLoss: 18.682518\n",
      "Train Epoch: 374 [2560/3276 (77%)]\tLoss: 18.884602\n",
      "\n",
      "Test set: Average loss: 18.0324, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 375 [0/3276 (0%)]\tLoss: 18.558483\n",
      "Train Epoch: 375 [1280/3276 (38%)]\tLoss: 17.771877\n",
      "Train Epoch: 375 [2560/3276 (77%)]\tLoss: 18.402405\n",
      "\n",
      "Test set: Average loss: 18.0325, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 376 [0/3276 (0%)]\tLoss: 18.955620\n",
      "Train Epoch: 376 [1280/3276 (38%)]\tLoss: 18.510101\n",
      "Train Epoch: 376 [2560/3276 (77%)]\tLoss: 19.172222\n",
      "\n",
      "Test set: Average loss: 18.0327, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 377 [0/3276 (0%)]\tLoss: 18.796432\n",
      "Train Epoch: 377 [1280/3276 (38%)]\tLoss: 17.873119\n",
      "Train Epoch: 377 [2560/3276 (77%)]\tLoss: 18.968504\n",
      "\n",
      "Test set: Average loss: 18.0328, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 378 [0/3276 (0%)]\tLoss: 17.833830\n",
      "Train Epoch: 378 [1280/3276 (38%)]\tLoss: 18.952747\n",
      "Train Epoch: 378 [2560/3276 (77%)]\tLoss: 18.706049\n",
      "\n",
      "Test set: Average loss: 18.0330, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 379 [0/3276 (0%)]\tLoss: 18.624466\n",
      "Train Epoch: 379 [1280/3276 (38%)]\tLoss: 18.430706\n",
      "Train Epoch: 379 [2560/3276 (77%)]\tLoss: 19.089138\n",
      "\n",
      "Test set: Average loss: 18.0331, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 380 [0/3276 (0%)]\tLoss: 18.366909\n",
      "Train Epoch: 380 [1280/3276 (38%)]\tLoss: 17.443361\n",
      "Train Epoch: 380 [2560/3276 (77%)]\tLoss: 17.137873\n",
      "\n",
      "Test set: Average loss: 18.0333, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 381 [0/3276 (0%)]\tLoss: 19.221077\n",
      "Train Epoch: 381 [1280/3276 (38%)]\tLoss: 18.542355\n",
      "Train Epoch: 381 [2560/3276 (77%)]\tLoss: 17.657726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 18.0334, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 382 [0/3276 (0%)]\tLoss: 17.462175\n",
      "Train Epoch: 382 [1280/3276 (38%)]\tLoss: 18.316818\n",
      "Train Epoch: 382 [2560/3276 (77%)]\tLoss: 17.375561\n",
      "\n",
      "Test set: Average loss: 18.0336, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 383 [0/3276 (0%)]\tLoss: 19.277599\n",
      "Train Epoch: 383 [1280/3276 (38%)]\tLoss: 18.810793\n",
      "Train Epoch: 383 [2560/3276 (77%)]\tLoss: 17.429920\n",
      "\n",
      "Test set: Average loss: 18.0337, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 384 [0/3276 (0%)]\tLoss: 18.264299\n",
      "Train Epoch: 384 [1280/3276 (38%)]\tLoss: 17.973122\n",
      "Train Epoch: 384 [2560/3276 (77%)]\tLoss: 18.018417\n",
      "\n",
      "Test set: Average loss: 18.0338, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 385 [0/3276 (0%)]\tLoss: 19.689098\n",
      "Train Epoch: 385 [1280/3276 (38%)]\tLoss: 19.236044\n",
      "Train Epoch: 385 [2560/3276 (77%)]\tLoss: 18.643095\n",
      "\n",
      "Test set: Average loss: 18.0340, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 386 [0/3276 (0%)]\tLoss: 18.882257\n",
      "Train Epoch: 386 [1280/3276 (38%)]\tLoss: 19.860802\n",
      "Train Epoch: 386 [2560/3276 (77%)]\tLoss: 18.450233\n",
      "\n",
      "Test set: Average loss: 18.0341, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 387 [0/3276 (0%)]\tLoss: 17.493111\n",
      "Train Epoch: 387 [1280/3276 (38%)]\tLoss: 17.716724\n",
      "Train Epoch: 387 [2560/3276 (77%)]\tLoss: 18.380140\n",
      "\n",
      "Test set: Average loss: 18.0343, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 388 [0/3276 (0%)]\tLoss: 18.299347\n",
      "Train Epoch: 388 [1280/3276 (38%)]\tLoss: 18.125797\n",
      "Train Epoch: 388 [2560/3276 (77%)]\tLoss: 17.245440\n",
      "\n",
      "Test set: Average loss: 18.0344, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 389 [0/3276 (0%)]\tLoss: 17.874252\n",
      "Train Epoch: 389 [1280/3276 (38%)]\tLoss: 18.275658\n",
      "Train Epoch: 389 [2560/3276 (77%)]\tLoss: 16.368504\n",
      "\n",
      "Test set: Average loss: 18.0345, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 390 [0/3276 (0%)]\tLoss: 17.988590\n",
      "Train Epoch: 390 [1280/3276 (38%)]\tLoss: 18.411758\n",
      "Train Epoch: 390 [2560/3276 (77%)]\tLoss: 18.063425\n",
      "\n",
      "Test set: Average loss: 18.0347, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 391 [0/3276 (0%)]\tLoss: 17.735382\n",
      "Train Epoch: 391 [1280/3276 (38%)]\tLoss: 18.909134\n",
      "Train Epoch: 391 [2560/3276 (77%)]\tLoss: 18.566782\n",
      "\n",
      "Test set: Average loss: 18.0348, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 392 [0/3276 (0%)]\tLoss: 18.050356\n",
      "Train Epoch: 392 [1280/3276 (38%)]\tLoss: 17.567263\n",
      "Train Epoch: 392 [2560/3276 (77%)]\tLoss: 16.487900\n",
      "\n",
      "Test set: Average loss: 18.0350, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 393 [0/3276 (0%)]\tLoss: 19.150406\n",
      "Train Epoch: 393 [1280/3276 (38%)]\tLoss: 18.105904\n",
      "Train Epoch: 393 [2560/3276 (77%)]\tLoss: 16.964931\n",
      "\n",
      "Test set: Average loss: 18.0351, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 394 [0/3276 (0%)]\tLoss: 19.277601\n",
      "Train Epoch: 394 [1280/3276 (38%)]\tLoss: 17.457537\n",
      "Train Epoch: 394 [2560/3276 (77%)]\tLoss: 19.052116\n",
      "\n",
      "Test set: Average loss: 18.0352, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 395 [0/3276 (0%)]\tLoss: 18.380482\n",
      "Train Epoch: 395 [1280/3276 (38%)]\tLoss: 19.001522\n",
      "Train Epoch: 395 [2560/3276 (77%)]\tLoss: 18.472290\n",
      "\n",
      "Test set: Average loss: 18.0354, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 396 [0/3276 (0%)]\tLoss: 18.622751\n",
      "Train Epoch: 396 [1280/3276 (38%)]\tLoss: 17.664656\n",
      "Train Epoch: 396 [2560/3276 (77%)]\tLoss: 18.000156\n",
      "\n",
      "Test set: Average loss: 18.0355, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 397 [0/3276 (0%)]\tLoss: 18.011726\n",
      "Train Epoch: 397 [1280/3276 (38%)]\tLoss: 17.617804\n",
      "Train Epoch: 397 [2560/3276 (77%)]\tLoss: 16.529903\n",
      "\n",
      "Test set: Average loss: 18.0356, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 398 [0/3276 (0%)]\tLoss: 18.610922\n",
      "Train Epoch: 398 [1280/3276 (38%)]\tLoss: 18.792927\n",
      "Train Epoch: 398 [2560/3276 (77%)]\tLoss: 17.615326\n",
      "\n",
      "Test set: Average loss: 18.0358, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 399 [0/3276 (0%)]\tLoss: 17.935202\n",
      "Train Epoch: 399 [1280/3276 (38%)]\tLoss: 17.840233\n",
      "Train Epoch: 399 [2560/3276 (77%)]\tLoss: 17.862183\n",
      "\n",
      "Test set: Average loss: 18.0359, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 400 [0/3276 (0%)]\tLoss: 16.107496\n",
      "Train Epoch: 400 [1280/3276 (38%)]\tLoss: 19.252832\n",
      "Train Epoch: 400 [2560/3276 (77%)]\tLoss: 17.834646\n",
      "\n",
      "Test set: Average loss: 18.0360, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 401 [0/3276 (0%)]\tLoss: 17.122143\n",
      "Train Epoch: 401 [1280/3276 (38%)]\tLoss: 18.593002\n",
      "Train Epoch: 401 [2560/3276 (77%)]\tLoss: 18.665678\n",
      "\n",
      "Test set: Average loss: 18.0361, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 402 [0/3276 (0%)]\tLoss: 18.464092\n",
      "Train Epoch: 402 [1280/3276 (38%)]\tLoss: 19.161577\n",
      "Train Epoch: 402 [2560/3276 (77%)]\tLoss: 17.560095\n",
      "\n",
      "Test set: Average loss: 18.0363, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 403 [0/3276 (0%)]\tLoss: 18.691872\n",
      "Train Epoch: 403 [1280/3276 (38%)]\tLoss: 17.199745\n",
      "Train Epoch: 403 [2560/3276 (77%)]\tLoss: 18.517113\n",
      "\n",
      "Test set: Average loss: 18.0364, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 404 [0/3276 (0%)]\tLoss: 18.205326\n",
      "Train Epoch: 404 [1280/3276 (38%)]\tLoss: 18.487993\n",
      "Train Epoch: 404 [2560/3276 (77%)]\tLoss: 19.726990\n",
      "\n",
      "Test set: Average loss: 18.0365, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 405 [0/3276 (0%)]\tLoss: 18.080158\n",
      "Train Epoch: 405 [1280/3276 (38%)]\tLoss: 18.640038\n",
      "Train Epoch: 405 [2560/3276 (77%)]\tLoss: 18.095522\n",
      "\n",
      "Test set: Average loss: 18.0367, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 406 [0/3276 (0%)]\tLoss: 17.882473\n",
      "Train Epoch: 406 [1280/3276 (38%)]\tLoss: 18.250071\n",
      "Train Epoch: 406 [2560/3276 (77%)]\tLoss: 15.896110\n",
      "\n",
      "Test set: Average loss: 18.0368, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 407 [0/3276 (0%)]\tLoss: 18.736668\n",
      "Train Epoch: 407 [1280/3276 (38%)]\tLoss: 19.850710\n",
      "Train Epoch: 407 [2560/3276 (77%)]\tLoss: 17.382359\n",
      "\n",
      "Test set: Average loss: 18.0369, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 408 [0/3276 (0%)]\tLoss: 17.000401\n",
      "Train Epoch: 408 [1280/3276 (38%)]\tLoss: 18.557085\n",
      "Train Epoch: 408 [2560/3276 (77%)]\tLoss: 18.740015\n",
      "\n",
      "Test set: Average loss: 18.0370, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 409 [0/3276 (0%)]\tLoss: 17.554430\n",
      "Train Epoch: 409 [1280/3276 (38%)]\tLoss: 19.776955\n",
      "Train Epoch: 409 [2560/3276 (77%)]\tLoss: 17.151760\n",
      "\n",
      "Test set: Average loss: 18.0372, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 410 [0/3276 (0%)]\tLoss: 18.025375\n",
      "Train Epoch: 410 [1280/3276 (38%)]\tLoss: 17.692404\n",
      "Train Epoch: 410 [2560/3276 (77%)]\tLoss: 17.938866\n",
      "\n",
      "Test set: Average loss: 18.0373, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 411 [0/3276 (0%)]\tLoss: 17.769587\n",
      "Train Epoch: 411 [1280/3276 (38%)]\tLoss: 18.214628\n",
      "Train Epoch: 411 [2560/3276 (77%)]\tLoss: 18.576033\n",
      "\n",
      "Test set: Average loss: 18.0374, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 412 [0/3276 (0%)]\tLoss: 18.530603\n",
      "Train Epoch: 412 [1280/3276 (38%)]\tLoss: 18.712215\n",
      "Train Epoch: 412 [2560/3276 (77%)]\tLoss: 18.445965\n",
      "\n",
      "Test set: Average loss: 18.0375, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 413 [0/3276 (0%)]\tLoss: 17.989117\n",
      "Train Epoch: 413 [1280/3276 (38%)]\tLoss: 17.630348\n",
      "Train Epoch: 413 [2560/3276 (77%)]\tLoss: 19.365322\n",
      "\n",
      "Test set: Average loss: 18.0377, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 414 [0/3276 (0%)]\tLoss: 19.580135\n",
      "Train Epoch: 414 [1280/3276 (38%)]\tLoss: 18.001238\n",
      "Train Epoch: 414 [2560/3276 (77%)]\tLoss: 17.111891\n",
      "\n",
      "Test set: Average loss: 18.0378, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 415 [0/3276 (0%)]\tLoss: 20.090370\n",
      "Train Epoch: 415 [1280/3276 (38%)]\tLoss: 17.634827\n",
      "Train Epoch: 415 [2560/3276 (77%)]\tLoss: 18.192177\n",
      "\n",
      "Test set: Average loss: 18.0379, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 416 [0/3276 (0%)]\tLoss: 18.772110\n",
      "Train Epoch: 416 [1280/3276 (38%)]\tLoss: 17.652035\n",
      "Train Epoch: 416 [2560/3276 (77%)]\tLoss: 18.178499\n",
      "\n",
      "Test set: Average loss: 18.0380, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 417 [0/3276 (0%)]\tLoss: 16.474777\n",
      "Train Epoch: 417 [1280/3276 (38%)]\tLoss: 17.931591\n",
      "Train Epoch: 417 [2560/3276 (77%)]\tLoss: 17.045647\n",
      "\n",
      "Test set: Average loss: 18.0381, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 418 [0/3276 (0%)]\tLoss: 18.505886\n",
      "Train Epoch: 418 [1280/3276 (38%)]\tLoss: 17.833065\n",
      "Train Epoch: 418 [2560/3276 (77%)]\tLoss: 17.863871\n",
      "\n",
      "Test set: Average loss: 18.0383, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 419 [0/3276 (0%)]\tLoss: 19.327745\n",
      "Train Epoch: 419 [1280/3276 (38%)]\tLoss: 19.513891\n",
      "Train Epoch: 419 [2560/3276 (77%)]\tLoss: 17.152313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 18.0384, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 420 [0/3276 (0%)]\tLoss: 17.720942\n",
      "Train Epoch: 420 [1280/3276 (38%)]\tLoss: 18.293127\n",
      "Train Epoch: 420 [2560/3276 (77%)]\tLoss: 18.267803\n",
      "\n",
      "Test set: Average loss: 18.0385, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 421 [0/3276 (0%)]\tLoss: 19.034039\n",
      "Train Epoch: 421 [1280/3276 (38%)]\tLoss: 17.823843\n",
      "Train Epoch: 421 [2560/3276 (77%)]\tLoss: 17.404940\n",
      "\n",
      "Test set: Average loss: 18.0386, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 422 [0/3276 (0%)]\tLoss: 19.620060\n",
      "Train Epoch: 422 [1280/3276 (38%)]\tLoss: 17.628212\n",
      "Train Epoch: 422 [2560/3276 (77%)]\tLoss: 17.618015\n",
      "\n",
      "Test set: Average loss: 18.0387, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 423 [0/3276 (0%)]\tLoss: 18.568916\n",
      "Train Epoch: 423 [1280/3276 (38%)]\tLoss: 18.554478\n",
      "Train Epoch: 423 [2560/3276 (77%)]\tLoss: 18.446781\n",
      "\n",
      "Test set: Average loss: 18.0388, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 424 [0/3276 (0%)]\tLoss: 17.898441\n",
      "Train Epoch: 424 [1280/3276 (38%)]\tLoss: 18.307148\n",
      "Train Epoch: 424 [2560/3276 (77%)]\tLoss: 20.022280\n",
      "\n",
      "Test set: Average loss: 18.0390, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 425 [0/3276 (0%)]\tLoss: 18.113335\n",
      "Train Epoch: 425 [1280/3276 (38%)]\tLoss: 17.479620\n",
      "Train Epoch: 425 [2560/3276 (77%)]\tLoss: 18.019472\n",
      "\n",
      "Test set: Average loss: 18.0391, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 426 [0/3276 (0%)]\tLoss: 16.218277\n",
      "Train Epoch: 426 [1280/3276 (38%)]\tLoss: 19.368406\n",
      "Train Epoch: 426 [2560/3276 (77%)]\tLoss: 17.980051\n",
      "\n",
      "Test set: Average loss: 18.0392, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 427 [0/3276 (0%)]\tLoss: 18.440746\n",
      "Train Epoch: 427 [1280/3276 (38%)]\tLoss: 18.489681\n",
      "Train Epoch: 427 [2560/3276 (77%)]\tLoss: 19.766756\n",
      "\n",
      "Test set: Average loss: 18.0393, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 428 [0/3276 (0%)]\tLoss: 18.158844\n",
      "Train Epoch: 428 [1280/3276 (38%)]\tLoss: 16.723555\n",
      "Train Epoch: 428 [2560/3276 (77%)]\tLoss: 17.131233\n",
      "\n",
      "Test set: Average loss: 18.0394, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 429 [0/3276 (0%)]\tLoss: 19.401924\n",
      "Train Epoch: 429 [1280/3276 (38%)]\tLoss: 17.895042\n",
      "Train Epoch: 429 [2560/3276 (77%)]\tLoss: 19.234386\n",
      "\n",
      "Test set: Average loss: 18.0395, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 430 [0/3276 (0%)]\tLoss: 17.694775\n",
      "Train Epoch: 430 [1280/3276 (38%)]\tLoss: 18.176342\n",
      "Train Epoch: 430 [2560/3276 (77%)]\tLoss: 19.445221\n",
      "\n",
      "Test set: Average loss: 18.0397, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 431 [0/3276 (0%)]\tLoss: 17.171417\n",
      "Train Epoch: 431 [1280/3276 (38%)]\tLoss: 18.178843\n",
      "Train Epoch: 431 [2560/3276 (77%)]\tLoss: 18.803308\n",
      "\n",
      "Test set: Average loss: 18.0398, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 432 [0/3276 (0%)]\tLoss: 18.028721\n",
      "Train Epoch: 432 [1280/3276 (38%)]\tLoss: 17.491846\n",
      "Train Epoch: 432 [2560/3276 (77%)]\tLoss: 18.383722\n",
      "\n",
      "Test set: Average loss: 18.0399, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 433 [0/3276 (0%)]\tLoss: 18.000660\n",
      "Train Epoch: 433 [1280/3276 (38%)]\tLoss: 18.338741\n",
      "Train Epoch: 433 [2560/3276 (77%)]\tLoss: 17.347153\n",
      "\n",
      "Test set: Average loss: 18.0400, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 434 [0/3276 (0%)]\tLoss: 18.725574\n",
      "Train Epoch: 434 [1280/3276 (38%)]\tLoss: 18.819910\n",
      "Train Epoch: 434 [2560/3276 (77%)]\tLoss: 17.573982\n",
      "\n",
      "Test set: Average loss: 18.0401, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 435 [0/3276 (0%)]\tLoss: 18.824284\n",
      "Train Epoch: 435 [1280/3276 (38%)]\tLoss: 18.489231\n",
      "Train Epoch: 435 [2560/3276 (77%)]\tLoss: 17.890327\n",
      "\n",
      "Test set: Average loss: 18.0402, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 436 [0/3276 (0%)]\tLoss: 19.344692\n",
      "Train Epoch: 436 [1280/3276 (38%)]\tLoss: 19.227509\n",
      "Train Epoch: 436 [2560/3276 (77%)]\tLoss: 18.749739\n",
      "\n",
      "Test set: Average loss: 18.0403, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 437 [0/3276 (0%)]\tLoss: 19.190037\n",
      "Train Epoch: 437 [1280/3276 (38%)]\tLoss: 17.313026\n",
      "Train Epoch: 437 [2560/3276 (77%)]\tLoss: 18.027668\n",
      "\n",
      "Test set: Average loss: 18.0404, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 438 [0/3276 (0%)]\tLoss: 18.234444\n",
      "Train Epoch: 438 [1280/3276 (38%)]\tLoss: 18.322535\n",
      "Train Epoch: 438 [2560/3276 (77%)]\tLoss: 18.905735\n",
      "\n",
      "Test set: Average loss: 18.0405, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 439 [0/3276 (0%)]\tLoss: 17.351370\n",
      "Train Epoch: 439 [1280/3276 (38%)]\tLoss: 18.876854\n",
      "Train Epoch: 439 [2560/3276 (77%)]\tLoss: 19.564537\n",
      "\n",
      "Test set: Average loss: 18.0406, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 440 [0/3276 (0%)]\tLoss: 18.764467\n",
      "Train Epoch: 440 [1280/3276 (38%)]\tLoss: 17.474876\n",
      "Train Epoch: 440 [2560/3276 (77%)]\tLoss: 19.366613\n",
      "\n",
      "Test set: Average loss: 18.0408, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 441 [0/3276 (0%)]\tLoss: 18.228832\n",
      "Train Epoch: 441 [1280/3276 (38%)]\tLoss: 17.025276\n",
      "Train Epoch: 441 [2560/3276 (77%)]\tLoss: 18.108170\n",
      "\n",
      "Test set: Average loss: 18.0409, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 442 [0/3276 (0%)]\tLoss: 18.198212\n",
      "Train Epoch: 442 [1280/3276 (38%)]\tLoss: 18.236000\n",
      "Train Epoch: 442 [2560/3276 (77%)]\tLoss: 19.424480\n",
      "\n",
      "Test set: Average loss: 18.0410, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 443 [0/3276 (0%)]\tLoss: 18.336370\n",
      "Train Epoch: 443 [1280/3276 (38%)]\tLoss: 19.623615\n",
      "Train Epoch: 443 [2560/3276 (77%)]\tLoss: 18.771109\n",
      "\n",
      "Test set: Average loss: 18.0411, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 444 [0/3276 (0%)]\tLoss: 19.235781\n",
      "Train Epoch: 444 [1280/3276 (38%)]\tLoss: 17.640362\n",
      "Train Epoch: 444 [2560/3276 (77%)]\tLoss: 18.650923\n",
      "\n",
      "Test set: Average loss: 18.0412, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 445 [0/3276 (0%)]\tLoss: 19.351013\n",
      "Train Epoch: 445 [1280/3276 (38%)]\tLoss: 18.117340\n",
      "Train Epoch: 445 [2560/3276 (77%)]\tLoss: 19.510675\n",
      "\n",
      "Test set: Average loss: 18.0413, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 446 [0/3276 (0%)]\tLoss: 17.548447\n",
      "Train Epoch: 446 [1280/3276 (38%)]\tLoss: 18.445251\n",
      "Train Epoch: 446 [2560/3276 (77%)]\tLoss: 17.778782\n",
      "\n",
      "Test set: Average loss: 18.0414, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 447 [0/3276 (0%)]\tLoss: 18.460667\n",
      "Train Epoch: 447 [1280/3276 (38%)]\tLoss: 17.057055\n",
      "Train Epoch: 447 [2560/3276 (77%)]\tLoss: 18.686232\n",
      "\n",
      "Test set: Average loss: 18.0415, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 448 [0/3276 (0%)]\tLoss: 18.939598\n",
      "Train Epoch: 448 [1280/3276 (38%)]\tLoss: 18.044479\n",
      "Train Epoch: 448 [2560/3276 (77%)]\tLoss: 18.429363\n",
      "\n",
      "Test set: Average loss: 18.0416, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 449 [0/3276 (0%)]\tLoss: 17.525206\n",
      "Train Epoch: 449 [1280/3276 (38%)]\tLoss: 18.205431\n",
      "Train Epoch: 449 [2560/3276 (77%)]\tLoss: 18.814693\n",
      "\n",
      "Test set: Average loss: 18.0417, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 450 [0/3276 (0%)]\tLoss: 18.983471\n",
      "Train Epoch: 450 [1280/3276 (38%)]\tLoss: 17.706713\n",
      "Train Epoch: 450 [2560/3276 (77%)]\tLoss: 18.361403\n",
      "\n",
      "Test set: Average loss: 18.0418, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 451 [0/3276 (0%)]\tLoss: 18.625256\n",
      "Train Epoch: 451 [1280/3276 (38%)]\tLoss: 19.074436\n",
      "Train Epoch: 451 [2560/3276 (77%)]\tLoss: 18.072781\n",
      "\n",
      "Test set: Average loss: 18.0419, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 452 [0/3276 (0%)]\tLoss: 18.779331\n",
      "Train Epoch: 452 [1280/3276 (38%)]\tLoss: 16.854284\n",
      "Train Epoch: 452 [2560/3276 (77%)]\tLoss: 17.014446\n",
      "\n",
      "Test set: Average loss: 18.0420, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 453 [0/3276 (0%)]\tLoss: 17.563099\n",
      "Train Epoch: 453 [1280/3276 (38%)]\tLoss: 18.265854\n",
      "Train Epoch: 453 [2560/3276 (77%)]\tLoss: 18.230280\n",
      "\n",
      "Test set: Average loss: 18.0421, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 454 [0/3276 (0%)]\tLoss: 16.869225\n",
      "Train Epoch: 454 [1280/3276 (38%)]\tLoss: 16.643160\n",
      "Train Epoch: 454 [2560/3276 (77%)]\tLoss: 17.380013\n",
      "\n",
      "Test set: Average loss: 18.0422, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 455 [0/3276 (0%)]\tLoss: 17.713774\n",
      "Train Epoch: 455 [1280/3276 (38%)]\tLoss: 18.038471\n",
      "Train Epoch: 455 [2560/3276 (77%)]\tLoss: 18.595953\n",
      "\n",
      "Test set: Average loss: 18.0423, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 456 [0/3276 (0%)]\tLoss: 17.584234\n",
      "Train Epoch: 456 [1280/3276 (38%)]\tLoss: 17.967878\n",
      "Train Epoch: 456 [2560/3276 (77%)]\tLoss: 18.776588\n",
      "\n",
      "Test set: Average loss: 18.0424, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 457 [0/3276 (0%)]\tLoss: 18.163534\n",
      "Train Epoch: 457 [1280/3276 (38%)]\tLoss: 17.933727\n",
      "Train Epoch: 457 [2560/3276 (77%)]\tLoss: 17.356640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 18.0425, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 458 [0/3276 (0%)]\tLoss: 18.791582\n",
      "Train Epoch: 458 [1280/3276 (38%)]\tLoss: 17.479225\n",
      "Train Epoch: 458 [2560/3276 (77%)]\tLoss: 18.832375\n",
      "\n",
      "Test set: Average loss: 18.0426, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 459 [0/3276 (0%)]\tLoss: 18.574505\n",
      "Train Epoch: 459 [1280/3276 (38%)]\tLoss: 17.775356\n",
      "Train Epoch: 459 [2560/3276 (77%)]\tLoss: 17.868666\n",
      "\n",
      "Test set: Average loss: 18.0427, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 460 [0/3276 (0%)]\tLoss: 18.103664\n",
      "Train Epoch: 460 [1280/3276 (38%)]\tLoss: 18.563673\n",
      "Train Epoch: 460 [2560/3276 (77%)]\tLoss: 18.962681\n",
      "\n",
      "Test set: Average loss: 18.0428, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 461 [0/3276 (0%)]\tLoss: 17.350130\n",
      "Train Epoch: 461 [1280/3276 (38%)]\tLoss: 17.967876\n",
      "Train Epoch: 461 [2560/3276 (77%)]\tLoss: 18.660276\n",
      "\n",
      "Test set: Average loss: 18.0429, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 462 [0/3276 (0%)]\tLoss: 16.983482\n",
      "Train Epoch: 462 [1280/3276 (38%)]\tLoss: 18.890743\n",
      "Train Epoch: 462 [2560/3276 (77%)]\tLoss: 17.292870\n",
      "\n",
      "Test set: Average loss: 18.0430, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 463 [0/3276 (0%)]\tLoss: 17.654142\n",
      "Train Epoch: 463 [1280/3276 (38%)]\tLoss: 18.787947\n",
      "Train Epoch: 463 [2560/3276 (77%)]\tLoss: 17.830141\n",
      "\n",
      "Test set: Average loss: 18.0431, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 464 [0/3276 (0%)]\tLoss: 17.138269\n",
      "Train Epoch: 464 [1280/3276 (38%)]\tLoss: 18.316528\n",
      "Train Epoch: 464 [2560/3276 (77%)]\tLoss: 19.232990\n",
      "\n",
      "Test set: Average loss: 18.0432, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 465 [0/3276 (0%)]\tLoss: 17.534668\n",
      "Train Epoch: 465 [1280/3276 (38%)]\tLoss: 17.963186\n",
      "Train Epoch: 465 [2560/3276 (77%)]\tLoss: 17.830402\n",
      "\n",
      "Test set: Average loss: 18.0433, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 466 [0/3276 (0%)]\tLoss: 18.808237\n",
      "Train Epoch: 466 [1280/3276 (38%)]\tLoss: 18.997568\n",
      "Train Epoch: 466 [2560/3276 (77%)]\tLoss: 16.950308\n",
      "\n",
      "Test set: Average loss: 18.0434, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 467 [0/3276 (0%)]\tLoss: 18.868816\n",
      "Train Epoch: 467 [1280/3276 (38%)]\tLoss: 18.583647\n",
      "Train Epoch: 467 [2560/3276 (77%)]\tLoss: 18.171307\n",
      "\n",
      "Test set: Average loss: 18.0435, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 468 [0/3276 (0%)]\tLoss: 17.608452\n",
      "Train Epoch: 468 [1280/3276 (38%)]\tLoss: 18.200109\n",
      "Train Epoch: 468 [2560/3276 (77%)]\tLoss: 17.615776\n",
      "\n",
      "Test set: Average loss: 18.0436, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 469 [0/3276 (0%)]\tLoss: 19.251698\n",
      "Train Epoch: 469 [1280/3276 (38%)]\tLoss: 18.160583\n",
      "Train Epoch: 469 [2560/3276 (77%)]\tLoss: 18.514767\n",
      "\n",
      "Test set: Average loss: 18.0437, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 470 [0/3276 (0%)]\tLoss: 18.736879\n",
      "Train Epoch: 470 [1280/3276 (38%)]\tLoss: 19.182764\n",
      "Train Epoch: 470 [2560/3276 (77%)]\tLoss: 16.954762\n",
      "\n",
      "Test set: Average loss: 18.0438, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 471 [0/3276 (0%)]\tLoss: 17.540043\n",
      "Train Epoch: 471 [1280/3276 (38%)]\tLoss: 18.787447\n",
      "Train Epoch: 471 [2560/3276 (77%)]\tLoss: 17.621597\n",
      "\n",
      "Test set: Average loss: 18.0439, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 472 [0/3276 (0%)]\tLoss: 17.070099\n",
      "Train Epoch: 472 [1280/3276 (38%)]\tLoss: 17.674170\n",
      "Train Epoch: 472 [2560/3276 (77%)]\tLoss: 17.369919\n",
      "\n",
      "Test set: Average loss: 18.0440, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 473 [0/3276 (0%)]\tLoss: 18.792875\n",
      "Train Epoch: 473 [1280/3276 (38%)]\tLoss: 18.429073\n",
      "Train Epoch: 473 [2560/3276 (77%)]\tLoss: 17.554035\n",
      "\n",
      "Test set: Average loss: 18.0441, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 474 [0/3276 (0%)]\tLoss: 17.469263\n",
      "Train Epoch: 474 [1280/3276 (38%)]\tLoss: 16.430428\n",
      "Train Epoch: 474 [2560/3276 (77%)]\tLoss: 17.164621\n",
      "\n",
      "Test set: Average loss: 18.0442, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 475 [0/3276 (0%)]\tLoss: 16.942244\n",
      "Train Epoch: 475 [1280/3276 (38%)]\tLoss: 18.134758\n",
      "Train Epoch: 475 [2560/3276 (77%)]\tLoss: 18.270624\n",
      "\n",
      "Test set: Average loss: 18.0442, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 476 [0/3276 (0%)]\tLoss: 19.342846\n",
      "Train Epoch: 476 [1280/3276 (38%)]\tLoss: 17.586130\n",
      "Train Epoch: 476 [2560/3276 (77%)]\tLoss: 18.403723\n",
      "\n",
      "Test set: Average loss: 18.0443, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 477 [0/3276 (0%)]\tLoss: 18.102663\n",
      "Train Epoch: 477 [1280/3276 (38%)]\tLoss: 16.942297\n",
      "Train Epoch: 477 [2560/3276 (77%)]\tLoss: 18.292101\n",
      "\n",
      "Test set: Average loss: 18.0444, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 478 [0/3276 (0%)]\tLoss: 17.859573\n",
      "Train Epoch: 478 [1280/3276 (38%)]\tLoss: 18.168356\n",
      "Train Epoch: 478 [2560/3276 (77%)]\tLoss: 19.339817\n",
      "\n",
      "Test set: Average loss: 18.0445, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 479 [0/3276 (0%)]\tLoss: 17.164381\n",
      "Train Epoch: 479 [1280/3276 (38%)]\tLoss: 17.979288\n",
      "Train Epoch: 479 [2560/3276 (77%)]\tLoss: 18.658854\n",
      "\n",
      "Test set: Average loss: 18.0446, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 480 [0/3276 (0%)]\tLoss: 17.724552\n",
      "Train Epoch: 480 [1280/3276 (38%)]\tLoss: 19.530966\n",
      "Train Epoch: 480 [2560/3276 (77%)]\tLoss: 18.113228\n",
      "\n",
      "Test set: Average loss: 18.0447, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 481 [0/3276 (0%)]\tLoss: 18.226433\n",
      "Train Epoch: 481 [1280/3276 (38%)]\tLoss: 17.949167\n",
      "Train Epoch: 481 [2560/3276 (77%)]\tLoss: 18.440693\n",
      "\n",
      "Test set: Average loss: 18.0448, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 482 [0/3276 (0%)]\tLoss: 18.227171\n",
      "Train Epoch: 482 [1280/3276 (38%)]\tLoss: 18.569866\n",
      "Train Epoch: 482 [2560/3276 (77%)]\tLoss: 17.806320\n",
      "\n",
      "Test set: Average loss: 18.0449, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 483 [0/3276 (0%)]\tLoss: 18.777382\n",
      "Train Epoch: 483 [1280/3276 (38%)]\tLoss: 17.564812\n",
      "Train Epoch: 483 [2560/3276 (77%)]\tLoss: 17.764475\n",
      "\n",
      "Test set: Average loss: 18.0450, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 484 [0/3276 (0%)]\tLoss: 18.704123\n",
      "Train Epoch: 484 [1280/3276 (38%)]\tLoss: 18.077076\n",
      "Train Epoch: 484 [2560/3276 (77%)]\tLoss: 17.967825\n",
      "\n",
      "Test set: Average loss: 18.0451, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 485 [0/3276 (0%)]\tLoss: 16.567110\n",
      "Train Epoch: 485 [1280/3276 (38%)]\tLoss: 17.976995\n",
      "Train Epoch: 485 [2560/3276 (77%)]\tLoss: 18.963314\n",
      "\n",
      "Test set: Average loss: 18.0451, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 486 [0/3276 (0%)]\tLoss: 20.173508\n",
      "Train Epoch: 486 [1280/3276 (38%)]\tLoss: 18.411655\n",
      "Train Epoch: 486 [2560/3276 (77%)]\tLoss: 17.518461\n",
      "\n",
      "Test set: Average loss: 18.0452, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 487 [0/3276 (0%)]\tLoss: 18.273207\n",
      "Train Epoch: 487 [1280/3276 (38%)]\tLoss: 18.238132\n",
      "Train Epoch: 487 [2560/3276 (77%)]\tLoss: 17.089520\n",
      "\n",
      "Test set: Average loss: 18.0453, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 488 [0/3276 (0%)]\tLoss: 17.896599\n",
      "Train Epoch: 488 [1280/3276 (38%)]\tLoss: 17.917625\n",
      "Train Epoch: 488 [2560/3276 (77%)]\tLoss: 17.661415\n",
      "\n",
      "Test set: Average loss: 18.0454, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 489 [0/3276 (0%)]\tLoss: 18.326014\n",
      "Train Epoch: 489 [1280/3276 (38%)]\tLoss: 17.185568\n",
      "Train Epoch: 489 [2560/3276 (77%)]\tLoss: 17.406944\n",
      "\n",
      "Test set: Average loss: 18.0455, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 490 [0/3276 (0%)]\tLoss: 18.321482\n",
      "Train Epoch: 490 [1280/3276 (38%)]\tLoss: 17.196321\n",
      "Train Epoch: 490 [2560/3276 (77%)]\tLoss: 18.784180\n",
      "\n",
      "Test set: Average loss: 18.0456, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 491 [0/3276 (0%)]\tLoss: 17.812565\n",
      "Train Epoch: 491 [1280/3276 (38%)]\tLoss: 18.498219\n",
      "Train Epoch: 491 [2560/3276 (77%)]\tLoss: 18.233391\n",
      "\n",
      "Test set: Average loss: 18.0457, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 492 [0/3276 (0%)]\tLoss: 17.978338\n",
      "Train Epoch: 492 [1280/3276 (38%)]\tLoss: 18.573185\n",
      "Train Epoch: 492 [2560/3276 (77%)]\tLoss: 19.216995\n",
      "\n",
      "Test set: Average loss: 18.0458, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 493 [0/3276 (0%)]\tLoss: 19.195728\n",
      "Train Epoch: 493 [1280/3276 (38%)]\tLoss: 19.461531\n",
      "Train Epoch: 493 [2560/3276 (77%)]\tLoss: 18.207962\n",
      "\n",
      "Test set: Average loss: 18.0458, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 494 [0/3276 (0%)]\tLoss: 16.954603\n",
      "Train Epoch: 494 [1280/3276 (38%)]\tLoss: 17.964188\n",
      "Train Epoch: 494 [2560/3276 (77%)]\tLoss: 19.649044\n",
      "\n",
      "Test set: Average loss: 18.0459, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 495 [0/3276 (0%)]\tLoss: 17.392740\n",
      "Train Epoch: 495 [1280/3276 (38%)]\tLoss: 17.874647\n",
      "Train Epoch: 495 [2560/3276 (77%)]\tLoss: 18.149988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 18.0460, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 496 [0/3276 (0%)]\tLoss: 17.557775\n",
      "Train Epoch: 496 [1280/3276 (38%)]\tLoss: 17.414454\n",
      "Train Epoch: 496 [2560/3276 (77%)]\tLoss: 19.401424\n",
      "\n",
      "Test set: Average loss: 18.0461, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 497 [0/3276 (0%)]\tLoss: 19.688623\n",
      "Train Epoch: 497 [1280/3276 (38%)]\tLoss: 18.162401\n",
      "Train Epoch: 497 [2560/3276 (77%)]\tLoss: 16.591221\n",
      "\n",
      "Test set: Average loss: 18.0462, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 498 [0/3276 (0%)]\tLoss: 18.301218\n",
      "Train Epoch: 498 [1280/3276 (38%)]\tLoss: 17.904922\n",
      "Train Epoch: 498 [2560/3276 (77%)]\tLoss: 17.367075\n",
      "\n",
      "Test set: Average loss: 18.0463, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 499 [0/3276 (0%)]\tLoss: 18.708023\n",
      "Train Epoch: 499 [1280/3276 (38%)]\tLoss: 18.242165\n",
      "Train Epoch: 499 [2560/3276 (77%)]\tLoss: 18.277739\n",
      "\n",
      "Test set: Average loss: 18.0464, Accuracy: 3276245/9486336 (35%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 18.16876692680212\n",
      "Training Accuracy: 34.256763947315704\n",
      "\n",
      "Test Loss: 17.957023267353808\n",
      "Testing Accuracy: 18.04635659245794\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate Accuracy\n",
    "print('Training Loss:', train_hist['train_loss_epoch'][-1])\n",
    "print('Training Accuracy:', train_hist['train_accu'][-1])\n",
    "print()\n",
    "print('Test Loss:', np.mean(train_hist['test_loss_epoch']))\n",
    "print('Testing Accuracy:', np.max(train_hist['test_loss_epoch']))\n",
    "print()\n",
    "\n",
    "plt.plot(train_hist['train_loss_epoch'],'r', label='Training Loss')\n",
    "plt.plot(train_hist['test_loss_epoch'],'b', label='Testing Loss')\n",
    "plt.title('Test Loss ' + str(train_hist['test_loss_epoch'][-1]))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(train_hist['train_accu'],'r', label='Training accuracy')\n",
    "plt.plot(train_hist['test_accu'],'b', label='Testing accuracy')\n",
    "plt.title('Test Accuracy : '+ str(np.max(train_hist['test_accu'])))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences\n",
    "* It is clear from the abive graph that, there is overfitting in the network.\n",
    "* I tried different menthods like changing dropouts and actiavtion functions to do address this.\n",
    "* This did not help as, even though the overfitting decreased the final test accuracy always decreased.\n",
    "* Adam gave beteer performance than SGD.\n",
    "* The haphazard motion of both error and accuracy is due to the random shuffling of data in minibatches after each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = []\n",
    "mask = []\n",
    "pred= []\n",
    "with torch.no_grad(): # as we dont need to backpropogate when calculating testing error and accuracy\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        #==== Getting the Prediction====\n",
    "        output = net(data)\n",
    "#         output[output>1] =1\n",
    "#         output[output<1] =0\n",
    "        img.append(np.transpose(data[0], (1,2,0)).numpy())\n",
    "        mask.append(np.transpose(target[0], (1,2,0))[:,:,0].numpy())\n",
    "        pred.append(np.transpose(output[0], (1,2,0))[:,:,0].numpy())\n",
    "#         break\n",
    "# # output[0].shape\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.imshow(np.transpose(data[0], (1,2,0))[:,:,0].numpy())\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.imshow(np.transpose(output[0],(1,2,0))[:,:,0].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "num = random.randint(1,100)\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(img[num])\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(mask[num])\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(pred[num])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
