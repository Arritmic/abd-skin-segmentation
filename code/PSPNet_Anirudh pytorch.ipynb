{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify the flowers shown in the flower dataset\n",
    "\n",
    "* The flowers are in color and you'll have to work with that. You can't turn them to greyscale etc.\n",
    "* Data augmentation is allowed here due to popular complaint, but the problem can be completed without it\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torchvision import datasets,models, transforms\n",
    "import time\n",
    "import sys\n",
    "# !pip install torchsummary \n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.backends.cudnn.benchmark=True\n",
    "from tqdm import tqdm\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "import random\n",
    "import scipy.misc as m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Training and Testing Data using Data Loader with Data Augmentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "IMG_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Dataset 1: Final abdomen images\n",
    "# TRAIN_PATH1 = ['../allabdomen/train/skin_train2019/']\n",
    "# MASK_PATH1 = ['../allabdomen/train/annotations/']\n",
    "# train_ids1 = next(os.walk(TRAIN_PATH1[0]))[2]\n",
    "# mask_ids1 = next(os.walk(MASK_PATH1[0]))[2]\n",
    "# train_ids1.sort()\n",
    "# mask_ids1.sort()\n",
    "# TRAIN_PATH1 = TRAIN_PATH1*len(train_ids1)\n",
    "# MASK_PATH1 = MASK_PATH1*len(train_ids1)\n",
    "\n",
    "# # # Dataset 2: Augmented Abdomen Images\n",
    "# TRAIN_PATH2 = ['../allabdomen/train/skin_augmented/']\n",
    "# MASK_PATH2 = ['../allabdomen/train/annotations_augmented/']\n",
    "# train_ids2 = next(os.walk(TRAIN_PATH2[0]))[2]\n",
    "# mask_ids2 = next(os.walk(MASK_PATH2[0]))[2]\n",
    "# train_ids2.sort()\n",
    "# mask_ids2.sort()\n",
    "# TRAIN_PATH2 = TRAIN_PATH2*len(train_ids2)\n",
    "# MASK_PATH2 = MASK_PATH2*len(train_ids2)\n",
    "\n",
    "# # # Combine everything\n",
    "# TRAIN_PATH = np.concatenate((TRAIN_PATH1,TRAIN_PATH2))\n",
    "# MASK_PATH = np.concatenate((MASK_PATH1,MASK_PATH2))\n",
    "# train_ids = np.concatenate((train_ids1,train_ids2))\n",
    "# mask_ids = np.concatenate((mask_ids1,mask_ids2))\n",
    "\n",
    "\n",
    "# # Get and resize train images and masks DONT RUN IN THIS CODE\n",
    "# X_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "# Y_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "# print('Getting and resizing train images and masks ... ')\n",
    "# sys.stdout.flush()\n",
    "# for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n",
    "#     path = TRAIN_PATH[n] + id_\n",
    "#     img = imread(path)[:,:,:IMG_CHANNELS]\n",
    "#     img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "#     X_train[n] = img\n",
    "\n",
    "# for n, id_ in tqdm(enumerate(mask_ids), total=len(mask_ids)):\n",
    "#     path = MASK_PATH[n] + id_\n",
    "#     img = imread(path)\n",
    "#     #if n in range(899,977):\n",
    "#         #img = img[:,:,1]\n",
    "#     img = np.expand_dims(resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', \n",
    "#                                       preserve_range=True), axis=-1)\n",
    "#     Y_train[n] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if training data looks all right\n",
    "# ix = random.randint(0, len(train_ids))\n",
    "# imshow(X_train[ix])\n",
    "# plt.show()\n",
    "# # imshow(np.squeeze(Y_train[436]))\n",
    "# imshow(Y_train[ix][:,:,0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Loading testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset 1: Final abdomen images\n",
    "# TRAIN_PATH_test = ['../../../../allabdomen/test/skin_test2019/']\n",
    "# MASK_PATH_test = ['../../../../allabdomen/test/annotations/']\n",
    "# train_ids_test = next(os.walk(TRAIN_PATH_test[0]))[2]\n",
    "# mask_ids_test = next(os.walk(MASK_PATH_test[0]))[2]\n",
    "# train_ids_test.sort()\n",
    "# mask_ids_test.sort()\n",
    "# TRAIN_PATH_test = TRAIN_PATH_test*len(train_ids_test)\n",
    "# MASK_PATH_test = MASK_PATH_test*len(train_ids_test)\n",
    "\n",
    "# X_test = np.zeros((len(train_ids_test), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "# Y_test = np.zeros((len(train_ids_test), IMG_HEIGHT, IMG_WIDTH, 1),dtype=np.bool)\n",
    "# print('Getting and resizing test images and masks ... ')\n",
    "# sys.stdout.flush()\n",
    "# for n, id_ in tqdm(enumerate(train_ids_test), total=len(train_ids_test)):\n",
    "#     path = TRAIN_PATH_test[n] + id_\n",
    "#     img = imread(path)[:,:,:IMG_CHANNELS]\n",
    "#     img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "#     X_test[n] = img\n",
    "\n",
    "# for n, id_ in tqdm(enumerate(mask_ids_test), total=len(mask_ids_test)):\n",
    "#     path = MASK_PATH_test[n] + id_\n",
    "#     img = imread(path)\n",
    "#     #if n in range(899,977):\n",
    "#         #img = img[:,:,1]\n",
    "#     img = np.expand_dims(resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', \n",
    "#                                       preserve_range=True), axis=-1)\n",
    "#     Y_test[n] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if training data looks all right\n",
    "# ix = random.randint(0, len(train_ids_test))\n",
    "# imshow(X_test[ix])\n",
    "# plt.show()\n",
    "# # imshow(np.squeeze(Y_train[436]))\n",
    "# imshow(Y_test[ix][:,:,0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the Loaded train np arrays\n",
    "# np.save('X_train.npy',X_train)\n",
    "# np.save('Y_train.npy',Y_train)\n",
    "\n",
    "# # Saving the Loaded test np arrays\n",
    "# np.save('X_test.npy',X_test)\n",
    "# np.save('Y_test.npy',Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: [0.53766632 0.55240142 0.52007403], std: [0.27631814 0.25418144 0.28067154]\n"
     ]
    }
   ],
   "source": [
    "# Loading the Training and Testing Data\n",
    "trainImages = np.load('X_train.npy')\n",
    "trainLabels = (np.load('Y_train.npy')*255).astype(np.uint8)\n",
    "\n",
    "X_test = np.load('X_test.npy')\n",
    "Y_test = (np.load('Y_test.npy')*255).astype(np.uint8)\n",
    "\n",
    "# Splitting the Data into Training and Test Data\n",
    "X_train, X_val,Y_train,Y_val = train_test_split(trainImages,trainLabels, test_size=0.15, shuffle = True)\n",
    "\n",
    "# Finding the mean and the Variance\n",
    "img_mean = np.mean(np.swapaxes(trainImages/255.0,0,1).reshape(3, -1), 1)\n",
    "img_std = np.std(np.swapaxes(trainImages/255.0,0,1).reshape(3, -1), 1)\n",
    "print(\"mean: {}, std: {}\".format(img_mean, img_std))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinLoader(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_arr, y_arr, transform=None, transform_label=None):\n",
    "        self.x_arr = x_arr\n",
    "        self.y_arr = y_arr\n",
    "        self.transform = transform\n",
    "        self.transform_label = transform_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x_arr.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.x_arr[index]\n",
    "        label = self.y_arr[index]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.transform_label is not None:\n",
    "            label = self.transform_label(label)\n",
    "        return img, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalizing the Data\n",
    "normalize = transforms.Normalize(mean=list(img_mean),std=list(img_std))\n",
    "batch_size =64\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    SkinLoader(X_train, Y_train, transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]),transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "# remove augmentation transforms in test loader\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    SkinLoader(X_val, Y_val, transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]),transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "    ])),shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    SkinLoader(X_test, Y_test, transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]),transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "    ])),shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Images below are not unormalized yet and therefore the variation in rgb colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for img,lbl in train_loader:\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(np.transpose(img[0], (1,2,0)).numpy())\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(np.transpose(lbl[0],(1,2,0))[:,:,0].numpy())\n",
    "    break\n",
    "# plt.show()\n",
    "# lbl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tying Segnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv2DBatchNormRelu(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        n_filters,\n",
    "        k_size,\n",
    "        stride,\n",
    "        padding,\n",
    "        bias=True,\n",
    "        dilation=1,\n",
    "        is_batchnorm=True,\n",
    "    ):\n",
    "        super(conv2DBatchNormRelu, self).__init__()\n",
    "\n",
    "        conv_mod = nn.Conv2d(\n",
    "            int(in_channels),\n",
    "            int(n_filters),\n",
    "            kernel_size=k_size,\n",
    "            padding=padding,\n",
    "            stride=stride,\n",
    "            bias=bias,\n",
    "            dilation=dilation,\n",
    "        )\n",
    "\n",
    "        if is_batchnorm:\n",
    "            self.cbr_unit = nn.Sequential(\n",
    "                conv_mod, nn.BatchNorm2d(int(n_filters)), nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.cbr_unit = nn.Sequential(conv_mod, nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.cbr_unit(inputs)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class segnetDown2(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(segnetDown2, self).__init__()\n",
    "        self.conv1 = conv2DBatchNormRelu(in_size, out_size, 3, 1, 1)\n",
    "        self.conv2 = conv2DBatchNormRelu(out_size, out_size, 3, 1, 1)\n",
    "        self.maxpool_with_argmax = nn.MaxPool2d(2, 2, return_indices=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.conv1(inputs)\n",
    "        outputs = self.conv2(outputs)\n",
    "        unpooled_shape = outputs.size()\n",
    "        outputs, indices = self.maxpool_with_argmax(outputs)\n",
    "        return outputs, indices, unpooled_shape\n",
    "\n",
    "\n",
    "class segnetDown3(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(segnetDown3, self).__init__()\n",
    "        self.conv1 = conv2DBatchNormRelu(in_size, out_size, 3, 1, 1)\n",
    "        self.conv2 = conv2DBatchNormRelu(out_size, out_size, 3, 1, 1)\n",
    "        self.conv3 = conv2DBatchNormRelu(out_size, out_size, 3, 1, 1)\n",
    "        self.maxpool_with_argmax = nn.MaxPool2d(2, 2, return_indices=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.conv1(inputs)\n",
    "        outputs = self.conv2(outputs)\n",
    "        outputs = self.conv3(outputs)\n",
    "        unpooled_shape = outputs.size()\n",
    "        outputs, indices = self.maxpool_with_argmax(outputs)\n",
    "        return outputs, indices, unpooled_shape\n",
    "\n",
    "\n",
    "class segnetUp2(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(segnetUp2, self).__init__()\n",
    "        self.unpool = nn.MaxUnpool2d(2, 2)\n",
    "        self.conv1 = conv2DBatchNormRelu(in_size, in_size, 3, 1, 1)\n",
    "        self.conv2 = conv2DBatchNormRelu(in_size, out_size, 3, 1, 1)\n",
    "\n",
    "    def forward(self, inputs, indices, output_shape):\n",
    "        outputs = self.unpool(input=inputs, indices=indices, output_size=output_shape)\n",
    "        outputs = self.conv1(outputs)\n",
    "        outputs = self.conv2(outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class segnetUp3(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(segnetUp3, self).__init__()\n",
    "        self.unpool = nn.MaxUnpool2d(2, 2)\n",
    "        self.conv1 = conv2DBatchNormRelu(in_size, in_size, 3, 1, 1)\n",
    "        self.conv2 = conv2DBatchNormRelu(in_size, in_size, 3, 1, 1)\n",
    "        self.conv3 = conv2DBatchNormRelu(in_size, out_size, 3, 1, 1)\n",
    "\n",
    "    def forward(self, inputs, indices, output_shape):\n",
    "        outputs = self.unpool(input=inputs, indices=indices, output_size=output_shape)\n",
    "        outputs = self.conv1(outputs)\n",
    "        outputs = self.conv2(outputs)\n",
    "        outputs = self.conv3(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): segnet(\n",
       "    (down1): segnetDown2(\n",
       "      (conv1): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv2): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (maxpool_with_argmax): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (down2): segnetDown2(\n",
       "      (conv1): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv2): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (maxpool_with_argmax): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (down3): segnetDown3(\n",
       "      (conv1): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv2): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv3): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (maxpool_with_argmax): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (down4): segnetDown3(\n",
       "      (conv1): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv2): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv3): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (maxpool_with_argmax): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (down5): segnetDown3(\n",
       "      (conv1): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv2): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv3): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (maxpool_with_argmax): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (up5): segnetUp3(\n",
       "      (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "      (conv1): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv2): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv3): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up4): segnetUp3(\n",
       "      (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "      (conv1): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv2): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv3): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up3): segnetUp3(\n",
       "      (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "      (conv1): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv2): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv3): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up2): segnetUp2(\n",
       "      (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "      (conv1): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv2): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up1): segnetUp2(\n",
       "      (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "      (conv1): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (conv2): conv2DBatchNormRelu(\n",
       "        (cbr_unit): Sequential(\n",
       "          (0): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class segnet(nn.Module):\n",
    "    def __init__(self, n_classes=1, in_channels=3, is_unpooling=True):\n",
    "        super(segnet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.is_unpooling = is_unpooling\n",
    "\n",
    "        self.down1 = segnetDown2(self.in_channels, 64)\n",
    "        self.down2 = segnetDown2(64, 128)\n",
    "        self.down3 = segnetDown3(128, 256)\n",
    "        self.down4 = segnetDown3(256, 512)\n",
    "        self.down5 = segnetDown3(512, 512)\n",
    "\n",
    "        self.up5 = segnetUp3(512, 512)\n",
    "        self.up4 = segnetUp3(512, 256)\n",
    "        self.up3 = segnetUp3(256, 128)\n",
    "        self.up2 = segnetUp2(128, 64)\n",
    "        self.up1 = segnetUp2(64, n_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        down1, indices_1, unpool_shape1 = self.down1(inputs)\n",
    "        down2, indices_2, unpool_shape2 = self.down2(down1)\n",
    "        down3, indices_3, unpool_shape3 = self.down3(down2)\n",
    "        down4, indices_4, unpool_shape4 = self.down4(down3)\n",
    "        down5, indices_5, unpool_shape5 = self.down5(down4)\n",
    "\n",
    "        up5 = self.up5(down5, indices_5, unpool_shape5)\n",
    "        up4 = self.up4(up5, indices_4, unpool_shape4)\n",
    "        up3 = self.up3(up4, indices_3, unpool_shape3)\n",
    "        up2 = self.up2(up3, indices_2, unpool_shape2)\n",
    "        up1 = self.up1(up2, indices_1, unpool_shape1)\n",
    "\n",
    "        return up1\n",
    "\n",
    "    def init_vgg16_params(self, vgg16):\n",
    "        blocks = [self.down1, self.down2, self.down3, self.down4, self.down5]\n",
    "\n",
    "        features = list(vgg16.features.children())\n",
    "\n",
    "        vgg_layers = []\n",
    "        for _layer in features:\n",
    "            if isinstance(_layer, nn.Conv2d):\n",
    "                vgg_layers.append(_layer)\n",
    "\n",
    "        merged_layers = []\n",
    "        for idx, conv_block in enumerate(blocks):\n",
    "            if idx < 2:\n",
    "                units = [conv_block.conv1.cbr_unit, conv_block.conv2.cbr_unit]\n",
    "            else:\n",
    "                units = [\n",
    "                    conv_block.conv1.cbr_unit,\n",
    "                    conv_block.conv2.cbr_unit,\n",
    "                    conv_block.conv3.cbr_unit,\n",
    "                ]\n",
    "            for _unit in units:\n",
    "                for _layer in _unit:\n",
    "                    if isinstance(_layer, nn.Conv2d):\n",
    "                        merged_layers.append(_layer)\n",
    "\n",
    "        assert len(vgg_layers) == len(merged_layers)\n",
    "\n",
    "        for l1, l2 in zip(vgg_layers, merged_layers):\n",
    "            if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n",
    "                assert l1.weight.size() == l2.weight.size()\n",
    "                assert l1.bias.size() == l2.bias.size()\n",
    "                l2.weight.data = l1.weight.data\n",
    "                l2.bias.data = l1.bias.data\n",
    "net = segnet()\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "net.init_vgg16_params(vgg16)\n",
    "net.to(device)\n",
    "net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying PSPNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# class _PyramidPoolingModule(nn.Module):\n",
    "#     def __init__(self, in_dim, reduction_dim, setting):\n",
    "#         super(_PyramidPoolingModule, self).__init__()\n",
    "#         self.features = []\n",
    "#         for s in setting:\n",
    "#             self.features.append(nn.Sequential(\n",
    "#                 nn.AdaptiveAvgPool2d(s),\n",
    "#                 nn.Conv2d(in_dim, reduction_dim, kernel_size=1, bias=False),\n",
    "#                 nn.BatchNorm2d(reduction_dim, momentum=.95),\n",
    "#                 nn.ReLU(inplace=True)\n",
    "#             ))\n",
    "#         self.features = nn.ModuleList(self.features)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x_size = x.size()\n",
    "#         out = [x]\n",
    "#         for f in self.features:\n",
    "#             out.append(F.upsample(f(x), x_size[2:], mode='bilinear'))\n",
    "#         out = torch.cat(out, 1)\n",
    "#         return out\n",
    "\n",
    "# class PSPNet(nn.Module):\n",
    "#     def __init__(self, num_classes, pretrained=True, use_aux=True):\n",
    "#         super(PSPNet, self).__init__()\n",
    "#         self.use_aux = use_aux\n",
    "#         resnet = models.resnet101()\n",
    "#         if pretrained:\n",
    "#             resnet.load_state_dict(torch.load('../../resnet101-5d3b4d8f.pth'))\n",
    "#         self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n",
    "#         self.layer1, self.layer2, self.layer3, self.layer4 = resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4\n",
    "\n",
    "#         for n, m in self.layer3.named_modules():\n",
    "#             if 'conv2' in n:\n",
    "#                 m.dilation, m.padding, m.stride = (2, 2), (2, 2), (1, 1)\n",
    "#             elif 'downsample.0' in n:\n",
    "#                 m.stride = (1, 1)\n",
    "#         for n, m in self.layer4.named_modules():\n",
    "#             if 'conv2' in n:\n",
    "#                 m.dilation, m.padding, m.stride = (4, 4), (4, 4), (1, 1)\n",
    "#             elif 'downsample.0' in n:\n",
    "#                 m.stride = (1, 1)\n",
    "\n",
    "#         self.ppm = _PyramidPoolingModule(2048, 512, (1, 2, 3, 6))\n",
    "#         self.final = nn.Sequential(\n",
    "#             nn.Conv2d(4096, 512, kernel_size=3, padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(512, momentum=.95),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "#         )\n",
    "\n",
    "#         if use_aux:\n",
    "#             self.aux_logits = nn.Conv2d(1024, num_classes, kernel_size=1)\n",
    "#             initialize_weights(self.aux_logits)\n",
    "\n",
    "#         initialize_weights(self.ppm, self.final)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x_size = x.size()\n",
    "#         x = self.layer0(x)\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "#         if self.training and self.use_aux:\n",
    "#             aux = self.aux_logits(x)\n",
    "#         x = self.layer4(x)\n",
    "#         x = self.ppm(x)\n",
    "#         x = self.final(x)\n",
    "#         if self.training and self.use_aux:\n",
    "#             return F.upsample(x, x_size[2:], mode='bilinear'), F.upsample(aux, x_size[2:], mode='bilinear')\n",
    "#         return F.upsample(x, x_size[2:], mode='bilinear')\n",
    "    \n",
    "# def initialize_weights(*models):\n",
    "#     for model in models:\n",
    "#         for module in model.modules():\n",
    "#             if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "#                 nn.init.kaiming_normal(module.weight)\n",
    "#                 if module.bias is not None:\n",
    "#                     module.bias.data.zero_()\n",
    "#             elif isinstance(module, nn.BatchNorm2d):\n",
    "#                 module.weight.data.fill_(1)    \n",
    "#                 module.bias.data.zero_()\n",
    "# net = PSPNet(num_classes=1)\n",
    "# net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If want to get a summary of the network uncomment the below line as well as the one in importing libraries.\n",
    "# summary(net,(3,32,32),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss2d(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True, ignore_index=255):\n",
    "        super(CrossEntropyLoss2d, self).__init__()\n",
    "        self.nll_loss = nn.NLLLoss2d(weight, size_average, ignore_index)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        return self.nll_loss(F.log_softmax(inputs), targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# criterion = nn.CrossEntropyLoss() \n",
    "criterion =  nn.BCELoss()\n",
    "# criterion = CrossEntropyLoss2d(size_average=True)# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9,weight_decay = 5e-4)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay= 5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # For Debugging\n",
    "# for batch_idx, (data, target) in enumerate(test_loader):\n",
    "# #     data, target = data.to(device), target.to(device)\n",
    "#     optimizer.zero_grad()\n",
    "#     output = net(data)\n",
    "#     break\n",
    "# data.shape\n",
    "# target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = criterion(output, target)\n",
    "# loss.backward()\n",
    "# optimizer.step()\n",
    "# train_hist['train_loss'].append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hist = {}\n",
    "train_hist['train_loss'] = []\n",
    "train_hist['test_loss'] = []\n",
    "train_hist['train_loss_epoch'] = []\n",
    "train_hist['test_loss_epoch'] = []\n",
    "train_hist['per_epoch_time'] = []\n",
    "train_hist['total_time'] = []\n",
    "train_hist['train_accu'] = []\n",
    "train_hist['test_accu'] = []\n",
    "epochs = 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    print('training start!!')\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_start_time = time.time()\n",
    "#         training_loss = 0\n",
    "        train_correct = 0\n",
    "        lo = []\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            #==== Forward Pass=====\n",
    "            output = model(data)\n",
    "            output[output>1] =1\n",
    "            output[output<1] =0\n",
    "            loss = criterion(output, target)\n",
    "            #=====Backward Pass=======\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #=== Loss Append to get loss of entire Batch====\n",
    "            train_hist['train_loss'].append(loss.item())\n",
    "            #==== Calculating Training Accuracy========= \n",
    "            train_correct += output.eq(target).sum().item()\n",
    "            #======= Logging results after every 20th batch============ \n",
    "            if batch_idx % 20 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "        #======== Getting Accuracy of the entire epoch by averaging of each batch===========    \n",
    "        train_hist['train_accu'].append(100 * (train_correct / (Y_train.shape[0]*128*128)))\n",
    "        #======== Getting Training Loss of the epoch by averaging across each batch\n",
    "        train_hist['train_loss_epoch'].append(np.mean(train_hist['train_loss']))\n",
    "        test(model)\n",
    "\n",
    "def test(model):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad(): # as we dont need to backpropogate when calculating testing error and accuracy\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            #==== Getting the Prediction======\n",
    "            output = model(data)\n",
    "            output[output>1] =1\n",
    "            output[output<1] =0\n",
    "            #===== Calculating the Loss=========\n",
    "            test_loss = criterion(output, target)\n",
    "            train_hist['test_loss'].append(test_loss.item())\n",
    "            # Calculating Testing Accuracy for the all inputs=========\n",
    "            correct += output.eq(target).sum().item()\n",
    "    #======= Getting Testing Accuracy for the Epoch========\n",
    "    train_hist['test_accu'].append(100. * correct / (Y_val.shape[0]*128*128))\n",
    "    #====== Getting Testing Error of Epoch========\n",
    "    train_hist['test_loss_epoch'].append(np.mean(train_hist['test_loss']))\n",
    "   #======= Logging results after every epoch ============ \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        np.mean(train_hist['test_loss']), correct, val_loader.dataset.y_arr.size,\n",
    "        100 * correct / (Y_val.shape[0]*128*128)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start!!\n",
      "Train Epoch: 0 [0/3276 (0%)]\tLoss: 17.344385\n",
      "Train Epoch: 0 [1280/3276 (38%)]\tLoss: 19.759876\n",
      "Train Epoch: 0 [2560/3276 (77%)]\tLoss: 16.813889\n",
      "\n",
      "Test set: Average loss: 17.8590, Accuracy: 3354950/9486336 (35%)\n",
      "\n",
      "Train Epoch: 1 [0/3276 (0%)]\tLoss: 18.003214\n",
      "Train Epoch: 1 [1280/3276 (38%)]\tLoss: 18.476240\n",
      "Train Epoch: 1 [2560/3276 (77%)]\tLoss: 17.897308\n",
      "\n",
      "Test set: Average loss: 17.7884, Accuracy: 3403442/9486336 (36%)\n",
      "\n",
      "Train Epoch: 2 [0/3276 (0%)]\tLoss: 18.582699\n",
      "Train Epoch: 2 [1280/3276 (38%)]\tLoss: 17.777491\n",
      "Train Epoch: 2 [2560/3276 (77%)]\tLoss: 19.860830\n",
      "\n",
      "Test set: Average loss: 17.7649, Accuracy: 3403359/9486336 (36%)\n",
      "\n",
      "Train Epoch: 3 [0/3276 (0%)]\tLoss: 19.437975\n",
      "Train Epoch: 3 [1280/3276 (38%)]\tLoss: 18.141926\n",
      "Train Epoch: 3 [2560/3276 (77%)]\tLoss: 18.203165\n",
      "\n",
      "Test set: Average loss: 17.7532, Accuracy: 3403358/9486336 (36%)\n",
      "\n",
      "Train Epoch: 4 [0/3276 (0%)]\tLoss: 18.646704\n",
      "Train Epoch: 4 [1280/3276 (38%)]\tLoss: 18.114336\n",
      "Train Epoch: 4 [2560/3276 (77%)]\tLoss: 18.095707\n",
      "\n",
      "Test set: Average loss: 17.7461, Accuracy: 3403363/9486336 (36%)\n",
      "\n",
      "Train Epoch: 5 [0/3276 (0%)]\tLoss: 17.924267\n",
      "Train Epoch: 5 [1280/3276 (38%)]\tLoss: 18.820385\n",
      "Train Epoch: 5 [2560/3276 (77%)]\tLoss: 17.575748\n",
      "\n",
      "Test set: Average loss: 17.7415, Accuracy: 3403363/9486336 (36%)\n",
      "\n",
      "Train Epoch: 6 [0/3276 (0%)]\tLoss: 18.990507\n",
      "Train Epoch: 6 [1280/3276 (38%)]\tLoss: 17.898388\n",
      "Train Epoch: 6 [2560/3276 (77%)]\tLoss: 17.416615\n",
      "\n",
      "Test set: Average loss: 17.7381, Accuracy: 3403363/9486336 (36%)\n",
      "\n",
      "Train Epoch: 7 [0/3276 (0%)]\tLoss: 16.533461\n",
      "Train Epoch: 7 [1280/3276 (38%)]\tLoss: 18.387754\n",
      "Train Epoch: 7 [2560/3276 (77%)]\tLoss: 18.886894\n",
      "\n",
      "Test set: Average loss: 17.7356, Accuracy: 3403362/9486336 (36%)\n",
      "\n",
      "Train Epoch: 8 [0/3276 (0%)]\tLoss: 17.629713\n",
      "Train Epoch: 8 [1280/3276 (38%)]\tLoss: 18.122980\n",
      "Train Epoch: 8 [2560/3276 (77%)]\tLoss: 17.249470\n",
      "\n",
      "Test set: Average loss: 17.7336, Accuracy: 3403359/9486336 (36%)\n",
      "\n",
      "Train Epoch: 9 [0/3276 (0%)]\tLoss: 17.166096\n",
      "Train Epoch: 9 [1280/3276 (38%)]\tLoss: 19.277075\n",
      "Train Epoch: 9 [2560/3276 (77%)]\tLoss: 18.588522\n",
      "\n",
      "Test set: Average loss: 17.7321, Accuracy: 3403359/9486336 (36%)\n",
      "\n",
      "Train Epoch: 10 [0/3276 (0%)]\tLoss: 18.609524\n",
      "Train Epoch: 10 [1280/3276 (38%)]\tLoss: 18.726837\n",
      "Train Epoch: 10 [2560/3276 (77%)]\tLoss: 18.029617\n",
      "\n",
      "Test set: Average loss: 17.7308, Accuracy: 3403361/9486336 (36%)\n",
      "\n",
      "Train Epoch: 11 [0/3276 (0%)]\tLoss: 19.374308\n",
      "Train Epoch: 11 [1280/3276 (38%)]\tLoss: 18.460772\n",
      "Train Epoch: 11 [2560/3276 (77%)]\tLoss: 19.173094\n",
      "\n",
      "Test set: Average loss: 17.7297, Accuracy: 3403357/9486336 (36%)\n",
      "\n",
      "Train Epoch: 12 [0/3276 (0%)]\tLoss: 18.964102\n",
      "Train Epoch: 12 [1280/3276 (38%)]\tLoss: 18.519588\n",
      "Train Epoch: 12 [2560/3276 (77%)]\tLoss: 18.095337\n",
      "\n",
      "Test set: Average loss: 17.7288, Accuracy: 3403356/9486336 (36%)\n",
      "\n",
      "Train Epoch: 13 [0/3276 (0%)]\tLoss: 17.992596\n",
      "Train Epoch: 13 [1280/3276 (38%)]\tLoss: 18.689657\n",
      "Train Epoch: 13 [2560/3276 (77%)]\tLoss: 18.944946\n",
      "\n",
      "Test set: Average loss: 17.7280, Accuracy: 3403352/9486336 (36%)\n",
      "\n",
      "Train Epoch: 14 [0/3276 (0%)]\tLoss: 18.442646\n",
      "Train Epoch: 14 [1280/3276 (38%)]\tLoss: 18.944710\n",
      "Train Epoch: 14 [2560/3276 (77%)]\tLoss: 18.872137\n",
      "\n",
      "Test set: Average loss: 17.7274, Accuracy: 3403354/9486336 (36%)\n",
      "\n",
      "Train Epoch: 15 [0/3276 (0%)]\tLoss: 19.086163\n",
      "Train Epoch: 15 [1280/3276 (38%)]\tLoss: 18.620354\n",
      "Train Epoch: 15 [2560/3276 (77%)]\tLoss: 18.432156\n",
      "\n",
      "Test set: Average loss: 17.7268, Accuracy: 3403356/9486336 (36%)\n",
      "\n",
      "Train Epoch: 16 [0/3276 (0%)]\tLoss: 18.042740\n",
      "Train Epoch: 16 [1280/3276 (38%)]\tLoss: 18.750898\n",
      "Train Epoch: 16 [2560/3276 (77%)]\tLoss: 17.577408\n",
      "\n",
      "Test set: Average loss: 17.7263, Accuracy: 3403358/9486336 (36%)\n",
      "\n",
      "Train Epoch: 17 [0/3276 (0%)]\tLoss: 18.084110\n",
      "Train Epoch: 17 [1280/3276 (38%)]\tLoss: 18.167248\n",
      "Train Epoch: 17 [2560/3276 (77%)]\tLoss: 18.692318\n",
      "\n",
      "Test set: Average loss: 17.7258, Accuracy: 3403355/9486336 (36%)\n",
      "\n",
      "Train Epoch: 18 [0/3276 (0%)]\tLoss: 18.483856\n",
      "Train Epoch: 18 [1280/3276 (38%)]\tLoss: 17.448076\n",
      "Train Epoch: 18 [2560/3276 (77%)]\tLoss: 19.746176\n",
      "\n",
      "Test set: Average loss: 17.7254, Accuracy: 3403358/9486336 (36%)\n",
      "\n",
      "Train Epoch: 19 [0/3276 (0%)]\tLoss: 19.803831\n",
      "Train Epoch: 19 [1280/3276 (38%)]\tLoss: 18.669788\n",
      "Train Epoch: 19 [2560/3276 (77%)]\tLoss: 16.232927\n",
      "\n",
      "Test set: Average loss: 17.7250, Accuracy: 3403350/9486336 (36%)\n",
      "\n",
      "Train Epoch: 20 [0/3276 (0%)]\tLoss: 18.666759\n",
      "Train Epoch: 20 [1280/3276 (38%)]\tLoss: 19.741011\n",
      "Train Epoch: 20 [2560/3276 (77%)]\tLoss: 19.086426\n",
      "\n",
      "Test set: Average loss: 17.7247, Accuracy: 3403348/9486336 (36%)\n",
      "\n",
      "Train Epoch: 21 [0/3276 (0%)]\tLoss: 17.224277\n",
      "Train Epoch: 21 [1280/3276 (38%)]\tLoss: 17.601624\n",
      "Train Epoch: 21 [2560/3276 (77%)]\tLoss: 18.248699\n",
      "\n",
      "Test set: Average loss: 17.7244, Accuracy: 3403352/9486336 (36%)\n",
      "\n",
      "Train Epoch: 22 [0/3276 (0%)]\tLoss: 18.833059\n",
      "Train Epoch: 22 [1280/3276 (38%)]\tLoss: 16.538231\n",
      "Train Epoch: 22 [2560/3276 (77%)]\tLoss: 18.188751\n",
      "\n",
      "Test set: Average loss: 17.7241, Accuracy: 3403352/9486336 (36%)\n",
      "\n",
      "Train Epoch: 23 [0/3276 (0%)]\tLoss: 18.438164\n",
      "Train Epoch: 23 [1280/3276 (38%)]\tLoss: 18.295105\n",
      "Train Epoch: 23 [2560/3276 (77%)]\tLoss: 17.572533\n",
      "\n",
      "Test set: Average loss: 17.7239, Accuracy: 3403356/9486336 (36%)\n",
      "\n",
      "Train Epoch: 24 [0/3276 (0%)]\tLoss: 18.409124\n",
      "Train Epoch: 24 [1280/3276 (38%)]\tLoss: 18.621910\n",
      "Train Epoch: 24 [2560/3276 (77%)]\tLoss: 17.213923\n",
      "\n",
      "Test set: Average loss: 17.7236, Accuracy: 3403352/9486336 (36%)\n",
      "\n",
      "Train Epoch: 25 [0/3276 (0%)]\tLoss: 18.784969\n",
      "Train Epoch: 25 [1280/3276 (38%)]\tLoss: 17.667000\n",
      "Train Epoch: 25 [2560/3276 (77%)]\tLoss: 17.113998\n",
      "\n",
      "Test set: Average loss: 17.7234, Accuracy: 3403354/9486336 (36%)\n",
      "\n",
      "Train Epoch: 26 [0/3276 (0%)]\tLoss: 18.556980\n",
      "Train Epoch: 26 [1280/3276 (38%)]\tLoss: 17.946375\n",
      "Train Epoch: 26 [2560/3276 (77%)]\tLoss: 18.235077\n",
      "\n",
      "Test set: Average loss: 17.7232, Accuracy: 3403359/9486336 (36%)\n",
      "\n",
      "Train Epoch: 27 [0/3276 (0%)]\tLoss: 18.427967\n",
      "Train Epoch: 27 [1280/3276 (38%)]\tLoss: 18.684704\n",
      "Train Epoch: 27 [2560/3276 (77%)]\tLoss: 17.833170\n",
      "\n",
      "Test set: Average loss: 17.7230, Accuracy: 3403353/9486336 (36%)\n",
      "\n",
      "Train Epoch: 28 [0/3276 (0%)]\tLoss: 17.862843\n",
      "Train Epoch: 28 [1280/3276 (38%)]\tLoss: 16.954153\n",
      "Train Epoch: 28 [2560/3276 (77%)]\tLoss: 17.751667\n",
      "\n",
      "Test set: Average loss: 17.7229, Accuracy: 3403352/9486336 (36%)\n",
      "\n",
      "Train Epoch: 29 [0/3276 (0%)]\tLoss: 17.682100\n",
      "Train Epoch: 29 [1280/3276 (38%)]\tLoss: 18.999992\n",
      "Train Epoch: 29 [2560/3276 (77%)]\tLoss: 17.083248\n",
      "\n",
      "Test set: Average loss: 17.7227, Accuracy: 3403350/9486336 (36%)\n",
      "\n",
      "Train Epoch: 30 [0/3276 (0%)]\tLoss: 19.518108\n",
      "Train Epoch: 30 [1280/3276 (38%)]\tLoss: 18.735455\n",
      "Train Epoch: 30 [2560/3276 (77%)]\tLoss: 18.656588\n",
      "\n",
      "Test set: Average loss: 17.7225, Accuracy: 3403340/9486336 (36%)\n",
      "\n",
      "Train Epoch: 31 [0/3276 (0%)]\tLoss: 17.861261\n",
      "Train Epoch: 31 [1280/3276 (38%)]\tLoss: 17.450739\n",
      "Train Epoch: 31 [2560/3276 (77%)]\tLoss: 18.024927\n",
      "\n",
      "Test set: Average loss: 17.7224, Accuracy: 3403345/9486336 (36%)\n",
      "\n",
      "Train Epoch: 32 [0/3276 (0%)]\tLoss: 19.023840\n",
      "Train Epoch: 32 [1280/3276 (38%)]\tLoss: 18.758381\n",
      "Train Epoch: 32 [2560/3276 (77%)]\tLoss: 17.644104\n",
      "\n",
      "Test set: Average loss: 17.7223, Accuracy: 3403346/9486336 (36%)\n",
      "\n",
      "Train Epoch: 33 [0/3276 (0%)]\tLoss: 18.571474\n",
      "Train Epoch: 33 [1280/3276 (38%)]\tLoss: 18.112043\n",
      "Train Epoch: 33 [2560/3276 (77%)]\tLoss: 17.970486\n",
      "\n",
      "Test set: Average loss: 17.7221, Accuracy: 3403349/9486336 (36%)\n",
      "\n",
      "Train Epoch: 34 [0/3276 (0%)]\tLoss: 18.468731\n",
      "Train Epoch: 34 [1280/3276 (38%)]\tLoss: 18.260399\n",
      "Train Epoch: 34 [2560/3276 (77%)]\tLoss: 17.314953\n",
      "\n",
      "Test set: Average loss: 17.7220, Accuracy: 3403350/9486336 (36%)\n",
      "\n",
      "Train Epoch: 35 [0/3276 (0%)]\tLoss: 18.406252\n",
      "Train Epoch: 35 [1280/3276 (38%)]\tLoss: 17.623917\n",
      "Train Epoch: 35 [2560/3276 (77%)]\tLoss: 17.761600\n",
      "\n",
      "Test set: Average loss: 17.7219, Accuracy: 3403352/9486336 (36%)\n",
      "\n",
      "Train Epoch: 36 [0/3276 (0%)]\tLoss: 18.959148\n",
      "Train Epoch: 36 [1280/3276 (38%)]\tLoss: 18.139212\n",
      "Train Epoch: 36 [2560/3276 (77%)]\tLoss: 19.214966\n",
      "\n",
      "Test set: Average loss: 17.7218, Accuracy: 3403354/9486336 (36%)\n",
      "\n",
      "Train Epoch: 37 [0/3276 (0%)]\tLoss: 17.740652\n",
      "Train Epoch: 37 [1280/3276 (38%)]\tLoss: 19.745937\n",
      "Train Epoch: 37 [2560/3276 (77%)]\tLoss: 17.306231\n",
      "\n",
      "Test set: Average loss: 17.7217, Accuracy: 3403358/9486336 (36%)\n",
      "\n",
      "Train Epoch: 38 [0/3276 (0%)]\tLoss: 17.612297\n",
      "Train Epoch: 38 [1280/3276 (38%)]\tLoss: 18.630262\n",
      "Train Epoch: 38 [2560/3276 (77%)]\tLoss: 17.529213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 17.7216, Accuracy: 3403360/9486336 (36%)\n",
      "\n",
      "Train Epoch: 39 [0/3276 (0%)]\tLoss: 18.828503\n",
      "Train Epoch: 39 [1280/3276 (38%)]\tLoss: 18.110041\n",
      "Train Epoch: 39 [2560/3276 (77%)]\tLoss: 18.242428\n",
      "\n",
      "Test set: Average loss: 17.7215, Accuracy: 3403369/9486336 (36%)\n",
      "\n",
      "Train Epoch: 40 [0/3276 (0%)]\tLoss: 18.372892\n",
      "Train Epoch: 40 [1280/3276 (38%)]\tLoss: 17.989168\n",
      "Train Epoch: 40 [2560/3276 (77%)]\tLoss: 18.734718\n",
      "\n",
      "Test set: Average loss: 17.7214, Accuracy: 3403373/9486336 (36%)\n",
      "\n",
      "Train Epoch: 41 [0/3276 (0%)]\tLoss: 17.868956\n",
      "Train Epoch: 41 [1280/3276 (38%)]\tLoss: 18.170410\n",
      "Train Epoch: 41 [2560/3276 (77%)]\tLoss: 17.719439\n",
      "\n",
      "Test set: Average loss: 17.7214, Accuracy: 3403373/9486336 (36%)\n",
      "\n",
      "Train Epoch: 42 [0/3276 (0%)]\tLoss: 17.905266\n",
      "Train Epoch: 42 [1280/3276 (38%)]\tLoss: 18.695850\n",
      "Train Epoch: 42 [2560/3276 (77%)]\tLoss: 17.812984\n",
      "\n",
      "Test set: Average loss: 17.7213, Accuracy: 3403378/9486336 (36%)\n",
      "\n",
      "Train Epoch: 43 [0/3276 (0%)]\tLoss: 18.441404\n",
      "Train Epoch: 43 [1280/3276 (38%)]\tLoss: 18.959518\n",
      "Train Epoch: 43 [2560/3276 (77%)]\tLoss: 19.115965\n",
      "\n",
      "Test set: Average loss: 17.7212, Accuracy: 3403380/9486336 (36%)\n",
      "\n",
      "Train Epoch: 44 [0/3276 (0%)]\tLoss: 18.010803\n",
      "Train Epoch: 44 [1280/3276 (38%)]\tLoss: 18.614162\n",
      "Train Epoch: 44 [2560/3276 (77%)]\tLoss: 18.293602\n",
      "\n",
      "Test set: Average loss: 17.7211, Accuracy: 3403385/9486336 (36%)\n",
      "\n",
      "Train Epoch: 45 [0/3276 (0%)]\tLoss: 17.617619\n",
      "Train Epoch: 45 [1280/3276 (38%)]\tLoss: 17.615591\n",
      "Train Epoch: 45 [2560/3276 (77%)]\tLoss: 18.117550\n",
      "\n",
      "Test set: Average loss: 17.7211, Accuracy: 3403388/9486336 (36%)\n",
      "\n",
      "Train Epoch: 46 [0/3276 (0%)]\tLoss: 18.305143\n",
      "Train Epoch: 46 [1280/3276 (38%)]\tLoss: 18.463039\n",
      "Train Epoch: 46 [2560/3276 (77%)]\tLoss: 18.387993\n",
      "\n",
      "Test set: Average loss: 17.7210, Accuracy: 3403392/9486336 (36%)\n",
      "\n",
      "Train Epoch: 47 [0/3276 (0%)]\tLoss: 18.245827\n",
      "Train Epoch: 47 [1280/3276 (38%)]\tLoss: 16.837315\n",
      "Train Epoch: 47 [2560/3276 (77%)]\tLoss: 18.031174\n",
      "\n",
      "Test set: Average loss: 17.7209, Accuracy: 3403386/9486336 (36%)\n",
      "\n",
      "Train Epoch: 48 [0/3276 (0%)]\tLoss: 18.257685\n",
      "Train Epoch: 48 [1280/3276 (38%)]\tLoss: 16.855576\n",
      "Train Epoch: 48 [2560/3276 (77%)]\tLoss: 18.088724\n",
      "\n",
      "Test set: Average loss: 17.7209, Accuracy: 3403385/9486336 (36%)\n",
      "\n",
      "Train Epoch: 49 [0/3276 (0%)]\tLoss: 18.126299\n",
      "Train Epoch: 49 [1280/3276 (38%)]\tLoss: 18.164904\n",
      "Train Epoch: 49 [2560/3276 (77%)]\tLoss: 18.420113\n",
      "\n",
      "Test set: Average loss: 17.7208, Accuracy: 3403385/9486336 (36%)\n",
      "\n",
      "Train Epoch: 50 [0/3276 (0%)]\tLoss: 18.828766\n",
      "Train Epoch: 50 [1280/3276 (38%)]\tLoss: 16.856260\n",
      "Train Epoch: 50 [2560/3276 (77%)]\tLoss: 18.577429\n",
      "\n",
      "Test set: Average loss: 17.7207, Accuracy: 3403381/9486336 (36%)\n",
      "\n",
      "Train Epoch: 51 [0/3276 (0%)]\tLoss: 17.789585\n",
      "Train Epoch: 51 [1280/3276 (38%)]\tLoss: 17.441858\n",
      "Train Epoch: 51 [2560/3276 (77%)]\tLoss: 17.830853\n",
      "\n",
      "Test set: Average loss: 17.7207, Accuracy: 3403382/9486336 (36%)\n",
      "\n",
      "Train Epoch: 52 [0/3276 (0%)]\tLoss: 18.028933\n",
      "Train Epoch: 52 [1280/3276 (38%)]\tLoss: 18.115522\n",
      "Train Epoch: 52 [2560/3276 (77%)]\tLoss: 19.196861\n",
      "\n",
      "Test set: Average loss: 17.7206, Accuracy: 3403382/9486336 (36%)\n",
      "\n",
      "Train Epoch: 53 [0/3276 (0%)]\tLoss: 17.383860\n",
      "Train Epoch: 53 [1280/3276 (38%)]\tLoss: 18.759119\n",
      "Train Epoch: 53 [2560/3276 (77%)]\tLoss: 19.558187\n",
      "\n",
      "Test set: Average loss: 17.7206, Accuracy: 3403382/9486336 (36%)\n",
      "\n",
      "Train Epoch: 54 [0/3276 (0%)]\tLoss: 20.029341\n",
      "Train Epoch: 54 [1280/3276 (38%)]\tLoss: 17.976072\n",
      "Train Epoch: 54 [2560/3276 (77%)]\tLoss: 19.048742\n",
      "\n",
      "Test set: Average loss: 17.7263, Accuracy: 3294502/9486336 (35%)\n",
      "\n",
      "Train Epoch: 55 [0/3276 (0%)]\tLoss: 17.773643\n",
      "Train Epoch: 55 [1280/3276 (38%)]\tLoss: 16.944561\n",
      "Train Epoch: 55 [2560/3276 (77%)]\tLoss: 19.317867\n",
      "\n",
      "Test set: Average loss: 17.7318, Accuracy: 3294059/9486336 (35%)\n",
      "\n",
      "Train Epoch: 56 [0/3276 (0%)]\tLoss: 18.462803\n",
      "Train Epoch: 56 [1280/3276 (38%)]\tLoss: 18.121529\n",
      "Train Epoch: 56 [2560/3276 (77%)]\tLoss: 18.562725\n",
      "\n",
      "Test set: Average loss: 17.7372, Accuracy: 3294053/9486336 (35%)\n",
      "\n",
      "Train Epoch: 57 [0/3276 (0%)]\tLoss: 17.079243\n",
      "Train Epoch: 57 [1280/3276 (38%)]\tLoss: 18.133915\n",
      "Train Epoch: 57 [2560/3276 (77%)]\tLoss: 19.332647\n",
      "\n",
      "Test set: Average loss: 17.7423, Accuracy: 3294043/9486336 (35%)\n",
      "\n",
      "Train Epoch: 58 [0/3276 (0%)]\tLoss: 18.005112\n",
      "Train Epoch: 58 [1280/3276 (38%)]\tLoss: 18.216183\n",
      "Train Epoch: 58 [2560/3276 (77%)]\tLoss: 17.922159\n",
      "\n",
      "Test set: Average loss: 17.7473, Accuracy: 3294048/9486336 (35%)\n",
      "\n",
      "Train Epoch: 59 [0/3276 (0%)]\tLoss: 17.374214\n",
      "Train Epoch: 59 [1280/3276 (38%)]\tLoss: 19.636713\n",
      "Train Epoch: 59 [2560/3276 (77%)]\tLoss: 18.669447\n",
      "\n",
      "Test set: Average loss: 17.7521, Accuracy: 3294050/9486336 (35%)\n",
      "\n",
      "Train Epoch: 60 [0/3276 (0%)]\tLoss: 18.309465\n",
      "Train Epoch: 60 [1280/3276 (38%)]\tLoss: 17.702471\n",
      "Train Epoch: 60 [2560/3276 (77%)]\tLoss: 19.086845\n",
      "\n",
      "Test set: Average loss: 17.7568, Accuracy: 3294047/9486336 (35%)\n",
      "\n",
      "Train Epoch: 61 [0/3276 (0%)]\tLoss: 18.260189\n",
      "Train Epoch: 61 [1280/3276 (38%)]\tLoss: 18.091385\n",
      "Train Epoch: 61 [2560/3276 (77%)]\tLoss: 19.085054\n",
      "\n",
      "Test set: Average loss: 17.7613, Accuracy: 3294052/9486336 (35%)\n",
      "\n",
      "Train Epoch: 62 [0/3276 (0%)]\tLoss: 18.977516\n",
      "Train Epoch: 62 [1280/3276 (38%)]\tLoss: 17.437958\n",
      "Train Epoch: 62 [2560/3276 (77%)]\tLoss: 17.950563\n",
      "\n",
      "Test set: Average loss: 17.7657, Accuracy: 3294051/9486336 (35%)\n",
      "\n",
      "Train Epoch: 63 [0/3276 (0%)]\tLoss: 17.862289\n",
      "Train Epoch: 63 [1280/3276 (38%)]\tLoss: 18.282665\n",
      "Train Epoch: 63 [2560/3276 (77%)]\tLoss: 19.330832\n",
      "\n",
      "Test set: Average loss: 17.7699, Accuracy: 3294047/9486336 (35%)\n",
      "\n",
      "Train Epoch: 64 [0/3276 (0%)]\tLoss: 17.597013\n",
      "Train Epoch: 64 [1280/3276 (38%)]\tLoss: 19.259867\n",
      "Train Epoch: 64 [2560/3276 (77%)]\tLoss: 18.581856\n",
      "\n",
      "Test set: Average loss: 17.7740, Accuracy: 3294046/9486336 (35%)\n",
      "\n",
      "Train Epoch: 65 [0/3276 (0%)]\tLoss: 17.440067\n",
      "Train Epoch: 65 [1280/3276 (38%)]\tLoss: 19.257259\n",
      "Train Epoch: 65 [2560/3276 (77%)]\tLoss: 17.852407\n",
      "\n",
      "Test set: Average loss: 17.7780, Accuracy: 3294046/9486336 (35%)\n",
      "\n",
      "Train Epoch: 66 [0/3276 (0%)]\tLoss: 17.096870\n",
      "Train Epoch: 66 [1280/3276 (38%)]\tLoss: 18.720831\n",
      "Train Epoch: 66 [2560/3276 (77%)]\tLoss: 18.129066\n",
      "\n",
      "Test set: Average loss: 17.7818, Accuracy: 3294049/9486336 (35%)\n",
      "\n",
      "Train Epoch: 67 [0/3276 (0%)]\tLoss: 17.747135\n",
      "Train Epoch: 67 [1280/3276 (38%)]\tLoss: 18.149778\n",
      "Train Epoch: 67 [2560/3276 (77%)]\tLoss: 18.767260\n",
      "\n",
      "Test set: Average loss: 17.7856, Accuracy: 3294049/9486336 (35%)\n",
      "\n",
      "Train Epoch: 68 [0/3276 (0%)]\tLoss: 16.465158\n",
      "Train Epoch: 68 [1280/3276 (38%)]\tLoss: 18.609207\n",
      "Train Epoch: 68 [2560/3276 (77%)]\tLoss: 18.379770\n",
      "\n",
      "Test set: Average loss: 17.7892, Accuracy: 3294051/9486336 (35%)\n",
      "\n",
      "Train Epoch: 69 [0/3276 (0%)]\tLoss: 18.426016\n",
      "Train Epoch: 69 [1280/3276 (38%)]\tLoss: 18.519062\n",
      "Train Epoch: 69 [2560/3276 (77%)]\tLoss: 17.458328\n",
      "\n",
      "Test set: Average loss: 17.7928, Accuracy: 3294049/9486336 (35%)\n",
      "\n",
      "Train Epoch: 70 [0/3276 (0%)]\tLoss: 17.889088\n",
      "Train Epoch: 70 [1280/3276 (38%)]\tLoss: 18.695139\n",
      "Train Epoch: 70 [2560/3276 (77%)]\tLoss: 18.925129\n",
      "\n",
      "Test set: Average loss: 17.7962, Accuracy: 3294050/9486336 (35%)\n",
      "\n",
      "Train Epoch: 71 [0/3276 (0%)]\tLoss: 18.786314\n",
      "Train Epoch: 71 [1280/3276 (38%)]\tLoss: 18.796062\n",
      "Train Epoch: 71 [2560/3276 (77%)]\tLoss: 18.506201\n",
      "\n",
      "Test set: Average loss: 17.7995, Accuracy: 3294039/9486336 (35%)\n",
      "\n",
      "Train Epoch: 72 [0/3276 (0%)]\tLoss: 18.200241\n",
      "Train Epoch: 72 [1280/3276 (38%)]\tLoss: 17.214951\n",
      "Train Epoch: 72 [2560/3276 (77%)]\tLoss: 17.355637\n",
      "\n",
      "Test set: Average loss: 17.8028, Accuracy: 3294043/9486336 (35%)\n",
      "\n",
      "Train Epoch: 73 [0/3276 (0%)]\tLoss: 17.037556\n",
      "Train Epoch: 73 [1280/3276 (38%)]\tLoss: 18.296711\n",
      "Train Epoch: 73 [2560/3276 (77%)]\tLoss: 17.451504\n",
      "\n",
      "Test set: Average loss: 17.8059, Accuracy: 3294042/9486336 (35%)\n",
      "\n",
      "Train Epoch: 74 [0/3276 (0%)]\tLoss: 18.761465\n",
      "Train Epoch: 74 [1280/3276 (38%)]\tLoss: 17.763893\n",
      "Train Epoch: 74 [2560/3276 (77%)]\tLoss: 17.257929\n",
      "\n",
      "Test set: Average loss: 17.8090, Accuracy: 3294042/9486336 (35%)\n",
      "\n",
      "Train Epoch: 75 [0/3276 (0%)]\tLoss: 17.585497\n",
      "Train Epoch: 75 [1280/3276 (38%)]\tLoss: 18.089487\n",
      "Train Epoch: 75 [2560/3276 (77%)]\tLoss: 19.250961\n",
      "\n",
      "Test set: Average loss: 17.8127, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 76 [0/3276 (0%)]\tLoss: 16.727219\n",
      "Train Epoch: 76 [1280/3276 (38%)]\tLoss: 16.931808\n",
      "Train Epoch: 76 [2560/3276 (77%)]\tLoss: 18.512501\n",
      "\n",
      "Test set: Average loss: 17.8163, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 77 [0/3276 (0%)]\tLoss: 17.738913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 77 [1280/3276 (38%)]\tLoss: 17.949009\n",
      "Train Epoch: 77 [2560/3276 (77%)]\tLoss: 19.561823\n",
      "\n",
      "Test set: Average loss: 17.8197, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 78 [0/3276 (0%)]\tLoss: 18.293497\n",
      "Train Epoch: 78 [1280/3276 (38%)]\tLoss: 18.746655\n",
      "Train Epoch: 78 [2560/3276 (77%)]\tLoss: 19.239763\n",
      "\n",
      "Test set: Average loss: 17.8231, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 79 [0/3276 (0%)]\tLoss: 17.805872\n",
      "Train Epoch: 79 [1280/3276 (38%)]\tLoss: 17.577225\n",
      "Train Epoch: 79 [2560/3276 (77%)]\tLoss: 18.885708\n",
      "\n",
      "Test set: Average loss: 17.8265, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 80 [0/3276 (0%)]\tLoss: 19.089508\n",
      "Train Epoch: 80 [1280/3276 (38%)]\tLoss: 18.602463\n",
      "Train Epoch: 80 [2560/3276 (77%)]\tLoss: 17.294268\n",
      "\n",
      "Test set: Average loss: 17.8297, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 81 [0/3276 (0%)]\tLoss: 18.250912\n",
      "Train Epoch: 81 [1280/3276 (38%)]\tLoss: 18.195312\n",
      "Train Epoch: 81 [2560/3276 (77%)]\tLoss: 18.040922\n",
      "\n",
      "Test set: Average loss: 17.8328, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 82 [0/3276 (0%)]\tLoss: 18.899307\n",
      "Train Epoch: 82 [1280/3276 (38%)]\tLoss: 18.032490\n",
      "Train Epoch: 82 [2560/3276 (77%)]\tLoss: 18.051859\n",
      "\n",
      "Test set: Average loss: 17.8359, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 83 [0/3276 (0%)]\tLoss: 17.473980\n",
      "Train Epoch: 83 [1280/3276 (38%)]\tLoss: 17.403543\n",
      "Train Epoch: 83 [2560/3276 (77%)]\tLoss: 18.303221\n",
      "\n",
      "Test set: Average loss: 17.8389, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 84 [0/3276 (0%)]\tLoss: 19.169140\n",
      "Train Epoch: 84 [1280/3276 (38%)]\tLoss: 17.016342\n",
      "Train Epoch: 84 [2560/3276 (77%)]\tLoss: 17.700653\n",
      "\n",
      "Test set: Average loss: 17.8419, Accuracy: 3276245/9486336 (35%)\n",
      "\n",
      "Train Epoch: 85 [0/3276 (0%)]\tLoss: 17.906479\n",
      "Train Epoch: 85 [1280/3276 (38%)]\tLoss: 18.121635\n"
     ]
    }
   ],
   "source": [
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluate Accuracy\n",
    "print('Training Loss:', train_hist['train_loss_epoch'][-1])\n",
    "print('Training Accuracy:', train_hist['train_accu'][-1])\n",
    "print()\n",
    "print('Test Loss:', np.mean(train_hist['test_loss_epoch']))\n",
    "print('Testing Accuracy:', np.max(train_hist['test_loss_epoch']))\n",
    "print()\n",
    "\n",
    "plt.plot(train_hist['train_loss_epoch'],'r', label='Training Loss')\n",
    "plt.plot(train_hist['test_loss_epoch'],'b', label='Testing Loss')\n",
    "plt.title('Test Loss ' + str(train_hist['test_loss_epoch'][-1]))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(train_hist['train_accu'],'r', label='Training accuracy')\n",
    "plt.plot(train_hist['test_accu'],'b', label='Testing accuracy')\n",
    "plt.title('Test Accuracy : '+ str(np.max(train_hist['test_accu'])))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences\n",
    "* It is clear from the abive graph that, there is overfitting in the network.\n",
    "* I tried different menthods like changing dropouts and actiavtion functions to do address this.\n",
    "* This did not help as, even though the overfitting decreased the final test accuracy always decreased.\n",
    "* Adam gave beteer performance than SGD.\n",
    "* The haphazard motion of both error and accuracy is due to the random shuffling of data in minibatches after each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = []\n",
    "mask = []\n",
    "pred= []\n",
    "with torch.no_grad(): # as we dont need to backpropogate when calculating testing error and accuracy\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        #==== Getting the Prediction====\n",
    "        output = net(data)\n",
    "        img.append(np.transpose(data[0], (1,2,0)).numpy())\n",
    "        mask.append(np.transpose(target[0], (1,2,0))[:,:,0].numpy())\n",
    "        pred.append(np.transpose(output[0], (1,2,0))[:,:,0].numpy())\n",
    "#         break\n",
    "# # output[0].shape\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.imshow(np.transpose(data[0], (1,2,0))[:,:,0].numpy())\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.imshow(np.transpose(output[0],(1,2,0))[:,:,0].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "num = random.randint(1,100)\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(img[num])\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(mask[num])\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(pred[num])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
