{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2 as cv\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='skimage')\n",
    "seed = 42\n",
    "random.seed = seed\n",
    "np.random.seed = seed\n",
    "IMG_CHANNELS = 3\n",
    "lrn = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.eager import context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir='./logs/feat/', **kwargs):\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation_lr_'+ str(lrn))\n",
    "        training_log_dir = os.path.join(log_dir, 'training_lr_'+ str(lrn))\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "    def set_model(self, model):\n",
    "        if context.executing_eagerly():\n",
    "            self.val_writer = tf.contrib.summary.create_file_writer(self.val_log_dir)\n",
    "        else:\n",
    "            self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def _write_custom_summaries(self, step, logs=None):\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if 'val_' in k}\n",
    "        if context.executing_eagerly():\n",
    "            with self.val_writer.as_default(), tf.contrib.summary.always_record_summaries():\n",
    "                for name, value in val_logs.items():\n",
    "                    tf.contrib.summary.scalar(name, value.item(), step=step)\n",
    "        else:\n",
    "            for name, value in val_logs.items():\n",
    "                summary = tf.Summary()\n",
    "                summary_value = summary.value.add()\n",
    "                summary_value.simple_value = value.item()\n",
    "                summary_value.tag = name\n",
    "                self.val_writer.add_summary(summary, step)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        logs = {k: v for k, v in logs.items() if not 'val_' in k}\n",
    "        super(TrainValTensorBoard, self)._write_custom_summaries(step, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_comp(msk, preds_test_t):\n",
    "    \n",
    "    act = msk.reshape(len(msk),1)\n",
    "    pr = preds_test_t\n",
    "\n",
    "    c = act == prpower\n",
    "    d = act & pr\n",
    "    e = act | pr\n",
    "    neg = act.sum()\n",
    "    pos = (len(act))-act.sum()\n",
    "\n",
    "    TN = float(d.sum())\n",
    "    FN = float(pr.sum()-d.sum())\n",
    "    TP = float((len(act))-e.sum())\n",
    "    FP = float(e.sum()-pr.sum())\n",
    "    acc = float(c.sum())/(len(act))\n",
    "\n",
    "    acc2 = float(TP+TN)/(TP+TN+FP+FN)\n",
    "#     mean_TP = round(TP/pos,6)\n",
    "#     mean_TN = round(TN/neg,6)\n",
    "#     mean_FP = round(FP/pos,6)\n",
    "#     mean_FN = round(FN/neg,6)\n",
    "    \n",
    "    return (acc,TP,TN,FP,FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class for extracting time per epoch\n",
    "class TimingCallback(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "cb = TimingCallback() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Dataset 1: HGR\n",
    "# TRAIN_PATH1 = ['../Input/Skin_Datasets/Dataset1_HGR/original_images/']\n",
    "# MASK_PATH1 = ['../Input/Skin_Datasets/Dataset1_HGR/skin_masks/']\n",
    "# train_ids1 = next(os.walk(TRAIN_PATH1[0]))[2]\n",
    "# mask_ids1 = next(os.walk(MASK_PATH1[0]))[2]\n",
    "# train_ids1.sort()\n",
    "# mask_ids1.sort()\n",
    "# TRAIN_PATH1 = TRAIN_PATH1*len(train_ids1)\n",
    "# MASK_PATH1 = MASK_PATH1*len(train_ids1)\n",
    "\n",
    "# # Dataset 2: TDSD\n",
    "# TRAIN_PATH5 = ['../Input/Skin_Datasets/Dataset2_TDSD/original_images/']\n",
    "# MASK_PATH5 = ['../Input/Skin_Datasets/Dataset2_TDSD/skin_masks/']\n",
    "# train_ids5 = next(os.walk(TRAIN_PATH5[0]))[2]\n",
    "# mask_ids5 = next(os.walk(MASK_PATH5[0]))[2]\n",
    "# train_ids5.sort()\n",
    "# mask_ids5.sort()\n",
    "# TRAIN_PATH5 = TRAIN_PATH5*len(train_ids5)\n",
    "# MASK_PATH5 = MASK_PATH5*len(train_ids5)\n",
    "\n",
    "# # Dataset 3: Schmugge\n",
    "# TRAIN_PATH6 = ['../Input/Skin_Datasets/Dataset3_Schmugge/original_images/']\n",
    "# MASK_PATH6 = ['../Input/Skin_Datasets/Dataset3_Schmugge/skin_masks/']\n",
    "# train_ids6 = next(os.walk(TRAIN_PATH6[0]))[2]\n",
    "# mask_ids6 = next(os.walk(MASK_PATH6[0]))[2]\n",
    "# train_ids6.sort()\n",
    "# mask_ids6.sort()\n",
    "# TRAIN_PATH6 = TRAIN_PATH6*len(train_ids6)\n",
    "# MASK_PATH6 = MASK_PATH6*len(train_ids6)\n",
    "\n",
    "# # # Dataset 4: Pratheepan\n",
    "# # TRAIN_PATH2 = ['../Input/Skin_Datasets/Dataset4_Pratheepan/original_images/']\n",
    "# # MASK_PATH2 = ['../Input/Skin_Datasets/Dataset4_Pratheepan/skin_masks/']\n",
    "# # train_ids2 = next(os.walk(TRAIN_PATH2[0]))[2]\n",
    "# # mask_ids2 = next(os.walk(MASK_PATH2[0]))[2]\n",
    "# # train_ids2.sort()\n",
    "# # mask_ids2.sort()\n",
    "# # TRAIN_PATH2 = TRAIN_PATH2*len(train_ids2)\n",
    "# # MASK_PATH2 = MASK_PATH2*len(train_ids2)\n",
    "\n",
    "# # Dataset 5: VDM\n",
    "# TRAIN_PATH3 = ['../Input/Skin_Datasets/Dataset5_VDM/original_images/']\n",
    "# MASK_PATH3 = ['../Input/Skin_Datasets/Dataset5_VDM/skin_masks/']\n",
    "# train_id3 = next(os.walk(TRAIN_PATH3[0]))[2]\n",
    "# mask_id3 = next(os.walk(MASK_PATH3[0]))[2]\n",
    "# train_id3.sort()\n",
    "# mask_id3.sort()\n",
    "# train_ids3 = train_id3[1:]\n",
    "# mask_ids3 = mask_id3[1:]\n",
    "# TRAIN_PATH3 = TRAIN_PATH3*len(train_ids3)\n",
    "# MASK_PATH3 = MASK_PATH3*len(train_ids3)\n",
    "\n",
    "# # Dataset 6: SFA\n",
    "# TRAIN_PATH4 = ['../Input/Skin_Datasets/Dataset6_SFA/original_images/']\n",
    "# MASK_PATH4 = ['../Input/Skin_Datasets/Dataset6_SFA/skin_masks/']\n",
    "# train_ids4 = next(os.walk(TRAIN_PATH4[0]))[2]\n",
    "# mask_ids4 = next(os.walk(MASK_PATH4[0]))[2]\n",
    "# train_ids4.sort()\n",
    "# mask_ids4.sort()\n",
    "# TRAIN_PATH4 = TRAIN_PATH4*len(train_ids4)\n",
    "# MASK_PATH4 = MASK_PATH4*len(train_ids4)\n",
    "\n",
    "# # Dataset 7: FSD\n",
    "# TRAIN_PATH7 = ['../Input/Skin_Datasets/Dataset7_FSD/original_images/']\n",
    "# MASK_PATH7 = ['../Input/Skin_Datasets/Dataset7_FSD/skin_masks/']\n",
    "# train_ids7 = next(os.walk(TRAIN_PATH7[0]))[2]\n",
    "# mask_ids7 = next(os.walk(MASK_PATH7[0]))[2]\n",
    "# train_ids7.sort()\n",
    "# mask_ids7.sort()\n",
    "# TRAIN_PATH7 = TRAIN_PATH7*len(train_ids7)\n",
    "# MASK_PATH7 = MASK_PATH7*len(train_ids7)\n",
    "\n",
    "# # ## Dataset 8: ABDOMEN\n",
    "# # TRAIN_PATH8 = ['../Input/Skin_Datasets/Dataset8_Abdomen/train/skin_train2019/']\n",
    "# # MASK_PATH8 = ['../Input/Skin_Datasets/Dataset8_Abdomen/train/annotations/']\n",
    "# # train_ids8 = next(os.walk(TRAIN_PATH8[0]))[2]\n",
    "# # mask_ids8 = next(os.walk(MASK_PATH8[0]))[2]\n",
    "# # train_ids8.sort()\n",
    "# # mask_ids8.sort()\n",
    "\n",
    "# # TRAIN_PATH8 = TRAIN_PATH8*len(train_ids8)\n",
    "# # MASK_PATH8 = MASK_PATH8*len(train_ids8)\n",
    "\n",
    "# # ## Dataset 8: ABDOMEN_New\n",
    "# TRAIN_PATH9 = ['../Input/Skin_Datasets/Dataset_NEW/original_images/']\n",
    "# MASK_PATH9 = ['../Input/Skin_Datasets/Dataset_NEW/skin_masks/']\n",
    "# train_ids9 = next(os.walk(TRAIN_PATH9[0]))[2]\n",
    "# mask_ids9 = next(os.walk(MASK_PATH9[0]))[2]\n",
    "# train_ids9.sort()\n",
    "# mask_ids9.sort()\n",
    "\n",
    "# TRAIN_PATH9 = TRAIN_PATH9*len(train_ids9)\n",
    "# MASK_PATH9 = MASK_PATH9*len(train_ids9)\n",
    "\n",
    "# # TRAIN_PATH2,MASK_PATH2,train_ids2,mask_ids2\n",
    "# # Combine everything\n",
    "# # TRAIN_PATH = np.concatenate((TRAIN_PATH1,TRAIN_PATH3,TRAIN_PATH4,TRAIN_PATH5, TRAIN_PATH6,TRAIN_PATH7)) #,TRAIN_PATH9\n",
    "# # MASK_PATH = np.concatenate((MASK_PATH1,MASK_PATH3,MASK_PATH4,MASK_PATH5,MASK_PATH6,MASK_PATH7))#,MASK_PATH9\n",
    "# # train_ids = np.concatenate((train_ids1,train_ids3,train_ids4,train_ids5,train_ids6,train_ids7)) #,train_ids9\n",
    "# # mask_ids = np.concatenate((mask_ids1,mask_ids3,mask_ids4,mask_ids5,mask_ids6,mask_ids7)) # ,mask_ids9\n",
    "\n",
    "# TRAIN_PATH = TRAIN_PATH9\n",
    "# MASK_PATH = MASK_PATH9\n",
    "# train_ids = train_ids9\n",
    "# mask_ids = mask_ids9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  # Extract Features and Corresponding Labels (RUN ONCE ONLY TO GET THE DATA)\n",
    "# print('Getting features and labels from images unreshaped... ')\n",
    "# sys.stdout.flush()\n",
    "\n",
    "# path = TRAIN_PATH[0] + train_ids[0]\n",
    "# img = imread(path)[:,:,:IMG_CHANNELS]\n",
    "# dat = img.reshape(img.shape[0]*img.shape[1],3)\n",
    "# hsv = cv.cvtColor(img, cv.COLOR_RGB2HSV)\n",
    "# hsv = hsv.reshape(img.shape[0]*img.shape[1],3)\n",
    "# lab = cv.cvtColor(img, cv.COLOR_RGB2Lab)\n",
    "# lab = lab.reshape(img.shape[0]*img.shape[1],3)\n",
    "\n",
    "# path = MASK_PATH[0] + mask_ids[0]\n",
    "# img = imread(path)\n",
    "# if img.ndim == 3:\n",
    "#     img = img[:,:,1]   \n",
    "# if (np.unique(img).size) > 2:\n",
    "#     img = img > 30     # Important, needed to make labels 0's and 1's only \n",
    "# else:\n",
    "#     img = img > 0\n",
    "# L = img.reshape(img.shape[0]*img.shape[1],1)\n",
    "\n",
    "# dat = np.concatenate((dat,hsv,lab,L),axis=1)\n",
    "\n",
    "# for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n",
    "#     path = TRAIN_PATH[n] + id_\n",
    "#     img = imread(path)[:,:,:IMG_CHANNELS]\n",
    "#     b = img.reshape(img.shape[0]*img.shape[1],3)\n",
    "#     hsv = cv.cvtColor(img, cv.COLOR_RGB2HSV)\n",
    "#     hsv = hsv.reshape(img.shape[0]*img.shape[1],3)\n",
    "#     lab = cv.cvtColor(img, cv.COLOR_RGB2Lab)\n",
    "#     lab = lab.reshape(img.shape[0]*img.shape[1],3)\n",
    "    \n",
    "#     path = MASK_PATH[n] + mask_ids[n]\n",
    "#     img = imread(path)\n",
    "#     if img.ndim == 3:\n",
    "#         img = img[:,:,1]\n",
    "        \n",
    "#     if (np.unique(img).size) > 2:\n",
    "#         img = img > 30     # Important, needed to make labels 0's and 1's only \n",
    "#     else:\n",
    "#         img = img > 0\n",
    "        \n",
    "#     L = img.reshape(img.shape[0]*img.shape[1],1)\n",
    "#     dat_temp = np.concatenate((b,hsv,lab,L),axis=1)\n",
    "#     dat = np.concatenate((dat,dat_temp))\n",
    "\n",
    "# ABD = dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dat.shape)\n",
    "# print(np.sum(dat[:,9]))\n",
    "# # np.save('VDM.npy',VDM)\n",
    "# print(np.unique(dat[:,9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the data then shuffle\n",
    "# ABD = np.load('ABD_TRAIN.npy')\n",
    "\n",
    "# FSD = np.load('FSD_TRAIN.npy')\n",
    "\n",
    "# HGR = np.load('HGR.npy')\n",
    "\n",
    "# PR = np.load('PR.npy')\n",
    "\n",
    "# SCHMG = np.load('SCHMG.npy')\n",
    "\n",
    "# SFA = np.load('SFA.npy')\n",
    "\n",
    "# TDSD = np.load('TDSD.npy')\n",
    "\n",
    "# VDM = np.load('VDM.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract and Balance Data\n",
    "# L=np.where(ABD[:,9] > 0)\n",
    "# K = L[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skin_abd = ABD[K[:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skin_abd = skin_abd[:60000]\n",
    "# L=np.where(ABD[:,9] == 0)\n",
    "# K = L[0]\n",
    "# noskin_abd = ABD[K[:]]\n",
    "# noskin_abd = noskin_abd[:140000]\n",
    "\n",
    "# # L=np.where(FSD[:,9] > 0)\n",
    "# # K = L[0]\n",
    "# # skin_fsd = FSD[K[:]]\n",
    "# # skin_fsd = skin_fsd[:30000]\n",
    "# # L=np.where(FSD[:,9] == 0)\n",
    "# # K = L[0]\n",
    "# # noskin_fsd = FSD[K[:]]\n",
    "# # noskin_fsd = noskin_fsd[:70000]\n",
    "\n",
    "# L=np.where(HGR[:,9] > 0)\n",
    "# K = L[0]\n",
    "# skin_hgr = HGR[K[:]]\n",
    "# skin_hgr = skin_hgr[:60000]\n",
    "# L=np.where(HGR[:,9] == 0)\n",
    "# K = L[0]\n",
    "# noskin_hgr = HGR[K[:]]\n",
    "# noskin_hgr = noskin_hgr[:140000]\n",
    "\n",
    "# # L=np.where(Pratheep[:,9] > 0)\n",
    "# # K = L[0]\n",
    "# # skin_pra = Pratheep[K[:]]\n",
    "# # skin_pra = skin_pra[:30000]\n",
    "# # L=np.where(Pratheep[:,9] == 0)\n",
    "# # K = L[0]\n",
    "# # noskin_pra = Pratheep[K[:]]\n",
    "# # noskin_pra = noskin_pra[:70000]\n",
    "\n",
    "# L=np.where(SCHMG[:,9] > 0)\n",
    "# K = L[0]\n",
    "# skin_schmg = SCHMG[K[:]]\n",
    "# skin_schmg = skin_schmg[:60000]\n",
    "# L=np.where(SCHMG[:,9] == 0)\n",
    "# K = L[0]\n",
    "# noskin_schmg = SCHMG[K[:]]\n",
    "# noskin_schmg = noskin_schmg[:140000]\n",
    "\n",
    "# L=np.where(SFA[:,9] > 0)\n",
    "# K = L[0]\n",
    "# skin_sfa = SFA[K[:]]\n",
    "# skin_sfa = skin_sfa[:60000]\n",
    "# L=np.where(SFA[:,9] == 0)\n",
    "# K = L[0]\n",
    "# noskin_sfa = SFA[K[:]]\n",
    "# noskin_sfa = noskin_sfa[:140000]\n",
    "\n",
    "# L=np.where(TDSD[:,9] > 0)\n",
    "# K = L[0]\n",
    "# skin_tdsd = TDSD[K[:]]\n",
    "# skin_tdsd = skin_tdsd[:60000]\n",
    "# L=np.where(TDSD[:,9] == 0)\n",
    "# K = L[0]\n",
    "# noskin_tdsd = TDSD[K[:]]\n",
    "# noskin_tdsd = noskin_tdsd[:140000]\n",
    "\n",
    "# L=np.where(VDM[:,9] > 0)\n",
    "# K = L[0]\n",
    "# skin_vdm = VDM[K[:]]\n",
    "# skin_vdm = skin_vdm[:60000]\n",
    "# L=np.where(VDM[:,9] == 0)\n",
    "# K = L[0]\n",
    "# noskin_vdm = VDM[K[:]]\n",
    "# noskin_vdm = noskin_vdm[:140000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Shuffled data\n",
    "# # alldata = np.concatenate((ABD,FSD,HGR,VDM,SCHMG,SFA,TDSD,VDM), axis = 0)\n",
    "# alldata = np.concatenate((skin_abd,skin_hgr,skin_schmg,skin_sfa,skin_tdsd,skin_vdm,noskin_abd,noskin_hgr,noskin_schmg,noskin_sfa,noskin_tdsd,noskin_vdm), axis = 0)\n",
    "# np.random.shuffle(alldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alldata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take = alldata[:2000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(take.shape)\n",
    "# feat = take[:,:9]\n",
    "# labels = take[:,9]\n",
    "# labels = (labels > 0).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"feat.npy\",feat)\n",
    "# np.save(\"labels.npy\",labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = np.load('./feat.npy')\n",
    "labels = np.load('./labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0723 14:13:27.938718 140467151304448 deprecation_wrapper.py:119] From /home/anirudh/.env1/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0723 14:13:27.953252 140467151304448 deprecation_wrapper.py:119] From /home/anirudh/.env1/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0723 14:13:27.956308 140467151304448 deprecation_wrapper.py:119] From /home/anirudh/.env1/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0723 14:13:27.968583 140467151304448 deprecation_wrapper.py:119] From /home/anirudh/.env1/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0723 14:13:27.976080 140467151304448 deprecation.py:506] From /home/anirudh/.env1/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0723 14:13:28.264107 140467151304448 deprecation_wrapper.py:119] From /home/anirudh/.env1/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the network\n",
    "model = Sequential()\n",
    "model.add(Dense(9, input_dim=9, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dropout(0.2, input_shape=(9,)))\n",
    "model.add(Dense(32, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dropout(0.2, input_shape=(32,)))\n",
    "model.add(Dense(64, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dropout(0.2, input_shape=(64,)))\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dropout(0.2, input_shape=(128,)))\n",
    "model.add(Dense(256, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dropout(0.2, input_shape=(256,)))\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dropout(0.2, input_shape=(128,)))\n",
    "model.add(Dense(64, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dropout(0.2, input_shape=(64,)))\n",
    "model.add(Dense(32, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dropout(0.2, input_shape=(32,)))\n",
    "model.add(Dense(9, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dropout(0.2, input_shape=(9,)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model = multi_gpu_model(model, gpus=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 14:13:31.749125 140467151304448 deprecation_wrapper.py:119] From /home/anirudh/.env1/lib/python3.5/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0723 14:13:31.757601 140467151304448 deprecation.py:323] From /home/anirudh/.env1/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "optimize = optimizers.Adam(lr = lrn,epsilon = 1e-8)\n",
    "earlystopper = EarlyStopping(patience=50, verbose=1)\n",
    "checkpointer = ModelCheckpoint('./model_feat/lr_' + str(lrn)+ '.h5', verbose=1, save_best_only=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimize, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 960000 samples, validate on 240000 samples\n",
      "Epoch 1/50\n",
      "960000/960000 [==============================] - 300s 312us/step - loss: 3.8765 - acc: 0.7127 - val_loss: 0.6060 - val_acc: 0.7171\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.60600, saving model to ./model_feat/lr_0.0001.h5\n",
      "Epoch 2/50\n",
      "960000/960000 [==============================] - 295s 308us/step - loss: 0.6144 - acc: 0.7166 - val_loss: 0.4773 - val_acc: 0.7171\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.60600 to 0.47728, saving model to ./model_feat/lr_0.0001.h5\n",
      "Epoch 3/50\n",
      "960000/960000 [==============================] - 296s 308us/step - loss: 0.4363 - acc: 0.7453 - val_loss: 0.4212 - val_acc: 0.8351\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.47728 to 0.42120, saving model to ./model_feat/lr_0.0001.h5\n",
      "Epoch 4/50\n",
      "960000/960000 [==============================] - 296s 308us/step - loss: 0.3841 - acc: 0.8294 - val_loss: 0.4057 - val_acc: 0.8321\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.42120 to 0.40572, saving model to ./model_feat/lr_0.0001.h5\n",
      "Epoch 5/50\n",
      "960000/960000 [==============================] - 295s 308us/step - loss: 0.3567 - acc: 0.8492 - val_loss: 0.4302 - val_acc: 0.8334\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.40572\n",
      "Epoch 6/50\n",
      "960000/960000 [==============================] - 296s 309us/step - loss: 0.3460 - acc: 0.8536 - val_loss: 0.4081 - val_acc: 0.8492\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.40572\n",
      "Epoch 7/50\n",
      "960000/960000 [==============================] - 291s 303us/step - loss: 0.3381 - acc: 0.8570 - val_loss: 0.4062 - val_acc: 0.8494\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.40572\n",
      "Epoch 8/50\n",
      "960000/960000 [==============================] - 294s 306us/step - loss: 0.3338 - acc: 0.8587 - val_loss: 0.3804 - val_acc: 0.8612\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.40572 to 0.38042, saving model to ./model_feat/lr_0.0001.h5\n",
      "Epoch 9/50\n",
      "960000/960000 [==============================] - 296s 309us/step - loss: 0.3294 - acc: 0.8611 - val_loss: 0.3944 - val_acc: 0.8619\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.38042\n",
      "Epoch 10/50\n",
      "960000/960000 [==============================] - 297s 309us/step - loss: 0.3273 - acc: 0.8629 - val_loss: 0.3924 - val_acc: 0.8650\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.38042\n",
      "Epoch 11/50\n",
      "960000/960000 [==============================] - 297s 309us/step - loss: 0.3259 - acc: 0.8638 - val_loss: 0.3898 - val_acc: 0.8630\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.38042\n",
      "Epoch 12/50\n",
      "960000/960000 [==============================] - 297s 309us/step - loss: 0.3244 - acc: 0.8649 - val_loss: 0.3876 - val_acc: 0.8661\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.38042\n",
      "Epoch 13/50\n",
      "960000/960000 [==============================] - 297s 309us/step - loss: 0.3231 - acc: 0.8652 - val_loss: 0.3725 - val_acc: 0.8662\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.38042 to 0.37247, saving model to ./model_feat/lr_0.0001.h5\n",
      "Epoch 14/50\n",
      "960000/960000 [==============================] - 297s 309us/step - loss: 0.3223 - acc: 0.8656 - val_loss: 0.3740 - val_acc: 0.8654\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.37247\n",
      "Epoch 15/50\n",
      "960000/960000 [==============================] - 295s 308us/step - loss: 0.3213 - acc: 0.8661 - val_loss: 0.3891 - val_acc: 0.8568\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.37247\n",
      "Epoch 16/50\n",
      "960000/960000 [==============================] - 296s 309us/step - loss: 0.3198 - acc: 0.8667 - val_loss: 0.3703 - val_acc: 0.8579\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.37247 to 0.37028, saving model to ./model_feat/lr_0.0001.h5\n",
      "Epoch 17/50\n",
      "960000/960000 [==============================] - 297s 309us/step - loss: 0.3182 - acc: 0.8672 - val_loss: 0.3654 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.37028 to 0.36536, saving model to ./model_feat/lr_0.0001.h5\n",
      "Epoch 18/50\n",
      "960000/960000 [==============================] - 296s 309us/step - loss: 0.3175 - acc: 0.8676 - val_loss: 0.3788 - val_acc: 0.8645\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.36536\n",
      "Epoch 19/50\n",
      "960000/960000 [==============================] - 297s 309us/step - loss: 0.3170 - acc: 0.8680 - val_loss: 0.3735 - val_acc: 0.8662\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.36536\n",
      "Epoch 20/50\n",
      "960000/960000 [==============================] - 297s 309us/step - loss: 0.3162 - acc: 0.8682 - val_loss: 0.3744 - val_acc: 0.8649\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.36536\n",
      "Epoch 21/50\n",
      "960000/960000 [==============================] - 297s 309us/step - loss: 0.3158 - acc: 0.8687 - val_loss: 0.3829 - val_acc: 0.8585\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.36536\n",
      "Epoch 22/50\n",
      "960000/960000 [==============================] - 297s 309us/step - loss: 0.3151 - acc: 0.8690 - val_loss: 0.3791 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.36536\n",
      "Epoch 23/50\n",
      "960000/960000 [==============================] - 297s 310us/step - loss: 0.3147 - acc: 0.8691 - val_loss: 0.3836 - val_acc: 0.8606\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.36536\n",
      "Epoch 24/50\n",
      "960000/960000 [==============================] - 298s 310us/step - loss: 0.3138 - acc: 0.8693 - val_loss: 0.3751 - val_acc: 0.8730\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.36536\n",
      "Epoch 25/50\n",
      "960000/960000 [==============================] - 297s 309us/step - loss: 0.3136 - acc: 0.8695 - val_loss: 0.3802 - val_acc: 0.8701\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.36536\n",
      "Epoch 26/50\n",
      "960000/960000 [==============================] - 297s 310us/step - loss: 0.3130 - acc: 0.8698 - val_loss: 0.3705 - val_acc: 0.8708\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.36536\n",
      "Epoch 27/50\n",
      "960000/960000 [==============================] - 297s 309us/step - loss: 0.3125 - acc: 0.8701 - val_loss: 0.3751 - val_acc: 0.8676\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.36536\n",
      "Epoch 28/50\n",
      "960000/960000 [==============================] - 297s 310us/step - loss: 0.3120 - acc: 0.8701 - val_loss: 0.3787 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.36536\n",
      "Epoch 29/50\n",
      "960000/960000 [==============================] - 295s 307us/step - loss: 0.3117 - acc: 0.8702 - val_loss: 0.3457 - val_acc: 0.8712\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.36536 to 0.34571, saving model to ./model_feat/lr_0.0001.h5\n",
      "Epoch 30/50\n",
      "960000/960000 [==============================] - 256s 266us/step - loss: 0.3112 - acc: 0.8701 - val_loss: 0.3426 - val_acc: 0.8697\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.34571 to 0.34263, saving model to ./model_feat/lr_0.0001.h5\n",
      "Epoch 31/50\n",
      "960000/960000 [==============================] - 278s 290us/step - loss: 0.3106 - acc: 0.8706 - val_loss: 0.3635 - val_acc: 0.8659\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.34263\n",
      "Epoch 32/50\n",
      "960000/960000 [==============================] - 299s 312us/step - loss: 0.3099 - acc: 0.8706 - val_loss: 0.3634 - val_acc: 0.8699\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.34263\n",
      "Epoch 33/50\n",
      "960000/960000 [==============================] - 299s 312us/step - loss: 0.3101 - acc: 0.8709 - val_loss: 0.3616 - val_acc: 0.8644\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.34263\n",
      "Epoch 34/50\n",
      "960000/960000 [==============================] - 299s 312us/step - loss: 0.3095 - acc: 0.8710 - val_loss: 0.3680 - val_acc: 0.8599\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.34263\n",
      "Epoch 35/50\n",
      "960000/960000 [==============================] - 300s 312us/step - loss: 0.3097 - acc: 0.8709 - val_loss: 0.3507 - val_acc: 0.8641\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.34263\n",
      "Epoch 36/50\n",
      "960000/960000 [==============================] - 299s 312us/step - loss: 0.3088 - acc: 0.8714 - val_loss: 0.3565 - val_acc: 0.8659\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.34263\n",
      "Epoch 37/50\n",
      "960000/960000 [==============================] - 299s 312us/step - loss: 0.3090 - acc: 0.8714 - val_loss: 0.3629 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.34263\n",
      "Epoch 38/50\n",
      "960000/960000 [==============================] - 299s 312us/step - loss: 0.3086 - acc: 0.8714 - val_loss: 0.3535 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.34263\n",
      "Epoch 39/50\n",
      "960000/960000 [==============================] - 299s 312us/step - loss: 0.3086 - acc: 0.8717 - val_loss: 0.3539 - val_acc: 0.8702\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.34263\n",
      "Epoch 40/50\n",
      "960000/960000 [==============================] - 299s 312us/step - loss: 0.3083 - acc: 0.8714 - val_loss: 0.3502 - val_acc: 0.8640\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.34263\n",
      "Epoch 41/50\n",
      "960000/960000 [==============================] - 299s 312us/step - loss: 0.3082 - acc: 0.8714 - val_loss: 0.3604 - val_acc: 0.8654\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.34263\n",
      "Epoch 42/50\n",
      "960000/960000 [==============================] - 299s 311us/step - loss: 0.3080 - acc: 0.8715 - val_loss: 0.3602 - val_acc: 0.8664\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.34263\n",
      "Epoch 43/50\n",
      "960000/960000 [==============================] - 298s 311us/step - loss: 0.3086 - acc: 0.8714 - val_loss: 0.3633 - val_acc: 0.8483\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.34263\n",
      "Epoch 44/50\n",
      "960000/960000 [==============================] - 298s 311us/step - loss: 0.3075 - acc: 0.8719 - val_loss: 0.3609 - val_acc: 0.8566\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.34263\n",
      "Epoch 45/50\n",
      "960000/960000 [==============================] - 299s 311us/step - loss: 0.3077 - acc: 0.8718 - val_loss: 0.3508 - val_acc: 0.8659\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.34263\n",
      "Epoch 46/50\n",
      "960000/960000 [==============================] - 299s 311us/step - loss: 0.3081 - acc: 0.8715 - val_loss: 0.3574 - val_acc: 0.8634\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.34263\n",
      "Epoch 47/50\n",
      "960000/960000 [==============================] - 299s 312us/step - loss: 0.3075 - acc: 0.8716 - val_loss: 0.3425 - val_acc: 0.8599\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.34263 to 0.34249, saving model to ./model_feat/lr_0.0001.h5\n",
      "Epoch 48/50\n",
      "960000/960000 [==============================] - 299s 311us/step - loss: 0.3075 - acc: 0.8716 - val_loss: 0.3659 - val_acc: 0.8543\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.34249\n",
      "Epoch 49/50\n",
      "960000/960000 [==============================] - 299s 311us/step - loss: 0.3075 - acc: 0.8717 - val_loss: 0.3543 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.34249\n",
      "Epoch 50/50\n",
      "960000/960000 [==============================] - 299s 311us/step - loss: 0.3071 - acc: 0.8717 - val_loss: 0.3531 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.34249\n"
     ]
    }
   ],
   "source": [
    "results = model.fit(feat, labels, validation_split=0.2, epochs=50, batch_size=64, shuffle=True, \n",
    "                    callbacks=[earlystopper, checkpointer, cb,TrainValTensorBoard(write_graph=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cd5f169708aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Summarize history for loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# #print(results.history.keys())\n",
    "\n",
    "# # Summarize history for loss\n",
    "# plt.plot(results.history['loss'])\n",
    "# plt.plot(results.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# # Summarize history for mean_iou\n",
    "# plt.plot(results.history['acc'])\n",
    "# plt.plot(results.history['val_acc'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# # Results and Plots\n",
    "# # model.summary()\n",
    "# print(\"UNET ARCHITECTURE\")\n",
    "# print (\"-------------------------------------------------------------\")\n",
    "# print(\"Total num of training images: %d\" % len(train_ids))\n",
    "# print(\"Max num of epochs: %d\" % 50)\n",
    "# print(\"Optimizer: %s\" % 'ADAM')\n",
    "# print(\"Batch size: %d\" % 64)\n",
    "# print(\"Loss function: %s\" % 'Binary Cross-Entropy')\n",
    "# print(\"Validation data percentage: %d\" % 10)\n",
    "# print(\"Early stoppping: %s\" % 'Yes')\n",
    "\n",
    "# ep = 50;\n",
    "# a = results.history[\"acc\"]\n",
    "# b = results.history[\"loss\"]\n",
    "# c = results.history[\"val_acc\"]\n",
    "# d = results.history[\"val_loss\"]\n",
    "# e = cb.times\n",
    "# print (\"-------------------------------------------------------------\")\n",
    "# header = \"#\"+\"    \"+\"Time sec\"+\"      \"+\"Tr_acc\"+\"     \"+\"Tr_loss\"+\"      \"+\"Vl_acc\"+\"     \"+\"Vl_loss\"\n",
    "# print(header)\n",
    "# print (\"-------------------------------------------------------------\")\n",
    "# for l in range(ep):\n",
    "#     str = \"%d\\t\\t%f\\t\\t%f\\t\\t%f\\t\\t%f\\t\\t%f\" % (l, round(e[l],4),round(a[l],4),round(b[l],4),round(c[l],4),d[l])\n",
    "#     print (str.expandtabs(2))\n",
    "# print (\"-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Evaluate the model\n",
    "# model = load_model('model-testall-26Mar2019.h5')\n",
    "# IMG_HEIGHT = 128\n",
    "# IMG_WIDTH = 128\n",
    "# IMG_CHANNELS = 3\n",
    "\n",
    "# # Compute Accuracy for Abdomen Set\n",
    "# ABD_PATH = ['/home/lalzogbi/Documents/Skin_Datasets/Dataset4_Pratheepan/original_images/']\n",
    "# MSK_PATH = ['/home/lalzogbi/Documents/Skin_Datasets/Dataset4_Pratheepan/skin_masks/']\n",
    "# # abd_ids = next(os.walk(TRAIN_PATH2[0]))[2]\n",
    "# # msk_ids = next(os.walk(MASK_PATH2[0]))[2]\n",
    "# # abd_ids.sort()\n",
    "# # msk_ids.sort()\n",
    "# # ABD_PATH = ABD_PATH*len(abd_ids)\n",
    "# # MSK_PATH = MSK_PATH*len(msk_ids)\n",
    "\n",
    "# # ABD_PATH = ['/home/lalzogbi/Documents/Skin_Datasets/Dataset7_SFA/original_images/']\n",
    "# # MSK_PATH = ['/home/lalzogbi/Documents/Skin_Datasets/Dataset7_SFA/skin_masks/']\n",
    "\n",
    "# ABD_PATH = TRAIN_PATH8\n",
    "# MSK_PATH = MASK_PATH8\n",
    "# abd_ids = next(os.walk(ABD_PATH[0]))[2]\n",
    "# msk_ids = next(os.walk(MSK_PATH[0]))[2]\n",
    "# abd_ids.sort()\n",
    "# msk_ids.sort()\n",
    "# abd_ids = abd_ids[-100:]\n",
    "# msk_ids = msk_ids[-100:]\n",
    "\n",
    "# print('Getting features and labels from images unreshaped... ')\n",
    "# sys.stdout.flush()\n",
    "\n",
    "# path = ABD_PATH[0] + abd_ids[0]\n",
    "# img = imread(path)[:,:,:IMG_CHANNELS]\n",
    "# dat = img.reshape(img.shape[0]*img.shape[1],3)\n",
    "# hsv = cv.cvtColor(img, cv.COLOR_RGB2HSV)\n",
    "# hsv = hsv.reshape(img.shape[0]*img.shape[1],3)\n",
    "# lab = cv.cvtColor(img, cv.COLOR_RGB2Lab)\n",
    "# lab = lab.reshape(img.shape[0]*img.shape[1],3)\n",
    "\n",
    "# path = MSK_PATH[0] + msk_ids[0]\n",
    "# img = imread(path)\n",
    "\n",
    "# if img.ndim == 3:\n",
    "#     img = img[:,:,1]\n",
    "    \n",
    "# # img = img > 30    \n",
    "# img = img > 0\n",
    "# img = img.astype(np.uint8)\n",
    "# L = img.reshape(img.shape[0]*img.shape[1],1)\n",
    "\n",
    "# dat = np.concatenate((dat,hsv,lab,L),axis=1)\n",
    "\n",
    "# for n, id_ in tqdm(enumerate(abd_ids), total=len(abd_ids)):\n",
    "#     path = ABD_PATH[n] + abd_ids[n]\n",
    "#     img = imread(path)[:,:,:IMG_CHANNELS]\n",
    "#     b = img.reshape(img.shape[0]*img.shape[1],3)\n",
    "#     hsv = cv.cvtColor(img, cv.COLOR_RGB2HSV)\n",
    "#     hsv = hsv.reshape(img.shape[0]*img.shape[1],3)\n",
    "#     lab = cv.cvtColor(img, cv.COLOR_RGB2Lab)\n",
    "#     lab = lab.reshape(img.shape[0]*img.shape[1],3)\n",
    "    \n",
    "#     path = MSK_PATH[n] + msk_ids[n]\n",
    "#     img = imread(path)\n",
    "\n",
    "#     if img.ndim == 3:\n",
    "#         img = img[:,:,1]\n",
    "    \n",
    "# #     img = img > 30    \n",
    "#     img = img > 0\n",
    "#     img = img.astype(np.uint8)\n",
    "#     L = img.reshape(img.shape[0]*img.shape[1],1)\n",
    "    \n",
    "#     dat_temp = np.concatenate((b,hsv,lab,L),axis=1)\n",
    "#     dat = np.concatenate((dat,dat_temp))\n",
    "   \n",
    "# X = dat[:,0:9]\n",
    "# Y = dat[:,9]\n",
    "\n",
    "# scores = model.evaluate(X, Y)\n",
    "# print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # scores = model.evaluate(X, Y)\n",
    "# # preds_test = model.predict(X, verbose=1)\n",
    "# # preds_test_t = (preds_test > 0.5).astype(np.uint8)\n",
    "# # print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# # Evaluate the model\n",
    "# model = load_model('model-masturbation2-11Feb2019.h5')\n",
    "# IMG_HEIGHT = 128\n",
    "# IMG_WIDTH = 128\n",
    "# IMG_CHANNELS = 3\n",
    "\n",
    "# # Compute Accuracy for Abdomen Set\n",
    "# ABD_PATH = '/home/lalzogbi/Documents/Skin_Datasets/allabdomen/val/skin_val2019/'\n",
    "# MSK_PATH = '/home/lalzogbi/Documents/Skin_Datasets/allabdomen/val/annotations/'\n",
    "\n",
    "# # ABD_PATH = ['/home/lalzogbi/Documents/Skin_Datasets/Dataset4_Pratheepan/original_images/']\n",
    "# # MSK_PATH = ['/home/lalzogbi/Documents/Skin_Datasets/Dataset4_Pratheepan/skin_masks/']\n",
    "# abd_ids = next(os.walk(ABD_PATH))[2]\n",
    "# msk_ids = next(os.walk(MSK_PATH))[2]\n",
    "# abd_ids.sort()\n",
    "# msk_ids.sort()\n",
    "# # ABD_PATH = ABD_PATH*len(abd_ids)\n",
    "# # MSK_PATH = MSK_PATH*len(msk_ids)\n",
    "# # print(len(abd_ids))\n",
    "\n",
    "# # abd_ids = abd_ids[6:]\n",
    "# # msk_ids = msk_ids[6:]\n",
    "\n",
    "# # Actual Predictions\n",
    "# # N = np.zeros((1,4))\n",
    "\n",
    "# # for n, id_ in tqdm(enumerate(abd_ids), total=len(abd_ids)):\n",
    "# path = ABD_PATH + abd_ids[21]\n",
    "# img = imread(path)[:,:,:IMG_CHANNELS]\n",
    "# t = img\n",
    "# #     img = resize(img, (128, 128), mode='constant', preserve_range=True).astype('uint8')\n",
    "# b = img.reshape(img.shape[0]*img.shape[1],3)\n",
    "\n",
    "# hsv = cv.cvtColor(img, cv.COLOR_RGB2HSV)\n",
    "# hsv = hsv.reshape(img.shape[0]*img.shape[1],3)\n",
    "# lab = cv.cvtColor(img, cv.COLOR_RGB2Lab)\n",
    "# lab = lab.reshape(img.shape[0]*img.shape[1],3)\n",
    "\n",
    "# path = MSK_PATH + msk_ids[21]\n",
    "# img = imread(path)\n",
    "# #     img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', \n",
    "# #                                       preserve_range=True).astype('uint8')\n",
    "\n",
    "# if img.ndim == 3:\n",
    "#     img = img[:,:,1]\n",
    "\n",
    "# img = img > 30    \n",
    "# img = img > 0\n",
    "# img = img.astype(np.uint8)\n",
    "# L = img.reshape(img.shape[0]*img.shape[1],1)\n",
    "# dat_temp = np.concatenate((b,hsv,lab,L),axis=1)\n",
    "\n",
    "# X = dat_temp[:,0:9]\n",
    "# Y = dat_temp[:,9]\n",
    "    \n",
    "# preds_test = model.predict(X[:int(X.shape[0])], verbose=1)\n",
    "\n",
    "# # Threshold predictions\n",
    "# preds_test_t = (preds_test > 0.5).astype(np.uint8)\n",
    "\n",
    "# # answer = acc_comp(Y, preds_test_t)\n",
    "\n",
    "# # a = answer[1]\n",
    "# # b = answer[2]\n",
    "# # c = answer[3]\n",
    "# # d = answer[4]\n",
    "\n",
    "# # K = np.array((a,b,c,d)).reshape(1,4)\n",
    "# # N = np.concatenate((N,K),axis = 0)\n",
    "# #     print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(preds_test_t.shape)\n",
    "# preds_test_t.reshape(t.shape[0],t.shape[1],1)\n",
    "# curly = preds_test_t.reshape(t.shape[0],t.shape[1])\n",
    "# imshow(curly*255)\n",
    "# plt.show()\n",
    "# cv.imwrite('darkskinFeatures.jpg',curly*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Overall accuracy on abdomen pictures\n",
    "# # answer = acc_comp(Y, preds_test_t)\n",
    "\n",
    "# # a = answer[1]\n",
    "# # b = answer[2]\n",
    "# # c = answer[3]\n",
    "# # d = answer[4]\n",
    "# # K = np.array((a,b,c,d)).reshape(1,4)\n",
    "# # print(K.shape)\n",
    "\n",
    "# # N = np.zeros((1,4))\n",
    "# # N = np.concatenate((N,K),axis = 0)\n",
    "# out = N[1:]\n",
    "# j = (out[:,0]+out[:,1])/(out[:,0]+out[:,1]+out[:,2]+out[:,3])\n",
    "# print(np.mean(j))\n",
    "# np.save('pra-features.npy',out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = np.reshape(preds_test_t,(img.shape[0],img.shape[1]))\n",
    "# plt.show()\n",
    "# imshow(output)\n",
    "# plt.show()\n",
    "# imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat = dat[0:800000,0:9]\n",
    "# labels = dat[0:800000,9]\n",
    "# labels = (labels > 0).astype(np.uint8)\n",
    "# print(feat.shape)\n",
    "# print(labels.shape)\n",
    "# X = dat[800000:,0:9]\n",
    "# Y = dat[800000:,9]\n",
    "# Y = (Y > 0).astype(np.uint8) \n",
    "# print(X.shape)\n",
    "# print(Y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
